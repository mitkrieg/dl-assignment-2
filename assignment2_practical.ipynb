{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1vZ5j3rl54tLkQTsL9I2Xr-r05pSDq6ru",
      "authorship_tag": "ABX9TyMJRHYOYpvvJ3+tEXLJ4ADx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a213a265a95c4f5f85c11b3a8c4d6dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af118d718b0b486d9b16d1c00b33634e",
              "IPY_MODEL_191de541ed3f4a5f8524c32c0a2cb18f"
            ],
            "layout": "IPY_MODEL_fe52d35dafba412696610583a86ea0c8"
          }
        },
        "af118d718b0b486d9b16d1c00b33634e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d5bd3879b784af2a2169a7069a21dfc",
            "placeholder": "​",
            "style": "IPY_MODEL_4db8d1919f2c46dbad9ba0411d8bee09",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "191de541ed3f4a5f8524c32c0a2cb18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afecc109efa8408d98a057af344b53fd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b930fa5d05f42c895ebccc6f630b458",
            "value": 1
          }
        },
        "fe52d35dafba412696610583a86ea0c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d5bd3879b784af2a2169a7069a21dfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db8d1919f2c46dbad9ba0411d8bee09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afecc109efa8408d98a057af344b53fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b930fa5d05f42c895ebccc6f630b458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b260305942349788e73dbcd2229dca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0eea056eb27a4812a3a08dd6b8aa966c",
              "IPY_MODEL_a7b43b9f6d69485da22d253d52935504"
            ],
            "layout": "IPY_MODEL_258623cec6284cd19f3c0376c92de570"
          }
        },
        "0eea056eb27a4812a3a08dd6b8aa966c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a70f3ae137e94f73a2f5f6585c1ccdf8",
            "placeholder": "​",
            "style": "IPY_MODEL_6e8a7f4c032945789c91e6650863de9f",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "a7b43b9f6d69485da22d253d52935504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9253dcb7a4422aaa710432dc8cb2c8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93385e286a164f15a929b36f11a5ee06",
            "value": 1
          }
        },
        "258623cec6284cd19f3c0376c92de570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a70f3ae137e94f73a2f5f6585c1ccdf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8a7f4c032945789c91e6650863de9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a9253dcb7a4422aaa710432dc8cb2c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93385e286a164f15a929b36f11a5ee06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c346ab8ab4474975826f4870034022ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ff922dcaf774532b3c59e1f868a8947",
              "IPY_MODEL_2f2e537db4bf4c6db4b242e3fe49d489"
            ],
            "layout": "IPY_MODEL_06878bab4d4f460c84d744bbe0acb44b"
          }
        },
        "4ff922dcaf774532b3c59e1f868a8947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_120afb4fb1154133913f397934b73dab",
            "placeholder": "​",
            "style": "IPY_MODEL_7d2b69941fd34fc390b4265eaee428f3",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "2f2e537db4bf4c6db4b242e3fe49d489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a3b08b3cb04485a9d99f6fe1f33024d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b2d07916b654d668c3c1a66758af01f",
            "value": 1
          }
        },
        "06878bab4d4f460c84d744bbe0acb44b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120afb4fb1154133913f397934b73dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d2b69941fd34fc390b4265eaee428f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a3b08b3cb04485a9d99f6fe1f33024d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b2d07916b654d668c3c1a66758af01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7ec2c84bfa2410dacb4d85de8d6d471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_217eca5728d44720b3423387359af7f0",
              "IPY_MODEL_94df51205539495db6d2d07b4d40cede"
            ],
            "layout": "IPY_MODEL_b119df608d7d4b5f90c9d3632f961b50"
          }
        },
        "217eca5728d44720b3423387359af7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c84f2a4775da4ffe9e662bccdfa5ff6f",
            "placeholder": "​",
            "style": "IPY_MODEL_bc3c4028e02f4b5faf7a263c0dafe234",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "94df51205539495db6d2d07b4d40cede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aaff92ed6d54975a2def9bad0946862",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c88c3245cb9b4675847f543b64b3ceef",
            "value": 1
          }
        },
        "b119df608d7d4b5f90c9d3632f961b50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c84f2a4775da4ffe9e662bccdfa5ff6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc3c4028e02f4b5faf7a263c0dafe234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aaff92ed6d54975a2def9bad0946862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c88c3245cb9b4675847f543b64b3ceef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ff1355693ba4a9aa80e6a07dd5b0112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82ca31ae1f6c4c2fafec02ca6de89fd7",
              "IPY_MODEL_7bc0e49dcf8b4afa8470989005dcabe2"
            ],
            "layout": "IPY_MODEL_02a55334a4954304bdd3e8b7aa803a25"
          }
        },
        "82ca31ae1f6c4c2fafec02ca6de89fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337623417eeb4758bb6963a67a653b21",
            "placeholder": "​",
            "style": "IPY_MODEL_86be621388144bbc9bec8536250d0afb",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "7bc0e49dcf8b4afa8470989005dcabe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_709f1f6335254b7a83e21daf16b2ab5a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6d1409497814256b4365b97fcb90da7",
            "value": 1
          }
        },
        "02a55334a4954304bdd3e8b7aa803a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337623417eeb4758bb6963a67a653b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86be621388144bbc9bec8536250d0afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "709f1f6335254b7a83e21daf16b2ab5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d1409497814256b4365b97fcb90da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09e8200fdab94d768427bd16aad636c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f1710f62e364d04869ab6ff149a9fd3",
              "IPY_MODEL_769295408fdc421b83c1ae610aedfef1"
            ],
            "layout": "IPY_MODEL_ea7dd40e203d4c6fb56e6921d1e18c4f"
          }
        },
        "8f1710f62e364d04869ab6ff149a9fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5319b520c3a84b858261417da3e5b48a",
            "placeholder": "​",
            "style": "IPY_MODEL_47406269ec134df8aa8c2583305f93e9",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "769295408fdc421b83c1ae610aedfef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82b58bfca1dc431bbd6aeb4a26234cfd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d05d30b4abe4540bd59dcfa5a7334da",
            "value": 1
          }
        },
        "ea7dd40e203d4c6fb56e6921d1e18c4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5319b520c3a84b858261417da3e5b48a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47406269ec134df8aa8c2583305f93e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82b58bfca1dc431bbd6aeb4a26234cfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d05d30b4abe4540bd59dcfa5a7334da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-2/blob/main/assignment2_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 5787 Deep Learning Assignment 2\n",
        "\n",
        "This notebook implements the \"small\" LSTM model as described in \"Recurrent Neural Network Regularization\" by Zaremba et al (2014)."
      ],
      "metadata": {
        "id": "VkzfolXnJkjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {
        "id": "h5Ra9H5dKOUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Weights & Biases"
      ],
      "metadata": {
        "id": "EmcvGaiIKSW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4R2RS5a0QQ9",
        "outputId": "d2eeabc8-f525-4cce-c00a-51a642005cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & GPU Check"
      ],
      "metadata": {
        "id": "wGqfF7ufKaJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import wandb\n",
        "\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "torch.cuda.manual_seed_all(123)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('CUDA GPU Available:',torch.cuda.is_available())\n",
        "print('MPS GPU Available:', torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "#   device = torch.device('mps')\n",
        "#   print('Pytorch GPU Build:',torch.backends.mps.is_built())\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ],
      "metadata": {
        "id": "9BLt6FUx0VtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254ac23e-4ef8-4b79-b28e-cddfc14dc234"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "CUDA GPU Available: True\n",
            "MPS GPU Available: False\n",
            "GPU Name: Tesla T4\n",
            "GPU Count: 1\n",
            "GPU Memory Allocated: 0\n",
            "GPU Memory Cached: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary & PTBText Dataset Classes\n",
        "\n",
        "Parse data from raw files & create dataset class to interact with"
      ],
      "metadata": {
        "id": "FAORGwMbKekd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, pre_built_dict: dict=None):\n",
        "        if pre_built_dict:\n",
        "            self.vocab = pre_built_dict\n",
        "        else:\n",
        "            self.vocab = {'<pad>': 0, '<oov>': 1, '<sos>': 2, '<eos>': 3, '<unk>': 4}\n",
        "        self.idx = len(self.vocab)\n",
        "\n",
        "    def add_word(self, word: str) -> None:\n",
        "        if word not in self.vocab:\n",
        "            self.vocab[word] = self.idx\n",
        "            self.idx += 1\n",
        "\n",
        "    def encode(self, tokens: list[str]) -> list[int]:\n",
        "        return [self.vocab.get(word, self.vocab['<unk>']) for word in tokens]\n",
        "\n",
        "    def decode(self, indicies: list[int]) -> list[str]:\n",
        "        return [list(self.vocab.keys())[list(self.vocab.values()).index(idx)] for idx in indicies]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "class PTBText(Dataset):\n",
        "    def __init__(self, path: str, vocab: Vocab=Vocab(), build_vocab=True, batch_size=20, seqence_length=20, device=torch.device('cpu')):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.vocab = vocab\n",
        "        self.data = self.load_data(build_vocab)\n",
        "        self.batch_size = batch_size\n",
        "        self.chunk_size = len(self.data) // batch_size\n",
        "        self.seq_len = seqence_length\n",
        "        self.minibatches = self.create_batches()\n",
        "\n",
        "    def load_data(self, build_vocab):\n",
        "        data = []\n",
        "        with open(self.path, 'r') as f:\n",
        "            count = 0\n",
        "            for line in f:\n",
        "                count += 1\n",
        "                tokens = line.strip().split() + ['<eos>']\n",
        "                if build_vocab:\n",
        "                    for token in tokens:\n",
        "                        self.vocab.add_word(token)\n",
        "\n",
        "                encoded_line = self.vocab.encode(tokens)\n",
        "                data.extend(encoded_line)\n",
        "        return data\n",
        "\n",
        "    def create_batches(self):\n",
        "        return [self.data[i*self.chunk_size: (i+1)*self.chunk_size] for i in range(self.batch_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, j):\n",
        "        inputs = torch.stack([\n",
        "            torch.LongTensor(self.minibatches[i][j * self.seq_len : (j + 1) * self.seq_len])\n",
        "            for i in range(self.batch_size)], dim=0)\n",
        "        labels = torch.stack([\n",
        "            torch.LongTensor(self.minibatches[i][j * self.seq_len + 1 : (j + 1) * self.seq_len + 1])\n",
        "            for i in range(self.batch_size)], dim=0)\n",
        "\n",
        "        return inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "    def get_tokens(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_decoded_tokens(self, idx):\n",
        "        return self.vocab.decode(self.data[idx])\n",
        "\n",
        "\n",
        "train = PTBText('./data/ptb.train.txt', device=device)\n",
        "val = PTBText('./data/ptb.valid.txt', vocab=train.vocab, build_vocab=False, device=device)\n",
        "test = PTBText('./data/ptb.test.txt', vocab=train.vocab, build_vocab=False, device=device)\n",
        "\n",
        "datasets = {\n",
        "    'train': train,\n",
        "    'val': val,\n",
        "    'test': test\n",
        "}\n",
        "\n",
        "print(\"Vocab size:\", len(train.vocab))\n",
        "print(\"Train data size:\", len(train))\n",
        "print(\"Val data size:\", len(val))\n",
        "print(\"Test data size:\", len(test))"
      ],
      "metadata": {
        "id": "KUza40e_165_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099701be-f50b-4949-865e-ccf9b39f46d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 10003\n",
            "Train data size: 929589\n",
            "Val data size: 73760\n",
            "Test data size: 82430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model Architecture\n",
        "\n",
        "The `ZamrembaRNN` module allows for either LSTM or GRU models to be implmented with or without dropout as described in the paper"
      ],
      "metadata": {
        "id": "N7x6ITOOKlbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ZamrembaRNN(nn.Module):\n",
        "    def __init__(self, rnn_type, vocab_size, batch_size=20, embedding_dim=200, hidden_dim=200, num_layers=2, dropout=0, rnn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "        self.batch_size = batch_size\n",
        "        if rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=rnn_dropout, batch_first=True)\n",
        "        elif rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid RNN type: must be 'lstm' or 'gru'\")\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        if dropout > 0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "\n",
        "        #LSTM has two states (hidden& cell) where as GRU only has one hidden state\n",
        "        if self.rnn_type == 'lstm':\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        elif self.rnn_type == 'gru':\n",
        "            output, hidden = self.rnn(output, hidden[0])\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        gen = torch.Generator().manual_seed(132)\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.embedding.weight, -initrange, initrange, generator=gen)\n",
        "        nn.init.uniform_(self.rnn.weight_ih_l0, -initrange, initrange, generator=gen)\n",
        "        nn.init.uniform_(self.rnn.weight_hh_l0, -initrange, initrange, generator=gen)\n",
        "        nn.init.uniform_(self.fc.weight, -initrange, initrange, generator=gen)"
      ],
      "metadata": {
        "id": "XRRsn3Df_mAi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training & Evaluation Loops"
      ],
      "metadata": {
        "id": "qjYvdz1ULLOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_hidden(model):\n",
        "    if model.rnn_type == 'lstm':\n",
        "        return (torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device),\n",
        "              torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device))\n",
        "    elif model.rnn_type == 'gru':\n",
        "        return torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device).unsqueeze(0)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid RNN type: must be 'lstm' or 'gru'\")\n",
        "\n",
        "def detach_hidden(hidden):\n",
        "    if isinstance(hidden, tuple):\n",
        "        return tuple([h.detach() for h in hidden])\n",
        "    else:\n",
        "        return hidden.detach()\n",
        "\n",
        "def train_epoch(model, dataset, loss_fn, optimizer, device, epoch, verbosity):\n",
        "    \"\"\"Train one epoch of a network\"\"\"\n",
        "    model.train()\n",
        "    batch_loss = 0\n",
        "\n",
        "    hidden = get_new_hidden(model)\n",
        "\n",
        "    for j in range(dataset.chunk_size // dataset.seq_len):\n",
        "\n",
        "        inputs, labels = dataset[j]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hidden = detach_hidden(hidden)\n",
        "\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        if model.rnn_type == 'gru':\n",
        "            hidden = hidden.unsqueeze(0)\n",
        "\n",
        "        loss = loss_fn(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "        if (j + 1) % verbosity == 0:\n",
        "            print(f'Batch #{j + 1} Loss: {batch_loss / verbosity}')\n",
        "            batch_loss = 0\n",
        "\n",
        "def perplexity(loss, batches):\n",
        "    return math.exp(loss / batches)\n",
        "\n",
        "def evaluate_model(title, model, dataset, loss_fn, seq_len, batch_size, epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataset) // (batch_size * seq_len)\n",
        "\n",
        "    hidden = get_new_hidden(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for j in range(num_batches):\n",
        "\n",
        "            inputs, labels = dataset[j]\n",
        "\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            if model.rnn_type == 'gru':\n",
        "                hidden = hidden.unsqueeze(0)\n",
        "            loss = loss_fn(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    perp = perplexity(total_loss, num_batches)\n",
        "    wandb.log({\n",
        "            f'{title}-loss': total_loss / num_batches,\n",
        "            f'{title}-perplexity': perp\n",
        "        }, step=epoch)\n",
        "\n",
        "    print(f'\\033[92m{title} perplexity: {perp:.6f} ||| loss {total_loss / num_batches:.6f}\\033[0m')\n",
        "\n",
        "    return perp\n",
        "\n",
        "def train_network(model, datasets, loss_fn, optimizer, schedule, device, epochs: int, verbosity: int):\n",
        "    for epoch in range(epochs):\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'----------- Epoch #{epoch + 1}, LR: {lr} ------------')\n",
        "        train_epoch(model, datasets['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "        train_perplexity = evaluate_model('Train', model, datasets['train'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        val_perplexity = evaluate_model('Validation', model, datasets['val'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        test_perplexity = evaluate_model('Test', model, datasets['test'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        print('------------------------------------\\n')\n",
        "\n",
        "        schedule.step()\n",
        "    print('----------- Train Complete! ------------')\n",
        "    return {\n",
        "        'train':train_perplexity,\n",
        "        'val':val_perplexity,\n",
        "        'test':test_perplexity\n",
        "    }"
      ],
      "metadata": {
        "id": "V3z6xiymAzyB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Models"
      ],
      "metadata": {
        "id": "o3Tn4mEGJgxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM No Regularization"
      ],
      "metadata": {
        "id": "ss17f_DgLSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 10\n",
        "learning_rate_decay = 0.5\n",
        "lr = 4\n",
        "dropout_rate = 0\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('lstm', len(train.vocab)).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-quad\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 14, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a213a265a95c4f5f85c11b3a8c4d6dcd",
            "af118d718b0b486d9b16d1c00b33634e",
            "191de541ed3f4a5f8524c32c0a2cb18f",
            "fe52d35dafba412696610583a86ea0c8",
            "8d5bd3879b784af2a2169a7069a21dfc",
            "4db8d1919f2c46dbad9ba0411d8bee09",
            "afecc109efa8408d98a057af344b53fd",
            "4b930fa5d05f42c895ebccc6f630b458",
            "6b260305942349788e73dbcd2229dca7",
            "0eea056eb27a4812a3a08dd6b8aa966c",
            "a7b43b9f6d69485da22d253d52935504",
            "258623cec6284cd19f3c0376c92de570",
            "a70f3ae137e94f73a2f5f6585c1ccdf8",
            "6e8a7f4c032945789c91e6650863de9f",
            "3a9253dcb7a4422aaa710432dc8cb2c8",
            "93385e286a164f15a929b36f11a5ee06"
          ]
        },
        "id": "0CDIw79OTAr6",
        "outputId": "5d1c856d-4b30-4263-ea21-9a707ea08940"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:3a04k55p) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a213a265a95c4f5f85c11b3a8c4d6dcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earthy-water-7</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/3a04k55p' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/3a04k55p</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240926_020007-3a04k55p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:3a04k55p). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240926_020016-9x4ttaq6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/9x4ttaq6' target=\"_blank\">pious-snowball-8</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/9x4ttaq6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/9x4ttaq6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 4 ------------\n",
            "Batch #500 Loss: 6.814496244430542\n",
            "Batch #1000 Loss: 6.190024091720581\n",
            "Batch #1500 Loss: 5.95184453201294\n",
            "Batch #2000 Loss: 5.80175147819519\n",
            "\u001b[92mTrain perplexity: 288.813738 ||| loss 5.665782\u001b[0m\n",
            "\u001b[92mValidation perplexity: 294.080869 ||| loss 5.683855\u001b[0m\n",
            "\u001b[92mTest perplexity: 287.957971 ||| loss 5.662815\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 4 ------------\n",
            "Batch #500 Loss: 5.615063290596009\n",
            "Batch #1000 Loss: 5.535354831695557\n",
            "Batch #1500 Loss: 5.4458003463745115\n",
            "Batch #2000 Loss: 5.38455379486084\n",
            "\u001b[92mTrain perplexity: 203.791267 ||| loss 5.317096\u001b[0m\n",
            "\u001b[92mValidation perplexity: 220.461390 ||| loss 5.395723\u001b[0m\n",
            "\u001b[92mTest perplexity: 214.807699 ||| loss 5.369743\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 4 ------------\n",
            "Batch #500 Loss: 5.295789780616761\n",
            "Batch #1000 Loss: 5.249943510055542\n",
            "Batch #1500 Loss: 5.1965356426239016\n",
            "Batch #2000 Loss: 5.156316002845764\n",
            "\u001b[92mTrain perplexity: 166.985471 ||| loss 5.117907\u001b[0m\n",
            "\u001b[92mValidation perplexity: 190.198644 ||| loss 5.248069\u001b[0m\n",
            "\u001b[92mTest perplexity: 184.965189 ||| loss 5.220168\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 4 ------------\n",
            "Batch #500 Loss: 5.1005642786026\n",
            "Batch #1000 Loss: 5.059943042755127\n",
            "Batch #1500 Loss: 5.022519619941711\n",
            "Batch #2000 Loss: 4.993981801986695\n",
            "\u001b[92mTrain perplexity: 141.519327 ||| loss 4.952436\u001b[0m\n",
            "\u001b[92mValidation perplexity: 169.197348 ||| loss 5.131066\u001b[0m\n",
            "\u001b[92mTest perplexity: 164.653321 ||| loss 5.103842\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 4 ------------\n",
            "Batch #500 Loss: 4.952375690460205\n",
            "Batch #1000 Loss: 4.910931775093078\n",
            "Batch #1500 Loss: 4.88071145439148\n",
            "Batch #2000 Loss: 4.8619288072586055\n",
            "\u001b[92mTrain perplexity: 124.163616 ||| loss 4.821600\u001b[0m\n",
            "\u001b[92mValidation perplexity: 156.063353 ||| loss 5.050262\u001b[0m\n",
            "\u001b[92mTest perplexity: 152.700183 ||| loss 5.028476\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 4 ------------\n",
            "Batch #500 Loss: 4.828995505332947\n",
            "Batch #1000 Loss: 4.785153299331665\n",
            "Batch #1500 Loss: 4.760435444355011\n",
            "Batch #2000 Loss: 4.748765632629395\n",
            "\u001b[92mTrain perplexity: 110.938002 ||| loss 4.708972\u001b[0m\n",
            "\u001b[92mValidation perplexity: 147.087353 ||| loss 4.991027\u001b[0m\n",
            "\u001b[92mTest perplexity: 143.246689 ||| loss 4.964568\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 4 ------------\n",
            "Batch #500 Loss: 4.722463170051575\n",
            "Batch #1000 Loss: 4.676240677833557\n",
            "Batch #1500 Loss: 4.657453967094422\n",
            "Batch #2000 Loss: 4.650091633319855\n",
            "\u001b[92mTrain perplexity: 101.016399 ||| loss 4.615283\u001b[0m\n",
            "\u001b[92mValidation perplexity: 140.863579 ||| loss 4.947792\u001b[0m\n",
            "\u001b[92mTest perplexity: 137.616502 ||| loss 4.924471\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 4 ------------\n",
            "Batch #500 Loss: 4.627607270717621\n",
            "Batch #1000 Loss: 4.579852375030518\n",
            "Batch #1500 Loss: 4.567068667411804\n",
            "Batch #2000 Loss: 4.564118046283722\n",
            "\u001b[92mTrain perplexity: 93.519098 ||| loss 4.538166\u001b[0m\n",
            "\u001b[92mValidation perplexity: 137.579307 ||| loss 4.924201\u001b[0m\n",
            "\u001b[92mTest perplexity: 134.657136 ||| loss 4.902732\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 4 ------------\n",
            "Batch #500 Loss: 4.5456001672744755\n",
            "Batch #1000 Loss: 4.494837258338928\n",
            "Batch #1500 Loss: 4.486348701953888\n",
            "Batch #2000 Loss: 4.487645203590393\n",
            "\u001b[92mTrain perplexity: 87.292133 ||| loss 4.469260\u001b[0m\n",
            "\u001b[92mValidation perplexity: 135.327869 ||| loss 4.907700\u001b[0m\n",
            "\u001b[92mTest perplexity: 132.047492 ||| loss 4.883162\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 4 ------------\n",
            "Batch #500 Loss: 4.471367019176483\n",
            "Batch #1000 Loss: 4.420014813899994\n",
            "Batch #1500 Loss: 4.41467496585846\n",
            "Batch #2000 Loss: 4.416802980422974\n",
            "\u001b[92mTrain perplexity: 81.925785 ||| loss 4.405814\u001b[0m\n",
            "\u001b[92mValidation perplexity: 134.102498 ||| loss 4.898604\u001b[0m\n",
            "\u001b[92mTest perplexity: 130.962141 ||| loss 4.874908\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 2.0 ------------\n",
            "Batch #500 Loss: 4.383173785209656\n",
            "Batch #1000 Loss: 4.298415382385254\n",
            "Batch #1500 Loss: 4.270872814655304\n",
            "Batch #2000 Loss: 4.254927682876587\n",
            "\u001b[92mTrain perplexity: 71.857896 ||| loss 4.274691\u001b[0m\n",
            "\u001b[92mValidation perplexity: 129.505050 ||| loss 4.863720\u001b[0m\n",
            "\u001b[92mTest perplexity: 126.213051 ||| loss 4.837971\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 1.0 ------------\n",
            "Batch #500 Loss: 4.29715141916275\n",
            "Batch #1000 Loss: 4.212103711605072\n",
            "Batch #1500 Loss: 4.179617018699646\n",
            "Batch #2000 Loss: 4.158776790142059\n",
            "\u001b[92mTrain perplexity: 66.302919 ||| loss 4.194234\u001b[0m\n",
            "\u001b[92mValidation perplexity: 127.869577 ||| loss 4.851011\u001b[0m\n",
            "\u001b[92mTest perplexity: 124.129267 ||| loss 4.821324\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 0.5 ------------\n",
            "Batch #500 Loss: 4.247687514305115\n",
            "Batch #1000 Loss: 4.16312681722641\n",
            "Batch #1500 Loss: 4.12942155456543\n",
            "Batch #2000 Loss: 4.1070581865310665\n",
            "\u001b[92mTrain perplexity: 63.138532 ||| loss 4.145331\u001b[0m\n",
            "\u001b[92mValidation perplexity: 126.955453 ||| loss 4.843836\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.997843 ||| loss 4.812167\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 0.25 ------------\n",
            "Batch #500 Loss: 4.220257702350616\n",
            "Batch #1000 Loss: 4.137447773933411\n",
            "Batch #1500 Loss: 4.102935510158539\n",
            "Batch #2000 Loss: 4.0797562069892885\n",
            "\u001b[92mTrain perplexity: 61.226368 ||| loss 4.114578\u001b[0m\n",
            "\u001b[92mValidation perplexity: 126.268721 ||| loss 4.838412\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.204430 ||| loss 4.805695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b260305942349788e73dbcd2229dca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▆▅▄▄▃▃▃▂▂▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.8057</td></tr><tr><td>Test-perplexity</td><td>122.20443</td></tr><tr><td>Train-loss</td><td>4.11458</td></tr><tr><td>Train-perplexity</td><td>61.22637</td></tr><tr><td>Validation-loss</td><td>4.83841</td></tr><tr><td>Validation-perplexity</td><td>126.26872</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pious-snowball-8</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/9x4ttaq6' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/9x4ttaq6</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240926_020016-9x4ttaq6/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save and Test Example"
      ],
      "metadata": {
        "id": "0AJh_GcvIh2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving\n",
        "torch.save(model.state_dict(), './models/lstm_noreg.pth')"
      ],
      "metadata": {
        "id": "aW4s0L-iDtzk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "sample = 'the financial outlook has become strong for a company that has had a tough'\n",
        "model = ZamrembaRNN('lstm', len(train.vocab)).to(device)\n",
        "model.load_state_dict(torch.load('./models/lstm_noreg.pth', weights_only=True))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    tokens = train.vocab.encode(sample.split())\n",
        "    inputs = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "    hidden = (torch.zeros(model.num_layers, 1, model.hidden_dim).to(device),\n",
        "              torch.zeros(model.num_layers, 1, model.hidden_dim).to(device))\n",
        "    outputs, hidden = model(inputs, hidden)\n",
        "\n",
        "    prediction = torch.argmax(outputs, dim=-1)\n",
        "    print(\" \".join(train.vocab.decode(prediction.squeeze().tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgDNRYFgFgM3",
        "outputId": "5853e7ce-8169-4752-dd94-b221908ca0f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "company times for been a <eos> the year <eos> would been a N impact\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM with Dropout"
      ],
      "metadata": {
        "id": "UQDV3GOtLZml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 11\n",
        "learning_rate_decay = 0.75\n",
        "lr = 6\n",
        "dropout_rate = 0.5\n",
        "lstm_dropout = 0.2\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('lstm', len(train.vocab), dropout=dropout_rate, rnn_dropout=lstm_dropout).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-quad\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'lstm_dropout':lstm_dropout,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 25, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c346ab8ab4474975826f4870034022ca",
            "4ff922dcaf774532b3c59e1f868a8947",
            "2f2e537db4bf4c6db4b242e3fe49d489",
            "06878bab4d4f460c84d744bbe0acb44b",
            "120afb4fb1154133913f397934b73dab",
            "7d2b69941fd34fc390b4265eaee428f3",
            "9a3b08b3cb04485a9d99f6fe1f33024d",
            "9b2d07916b654d668c3c1a66758af01f",
            "c7ec2c84bfa2410dacb4d85de8d6d471",
            "217eca5728d44720b3423387359af7f0",
            "94df51205539495db6d2d07b4d40cede",
            "b119df608d7d4b5f90c9d3632f961b50",
            "c84f2a4775da4ffe9e662bccdfa5ff6f",
            "bc3c4028e02f4b5faf7a263c0dafe234",
            "4aaff92ed6d54975a2def9bad0946862",
            "c88c3245cb9b4675847f543b64b3ceef"
          ]
        },
        "id": "baKhTg-1LYrI",
        "outputId": "275f363c-d0bd-4a51-81b5-cc4e2520fd40"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:tsdikwh4) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c346ab8ab4474975826f4870034022ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▅▄▃▃▂▂▁</td></tr><tr><td>Test-perplexity</td><td>█▆▄▃▂▃▂▁▁</td></tr><tr><td>Train-loss</td><td>█▆▅▄▃▃▂▂▁</td></tr><tr><td>Train-perplexity</td><td>█▆▄▃▃▂▂▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▅▄▃▃▂▂▁</td></tr><tr><td>Validation-perplexity</td><td>█▆▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>5.78995</td></tr><tr><td>Test-perplexity</td><td>326.99583</td></tr><tr><td>Train-loss</td><td>5.74052</td></tr><tr><td>Train-perplexity</td><td>311.22588</td></tr><tr><td>Validation-loss</td><td>5.81553</td></tr><tr><td>Validation-perplexity</td><td>335.47018</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">tough-tree-57</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/tsdikwh4' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/tsdikwh4</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240926_023417-tsdikwh4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:tsdikwh4). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240926_023700-oadykdza</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/oadykdza' target=\"_blank\">fluent-jazz-58</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/oadykdza' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/oadykdza</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 6 ------------\n",
            "Batch #500 Loss: 6.715802015304566\n",
            "Batch #1000 Loss: 6.174229908943176\n",
            "Batch #1500 Loss: 5.916929319381714\n",
            "Batch #2000 Loss: 5.751367631912231\n",
            "\u001b[92mTrain perplexity: 245.219109 ||| loss 5.502152\u001b[0m\n",
            "\u001b[92mValidation perplexity: 253.499155 ||| loss 5.535360\u001b[0m\n",
            "\u001b[92mTest perplexity: 248.927253 ||| loss 5.517161\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 6 ------------\n",
            "Batch #500 Loss: 5.580333048820496\n",
            "Batch #1000 Loss: 5.504081024169922\n",
            "Batch #1500 Loss: 5.426043882369995\n",
            "Batch #2000 Loss: 5.372735411643982\n",
            "\u001b[92mTrain perplexity: 172.691535 ||| loss 5.151507\u001b[0m\n",
            "\u001b[92mValidation perplexity: 188.944896 ||| loss 5.241455\u001b[0m\n",
            "\u001b[92mTest perplexity: 184.678262 ||| loss 5.218615\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 6 ------------\n",
            "Batch #500 Loss: 5.304149119377136\n",
            "Batch #1000 Loss: 5.26002091217041\n",
            "Batch #1500 Loss: 5.2167480230331424\n",
            "Batch #2000 Loss: 5.190609329223633\n",
            "\u001b[92mTrain perplexity: 141.581288 ||| loss 4.952874\u001b[0m\n",
            "\u001b[92mValidation perplexity: 161.570155 ||| loss 5.084939\u001b[0m\n",
            "\u001b[92mTest perplexity: 158.323721 ||| loss 5.064642\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 6 ------------\n",
            "Batch #500 Loss: 5.150377219200134\n",
            "Batch #1000 Loss: 5.115630170822143\n",
            "Batch #1500 Loss: 5.086976424217224\n",
            "Batch #2000 Loss: 5.070918804168701\n",
            "\u001b[92mTrain perplexity: 124.306165 ||| loss 4.822748\u001b[0m\n",
            "\u001b[92mValidation perplexity: 147.010861 ||| loss 4.990506\u001b[0m\n",
            "\u001b[92mTest perplexity: 143.792772 ||| loss 4.968373\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 6 ------------\n",
            "Batch #500 Loss: 5.048728226661682\n",
            "Batch #1000 Loss: 5.012372077941895\n",
            "Batch #1500 Loss: 4.9947995767593385\n",
            "Batch #2000 Loss: 4.986089521408081\n",
            "\u001b[92mTrain perplexity: 111.494230 ||| loss 4.713973\u001b[0m\n",
            "\u001b[92mValidation perplexity: 136.108902 ||| loss 4.913455\u001b[0m\n",
            "\u001b[92mTest perplexity: 133.183469 ||| loss 4.891728\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 6 ------------\n",
            "Batch #500 Loss: 4.9722390546798705\n",
            "Batch #1000 Loss: 4.937046585083007\n",
            "Batch #1500 Loss: 4.926299643516541\n",
            "Batch #2000 Loss: 4.920165413856506\n",
            "\u001b[92mTrain perplexity: 102.728962 ||| loss 4.632094\u001b[0m\n",
            "\u001b[92mValidation perplexity: 128.938320 ||| loss 4.859334\u001b[0m\n",
            "\u001b[92mTest perplexity: 126.488248 ||| loss 4.840149\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 6 ------------\n",
            "Batch #500 Loss: 4.913687258720398\n",
            "Batch #1000 Loss: 4.878606266975403\n",
            "Batch #1500 Loss: 4.87154206943512\n",
            "Batch #2000 Loss: 4.872706231117249\n",
            "\u001b[92mTrain perplexity: 96.361026 ||| loss 4.568102\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.075344 ||| loss 4.820889\u001b[0m\n",
            "\u001b[92mTest perplexity: 121.532576 ||| loss 4.800182\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 6 ------------\n",
            "Batch #500 Loss: 4.865257972717285\n",
            "Batch #1000 Loss: 4.827260082244873\n",
            "Batch #1500 Loss: 4.824097198486328\n",
            "Batch #2000 Loss: 4.828004370689392\n",
            "\u001b[92mTrain perplexity: 91.191722 ||| loss 4.512964\u001b[0m\n",
            "\u001b[92mValidation perplexity: 120.371680 ||| loss 4.790584\u001b[0m\n",
            "\u001b[92mTest perplexity: 118.121428 ||| loss 4.771713\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 6 ------------\n",
            "Batch #500 Loss: 4.8246146020889285\n",
            "Batch #1000 Loss: 4.790309258460999\n",
            "Batch #1500 Loss: 4.785930567741394\n",
            "Batch #2000 Loss: 4.7917701463699345\n",
            "\u001b[92mTrain perplexity: 86.859857 ||| loss 4.464296\u001b[0m\n",
            "\u001b[92mValidation perplexity: 117.912091 ||| loss 4.769939\u001b[0m\n",
            "\u001b[92mTest perplexity: 115.014509 ||| loss 4.745058\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 6 ------------\n",
            "Batch #500 Loss: 4.790302505493164\n",
            "Batch #1000 Loss: 4.75526812839508\n",
            "Batch #1500 Loss: 4.753057251930237\n",
            "Batch #2000 Loss: 4.762611187934875\n",
            "\u001b[92mTrain perplexity: 83.261083 ||| loss 4.421981\u001b[0m\n",
            "\u001b[92mValidation perplexity: 114.905302 ||| loss 4.744108\u001b[0m\n",
            "\u001b[92mTest perplexity: 112.153247 ||| loss 4.719866\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 6 ------------\n",
            "Batch #500 Loss: 4.7625207705497745\n",
            "Batch #1000 Loss: 4.721354150772095\n",
            "Batch #1500 Loss: 4.728633548259735\n",
            "Batch #2000 Loss: 4.73682531452179\n",
            "\u001b[92mTrain perplexity: 80.132431 ||| loss 4.383681\u001b[0m\n",
            "\u001b[92mValidation perplexity: 112.557509 ||| loss 4.723464\u001b[0m\n",
            "\u001b[92mTest perplexity: 110.153045 ||| loss 4.701871\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 4.5 ------------\n",
            "Batch #500 Loss: 4.721889142990112\n",
            "Batch #1000 Loss: 4.675824855804444\n",
            "Batch #1500 Loss: 4.672105610847473\n",
            "Batch #2000 Loss: 4.677983141899109\n",
            "\u001b[92mTrain perplexity: 75.347292 ||| loss 4.322108\u001b[0m\n",
            "\u001b[92mValidation perplexity: 108.898046 ||| loss 4.690412\u001b[0m\n",
            "\u001b[92mTest perplexity: 106.365770 ||| loss 4.666884\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 3.375 ------------\n",
            "Batch #500 Loss: 4.678678594112396\n",
            "Batch #1000 Loss: 4.628295073986053\n",
            "Batch #1500 Loss: 4.6211633796691896\n",
            "Batch #2000 Loss: 4.630164286136627\n",
            "\u001b[92mTrain perplexity: 71.533633 ||| loss 4.270168\u001b[0m\n",
            "\u001b[92mValidation perplexity: 106.212963 ||| loss 4.665446\u001b[0m\n",
            "\u001b[92mTest perplexity: 103.653337 ||| loss 4.641052\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 2.53125 ------------\n",
            "Batch #500 Loss: 4.645057863235474\n",
            "Batch #1000 Loss: 4.591534541606903\n",
            "Batch #1500 Loss: 4.58590846157074\n",
            "Batch #2000 Loss: 4.589065101146698\n",
            "\u001b[92mTrain perplexity: 68.759356 ||| loss 4.230613\u001b[0m\n",
            "\u001b[92mValidation perplexity: 104.046907 ||| loss 4.644842\u001b[0m\n",
            "\u001b[92mTest perplexity: 101.504293 ||| loss 4.620101\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15, LR: 1.8984375 ------------\n",
            "Batch #500 Loss: 4.613668557643891\n",
            "Batch #1000 Loss: 4.564731673717499\n",
            "Batch #1500 Loss: 4.5548930845260625\n",
            "Batch #2000 Loss: 4.5653936724662785\n",
            "\u001b[92mTrain perplexity: 66.867761 ||| loss 4.202717\u001b[0m\n",
            "\u001b[92mValidation perplexity: 102.878057 ||| loss 4.633544\u001b[0m\n",
            "\u001b[92mTest perplexity: 100.345966 ||| loss 4.608624\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16, LR: 1.423828125 ------------\n",
            "Batch #500 Loss: 4.590275894641876\n",
            "Batch #1000 Loss: 4.539573567390442\n",
            "Batch #1500 Loss: 4.535734784603119\n",
            "Batch #2000 Loss: 4.537842315196991\n",
            "\u001b[92mTrain perplexity: 65.255525 ||| loss 4.178311\u001b[0m\n",
            "\u001b[92mValidation perplexity: 101.853357 ||| loss 4.623534\u001b[0m\n",
            "\u001b[92mTest perplexity: 98.868137 ||| loss 4.593787\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17, LR: 1.06787109375 ------------\n",
            "Batch #500 Loss: 4.574289507865906\n",
            "Batch #1000 Loss: 4.523776907920837\n",
            "Batch #1500 Loss: 4.516800189495087\n",
            "Batch #2000 Loss: 4.523490722179413\n",
            "\u001b[92mTrain perplexity: 64.124337 ||| loss 4.160824\u001b[0m\n",
            "\u001b[92mValidation perplexity: 101.091324 ||| loss 4.616024\u001b[0m\n",
            "\u001b[92mTest perplexity: 98.305233 ||| loss 4.588077\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18, LR: 0.8009033203125 ------------\n",
            "Batch #500 Loss: 4.561330324649811\n",
            "Batch #1000 Loss: 4.506555796146393\n",
            "Batch #1500 Loss: 4.501532165527344\n",
            "Batch #2000 Loss: 4.506680952072143\n",
            "\u001b[92mTrain perplexity: 63.257343 ||| loss 4.147211\u001b[0m\n",
            "\u001b[92mValidation perplexity: 100.612050 ||| loss 4.611272\u001b[0m\n",
            "\u001b[92mTest perplexity: 97.602303 ||| loss 4.580901\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19, LR: 0.600677490234375 ------------\n",
            "Batch #500 Loss: 4.548927993774414\n",
            "Batch #1000 Loss: 4.493200227737427\n",
            "Batch #1500 Loss: 4.491654128074646\n",
            "Batch #2000 Loss: 4.494504930973053\n",
            "\u001b[92mTrain perplexity: 62.544810 ||| loss 4.135883\u001b[0m\n",
            "\u001b[92mValidation perplexity: 100.200142 ||| loss 4.607170\u001b[0m\n",
            "\u001b[92mTest perplexity: 97.079398 ||| loss 4.575529\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20, LR: 0.45050811767578125 ------------\n",
            "Batch #500 Loss: 4.5419037771224975\n",
            "Batch #1000 Loss: 4.488370267391205\n",
            "Batch #1500 Loss: 4.484553546905517\n",
            "Batch #2000 Loss: 4.488819628238678\n",
            "\u001b[92mTrain perplexity: 62.126721 ||| loss 4.129176\u001b[0m\n",
            "\u001b[92mValidation perplexity: 99.812551 ||| loss 4.603294\u001b[0m\n",
            "\u001b[92mTest perplexity: 96.678456 ||| loss 4.571391\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21, LR: 0.33788108825683594 ------------\n",
            "Batch #500 Loss: 4.5382232007980345\n",
            "Batch #1000 Loss: 4.482577589511871\n",
            "Batch #1500 Loss: 4.47735910987854\n",
            "Batch #2000 Loss: 4.482152131080627\n",
            "\u001b[92mTrain perplexity: 61.769199 ||| loss 4.123405\u001b[0m\n",
            "\u001b[92mValidation perplexity: 99.626911 ||| loss 4.601432\u001b[0m\n",
            "\u001b[92mTest perplexity: 96.404106 ||| loss 4.568549\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22, LR: 0.25341081619262695 ------------\n",
            "Batch #500 Loss: 4.529885790348053\n",
            "Batch #1000 Loss: 4.476982358455658\n",
            "Batch #1500 Loss: 4.47254176902771\n",
            "Batch #2000 Loss: 4.479150711536407\n",
            "\u001b[92mTrain perplexity: 61.462798 ||| loss 4.118432\u001b[0m\n",
            "\u001b[92mValidation perplexity: 99.507679 ||| loss 4.600235\u001b[0m\n",
            "\u001b[92mTest perplexity: 96.197483 ||| loss 4.566403\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23, LR: 0.19005811214447021 ------------\n",
            "Batch #500 Loss: 4.526513960838318\n",
            "Batch #1000 Loss: 4.469308111667633\n",
            "Batch #1500 Loss: 4.46981530380249\n",
            "Batch #2000 Loss: 4.477700016975403\n",
            "\u001b[92mTrain perplexity: 61.273739 ||| loss 4.115351\u001b[0m\n",
            "\u001b[92mValidation perplexity: 99.373198 ||| loss 4.598882\u001b[0m\n",
            "\u001b[92mTest perplexity: 95.988549 ||| loss 4.564229\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24, LR: 0.14254358410835266 ------------\n",
            "Batch #500 Loss: 4.522658406734466\n",
            "Batch #1000 Loss: 4.473413990974426\n",
            "Batch #1500 Loss: 4.468973248958588\n",
            "Batch #2000 Loss: 4.473736958503723\n",
            "\u001b[92mTrain perplexity: 61.124577 ||| loss 4.112914\u001b[0m\n",
            "\u001b[92mValidation perplexity: 99.237220 ||| loss 4.597513\u001b[0m\n",
            "\u001b[92mTest perplexity: 95.898314 ||| loss 4.563288\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25, LR: 0.1069076880812645 ------------\n",
            "Batch #500 Loss: 4.520719645500183\n",
            "Batch #1000 Loss: 4.47157303237915\n",
            "Batch #1500 Loss: 4.464620359897614\n",
            "Batch #2000 Loss: 4.4725984230041504\n",
            "\u001b[92mTrain perplexity: 61.029376 ||| loss 4.111355\u001b[0m\n",
            "\u001b[92mValidation perplexity: 99.170992 ||| loss 4.596846\u001b[0m\n",
            "\u001b[92mTest perplexity: 95.794478 ||| loss 4.562205\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7ec2c84bfa2410dacb4d85de8d6d471"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.56221</td></tr><tr><td>Test-perplexity</td><td>95.79448</td></tr><tr><td>Train-loss</td><td>4.11136</td></tr><tr><td>Train-perplexity</td><td>61.02938</td></tr><tr><td>Validation-loss</td><td>4.59685</td></tr><tr><td>Validation-perplexity</td><td>99.17099</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fluent-jazz-58</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/oadykdza' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/oadykdza</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240926_023700-oadykdza/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './models/lstm_drop.pth')"
      ],
      "metadata": {
        "id": "3Ku18WyrIvgf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU No Regularization"
      ],
      "metadata": {
        "id": "p_T8dAqcLosM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 6\n",
        "learning_rate_decay = 0.5\n",
        "lr = 1\n",
        "dropout_rate = 0\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('gru', len(train.vocab)).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-quad\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 14, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0ff1355693ba4a9aa80e6a07dd5b0112",
            "82ca31ae1f6c4c2fafec02ca6de89fd7",
            "7bc0e49dcf8b4afa8470989005dcabe2",
            "02a55334a4954304bdd3e8b7aa803a25",
            "337623417eeb4758bb6963a67a653b21",
            "86be621388144bbc9bec8536250d0afb",
            "709f1f6335254b7a83e21daf16b2ab5a",
            "e6d1409497814256b4365b97fcb90da7"
          ]
        },
        "id": "8VG4kuatBR_r",
        "outputId": "525978d7-5ab9-44e9-b531-5e4222956610"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240926_024442-nk5si5zy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/nk5si5zy' target=\"_blank\">atomic-dust-59</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/nk5si5zy' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/nk5si5zy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 1 ------------\n",
            "Batch #500 Loss: 6.746936987876892\n",
            "Batch #1000 Loss: 6.250378631591797\n",
            "Batch #1500 Loss: 5.963424120903015\n",
            "Batch #2000 Loss: 5.783058778762817\n",
            "\u001b[92mTrain perplexity: 276.287559 ||| loss 5.621442\u001b[0m\n",
            "\u001b[92mValidation perplexity: 283.033889 ||| loss 5.645567\u001b[0m\n",
            "\u001b[92mTest perplexity: 276.072023 ||| loss 5.620662\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 1 ------------\n",
            "Batch #500 Loss: 5.577802412033081\n",
            "Batch #1000 Loss: 5.488571425437927\n",
            "Batch #1500 Loss: 5.389683073043823\n",
            "Batch #2000 Loss: 5.3236801404953\n",
            "\u001b[92mTrain perplexity: 190.024128 ||| loss 5.247151\u001b[0m\n",
            "\u001b[92mValidation perplexity: 208.749742 ||| loss 5.341136\u001b[0m\n",
            "\u001b[92mTest perplexity: 203.273951 ||| loss 5.314555\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 1 ------------\n",
            "Batch #500 Loss: 5.230614698410034\n",
            "Batch #1000 Loss: 5.177843736648559\n",
            "Batch #1500 Loss: 5.1162847738265995\n",
            "Batch #2000 Loss: 5.0796004562377925\n",
            "\u001b[92mTrain perplexity: 151.169566 ||| loss 5.018402\u001b[0m\n",
            "\u001b[92mValidation perplexity: 176.822399 ||| loss 5.175146\u001b[0m\n",
            "\u001b[92mTest perplexity: 171.744268 ||| loss 5.146007\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 1 ------------\n",
            "Batch #500 Loss: 5.018083183288574\n",
            "Batch #1000 Loss: 4.97202623462677\n",
            "Batch #1500 Loss: 4.926371955871582\n",
            "Batch #2000 Loss: 4.903799425125122\n",
            "\u001b[92mTrain perplexity: 127.849390 ||| loss 4.850853\u001b[0m\n",
            "\u001b[92mValidation perplexity: 159.204624 ||| loss 5.070190\u001b[0m\n",
            "\u001b[92mTest perplexity: 154.361356 ||| loss 5.039296\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 1 ------------\n",
            "Batch #500 Loss: 4.857710514068604\n",
            "Batch #1000 Loss: 4.8117274494171145\n",
            "Batch #1500 Loss: 4.776608828544616\n",
            "Batch #2000 Loss: 4.762691987037659\n",
            "\u001b[92mTrain perplexity: 111.615685 ||| loss 4.715062\u001b[0m\n",
            "\u001b[92mValidation perplexity: 148.187781 ||| loss 4.998480\u001b[0m\n",
            "\u001b[92mTest perplexity: 143.475711 ||| loss 4.966166\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 1 ------------\n",
            "Batch #500 Loss: 4.7255857391357425\n",
            "Batch #1000 Loss: 4.678293148994446\n",
            "Batch #1500 Loss: 4.650533174037934\n",
            "Batch #2000 Loss: 4.643100369930267\n",
            "\u001b[92mTrain perplexity: 99.690096 ||| loss 4.602066\u001b[0m\n",
            "\u001b[92mValidation perplexity: 141.196043 ||| loss 4.950149\u001b[0m\n",
            "\u001b[92mTest perplexity: 136.655401 ||| loss 4.917462\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 0.5 ------------\n",
            "Batch #500 Loss: 4.592914761543274\n",
            "Batch #1000 Loss: 4.526511380195617\n",
            "Batch #1500 Loss: 4.493720716953278\n",
            "Batch #2000 Loss: 4.480194727420807\n",
            "\u001b[92mTrain perplexity: 88.156863 ||| loss 4.479118\u001b[0m\n",
            "\u001b[92mValidation perplexity: 132.177318 ||| loss 4.884144\u001b[0m\n",
            "\u001b[92mTest perplexity: 127.743066 ||| loss 4.850021\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 0.25 ------------\n",
            "Batch #500 Loss: 4.51072280883789\n",
            "Batch #1000 Loss: 4.444576416492462\n",
            "Batch #1500 Loss: 4.41261588191986\n",
            "Batch #2000 Loss: 4.397146368980407\n",
            "\u001b[92mTrain perplexity: 81.952902 ||| loss 4.406145\u001b[0m\n",
            "\u001b[92mValidation perplexity: 127.246023 ||| loss 4.846122\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.747019 ||| loss 4.810125\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 0.125 ------------\n",
            "Batch #500 Loss: 4.465306910037994\n",
            "Batch #1000 Loss: 4.400420329093933\n",
            "Batch #1500 Loss: 4.369666408061981\n",
            "Batch #2000 Loss: 4.353348619937897\n",
            "\u001b[92mTrain perplexity: 78.740052 ||| loss 4.366152\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.694371 ||| loss 4.825866\u001b[0m\n",
            "\u001b[92mTest perplexity: 120.171033 ||| loss 4.788916\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 0.0625 ------------\n",
            "Batch #500 Loss: 4.440652503490448\n",
            "Batch #1000 Loss: 4.376888894081116\n",
            "Batch #1500 Loss: 4.3469574418067936\n",
            "Batch #2000 Loss: 4.330099240779877\n",
            "\u001b[92mTrain perplexity: 77.220526 ||| loss 4.346665\u001b[0m\n",
            "\u001b[92mValidation perplexity: 123.581618 ||| loss 4.816902\u001b[0m\n",
            "\u001b[92mTest perplexity: 118.981236 ||| loss 4.778966\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 0.03125 ------------\n",
            "Batch #500 Loss: 4.42747218799591\n",
            "Batch #1000 Loss: 4.3644394798278805\n",
            "Batch #1500 Loss: 4.334924816131592\n",
            "Batch #2000 Loss: 4.317654565811157\n",
            "\u001b[92mTrain perplexity: 76.484948 ||| loss 4.337094\u001b[0m\n",
            "\u001b[92mValidation perplexity: 123.047937 ||| loss 4.812574\u001b[0m\n",
            "\u001b[92mTest perplexity: 118.319618 ||| loss 4.773390\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 0.015625 ------------\n",
            "Batch #500 Loss: 4.420421395778656\n",
            "Batch #1000 Loss: 4.35783311700821\n",
            "Batch #1500 Loss: 4.328587474822998\n",
            "Batch #2000 Loss: 4.310978937625885\n",
            "\u001b[92mTrain perplexity: 76.130643 ||| loss 4.332451\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.764738 ||| loss 4.810270\u001b[0m\n",
            "\u001b[92mTest perplexity: 117.939244 ||| loss 4.770170\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 0.0078125 ------------\n",
            "Batch #500 Loss: 4.4166661324501035\n",
            "Batch #1000 Loss: 4.354323449134827\n",
            "Batch #1500 Loss: 4.325295361995697\n",
            "Batch #2000 Loss: 4.307417016029358\n",
            "\u001b[92mTrain perplexity: 75.968545 ||| loss 4.330319\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.639733 ||| loss 4.809251\u001b[0m\n",
            "\u001b[92mTest perplexity: 117.767249 ||| loss 4.768710\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 0.00390625 ------------\n",
            "Batch #500 Loss: 4.414683834552765\n",
            "Batch #1000 Loss: 4.352493472576142\n",
            "Batch #1500 Loss: 4.323551053524017\n",
            "Batch #2000 Loss: 4.305504279613495\n",
            "\u001b[92mTrain perplexity: 75.895620 ||| loss 4.329359\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.583512 ||| loss 4.808793\u001b[0m\n",
            "\u001b[92mTest perplexity: 117.697603 ||| loss 4.768119\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ff1355693ba4a9aa80e6a07dd5b0112"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▅▄▃▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▅▄▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▅▄▃▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.76812</td></tr><tr><td>Test-perplexity</td><td>117.6976</td></tr><tr><td>Train-loss</td><td>4.32936</td></tr><tr><td>Train-perplexity</td><td>75.89562</td></tr><tr><td>Validation-loss</td><td>4.80879</td></tr><tr><td>Validation-perplexity</td><td>122.58351</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">atomic-dust-59</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/nk5si5zy' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/nk5si5zy</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240926_024442-nk5si5zy/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './models/gru_noreg.pth')"
      ],
      "metadata": {
        "id": "ZmZB-avRPm1R"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU with Dropout"
      ],
      "metadata": {
        "id": "V9EdqsJCLv1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 20\n",
        "learning_rate_decay = 0.75\n",
        "lr = 1\n",
        "dropout_rate = 0.5\n",
        "gru_dropout = 0.2\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('gru', len(train.vocab), dropout=dropout_rate, rnn_dropout=gru_dropout).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-quad\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'lstm_dropout':gru_dropout,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 25, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "09e8200fdab94d768427bd16aad636c9",
            "8f1710f62e364d04869ab6ff149a9fd3",
            "769295408fdc421b83c1ae610aedfef1",
            "ea7dd40e203d4c6fb56e6921d1e18c4f",
            "5319b520c3a84b858261417da3e5b48a",
            "47406269ec134df8aa8c2583305f93e9",
            "82b58bfca1dc431bbd6aeb4a26234cfd",
            "5d05d30b4abe4540bd59dcfa5a7334da"
          ]
        },
        "id": "kXwylxgxLNEA",
        "outputId": "3f60015b-c488-4373-c174-064313e30ddd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240926_025244-qlelgi3q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/qlelgi3q' target=\"_blank\">peachy-water-9</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/qlelgi3q' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/qlelgi3q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 1 ------------\n",
            "Batch #500 Loss: 6.797517258644104\n",
            "Batch #1000 Loss: 6.367005604743958\n",
            "Batch #1500 Loss: 6.148962080001831\n",
            "Batch #2000 Loss: 6.013281662940979\n",
            "\u001b[92mTrain perplexity: 323.513344 ||| loss 5.779240\u001b[0m\n",
            "\u001b[92mValidation perplexity: 326.051946 ||| loss 5.787057\u001b[0m\n",
            "\u001b[92mTest perplexity: 317.177546 ||| loss 5.759462\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 1 ------------\n",
            "Batch #500 Loss: 5.855956418037414\n",
            "Batch #1000 Loss: 5.791939136505127\n",
            "Batch #1500 Loss: 5.711890823364258\n",
            "Batch #2000 Loss: 5.662339807510376\n",
            "\u001b[92mTrain perplexity: 232.388096 ||| loss 5.448409\u001b[0m\n",
            "\u001b[92mValidation perplexity: 243.006195 ||| loss 5.493087\u001b[0m\n",
            "\u001b[92mTest perplexity: 235.889802 ||| loss 5.463365\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 1 ------------\n",
            "Batch #500 Loss: 5.592837042808533\n",
            "Batch #1000 Loss: 5.5585172700881955\n",
            "Batch #1500 Loss: 5.510842499732971\n",
            "Batch #2000 Loss: 5.479744264602661\n",
            "\u001b[92mTrain perplexity: 195.891837 ||| loss 5.277563\u001b[0m\n",
            "\u001b[92mValidation perplexity: 211.678643 ||| loss 5.355069\u001b[0m\n",
            "\u001b[92mTest perplexity: 205.846185 ||| loss 5.327129\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 1 ------------\n",
            "Batch #500 Loss: 5.437066217422485\n",
            "Batch #1000 Loss: 5.412470933914185\n",
            "Batch #1500 Loss: 5.37554295539856\n",
            "Batch #2000 Loss: 5.356618370056152\n",
            "\u001b[92mTrain perplexity: 169.106101 ||| loss 5.130526\u001b[0m\n",
            "\u001b[92mValidation perplexity: 187.575800 ||| loss 5.234183\u001b[0m\n",
            "\u001b[92mTest perplexity: 182.284037 ||| loss 5.205566\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 1 ------------\n",
            "Batch #500 Loss: 5.324655474662781\n",
            "Batch #1000 Loss: 5.3004812269210815\n",
            "Batch #1500 Loss: 5.276010380744934\n",
            "Batch #2000 Loss: 5.260523550033569\n",
            "\u001b[92mTrain perplexity: 151.094689 ||| loss 5.017907\u001b[0m\n",
            "\u001b[92mValidation perplexity: 172.111080 ||| loss 5.148140\u001b[0m\n",
            "\u001b[92mTest perplexity: 167.339107 ||| loss 5.120022\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 1 ------------\n",
            "Batch #500 Loss: 5.239964010238648\n",
            "Batch #1000 Loss: 5.215351406097412\n",
            "Batch #1500 Loss: 5.194740656852722\n",
            "Batch #2000 Loss: 5.181976194381714\n",
            "\u001b[92mTrain perplexity: 137.018660 ||| loss 4.920117\u001b[0m\n",
            "\u001b[92mValidation perplexity: 159.552198 ||| loss 5.072371\u001b[0m\n",
            "\u001b[92mTest perplexity: 155.798513 ||| loss 5.048564\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 1 ------------\n",
            "Batch #500 Loss: 5.166639088630676\n",
            "Batch #1000 Loss: 5.141274247169495\n",
            "Batch #1500 Loss: 5.126710821151733\n",
            "Batch #2000 Loss: 5.121401114463806\n",
            "\u001b[92mTrain perplexity: 127.036876 ||| loss 4.844477\u001b[0m\n",
            "\u001b[92mValidation perplexity: 151.681158 ||| loss 5.021781\u001b[0m\n",
            "\u001b[92mTest perplexity: 148.308467 ||| loss 4.999294\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 1 ------------\n",
            "Batch #500 Loss: 5.104573237419128\n",
            "Batch #1000 Loss: 5.084654794692993\n",
            "Batch #1500 Loss: 5.0672662029266355\n",
            "Batch #2000 Loss: 5.0637342014312745\n",
            "\u001b[92mTrain perplexity: 119.301543 ||| loss 4.781654\u001b[0m\n",
            "\u001b[92mValidation perplexity: 145.550658 ||| loss 4.980524\u001b[0m\n",
            "\u001b[92mTest perplexity: 142.128730 ||| loss 4.956733\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 1 ------------\n",
            "Batch #500 Loss: 5.0544045610427855\n",
            "Batch #1000 Loss: 5.028558453559875\n",
            "Batch #1500 Loss: 5.01801000213623\n",
            "Batch #2000 Loss: 5.014636217117309\n",
            "\u001b[92mTrain perplexity: 112.104037 ||| loss 4.719427\u001b[0m\n",
            "\u001b[92mValidation perplexity: 140.102261 ||| loss 4.942373\u001b[0m\n",
            "\u001b[92mTest perplexity: 136.665995 ||| loss 4.917540\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 1 ------------\n",
            "Batch #500 Loss: 5.003770101547241\n",
            "Batch #1000 Loss: 4.982720480918884\n",
            "Batch #1500 Loss: 4.974564330101013\n",
            "Batch #2000 Loss: 4.978257697105407\n",
            "\u001b[92mTrain perplexity: 106.074463 ||| loss 4.664141\u001b[0m\n",
            "\u001b[92mValidation perplexity: 135.518275 ||| loss 4.909107\u001b[0m\n",
            "\u001b[92mTest perplexity: 132.155607 ||| loss 4.883980\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 1 ------------\n",
            "Batch #500 Loss: 4.968041541099549\n",
            "Batch #1000 Loss: 4.9389799127578735\n",
            "Batch #1500 Loss: 4.934016255378723\n",
            "Batch #2000 Loss: 4.938952889442444\n",
            "\u001b[92mTrain perplexity: 101.133162 ||| loss 4.616438\u001b[0m\n",
            "\u001b[92mValidation perplexity: 131.481839 ||| loss 4.878869\u001b[0m\n",
            "\u001b[92mTest perplexity: 128.621219 ||| loss 4.856872\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 1 ------------\n",
            "Batch #500 Loss: 4.931211761474609\n",
            "Batch #1000 Loss: 4.900438928604126\n",
            "Batch #1500 Loss: 4.89693929195404\n",
            "Batch #2000 Loss: 4.901623105049134\n",
            "\u001b[92mTrain perplexity: 96.874697 ||| loss 4.573418\u001b[0m\n",
            "\u001b[92mValidation perplexity: 128.732713 ||| loss 4.857738\u001b[0m\n",
            "\u001b[92mTest perplexity: 125.658692 ||| loss 4.833569\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 1 ------------\n",
            "Batch #500 Loss: 4.895583441734314\n",
            "Batch #1000 Loss: 4.869316390037537\n",
            "Batch #1500 Loss: 4.867301015853882\n",
            "Batch #2000 Loss: 4.871541302680969\n",
            "\u001b[92mTrain perplexity: 93.067719 ||| loss 4.533327\u001b[0m\n",
            "\u001b[92mValidation perplexity: 126.278234 ||| loss 4.838488\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.881581 ||| loss 4.811221\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 1 ------------\n",
            "Batch #500 Loss: 4.868428201675415\n",
            "Batch #1000 Loss: 4.838587052345276\n",
            "Batch #1500 Loss: 4.8407669944763185\n",
            "Batch #2000 Loss: 4.843655788421631\n",
            "\u001b[92mTrain perplexity: 90.226312 ||| loss 4.502321\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.535469 ||| loss 4.824591\u001b[0m\n",
            "\u001b[92mTest perplexity: 121.342961 ||| loss 4.798621\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15, LR: 1 ------------\n",
            "Batch #500 Loss: 4.841664880752563\n",
            "Batch #1000 Loss: 4.80772652721405\n",
            "Batch #1500 Loss: 4.811952700614929\n",
            "Batch #2000 Loss: 4.819194265365601\n",
            "\u001b[92mTrain perplexity: 87.245874 ||| loss 4.468730\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.393969 ||| loss 4.807245\u001b[0m\n",
            "\u001b[92mTest perplexity: 119.321868 ||| loss 4.781825\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16, LR: 1 ------------\n",
            "Batch #500 Loss: 4.815823871612549\n",
            "Batch #1000 Loss: 4.7862491760253905\n",
            "Batch #1500 Loss: 4.787039421081543\n",
            "Batch #2000 Loss: 4.79640259552002\n",
            "\u001b[92mTrain perplexity: 84.402463 ||| loss 4.435597\u001b[0m\n",
            "\u001b[92mValidation perplexity: 120.279726 ||| loss 4.789820\u001b[0m\n",
            "\u001b[92mTest perplexity: 117.528842 ||| loss 4.766684\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17, LR: 1 ------------\n",
            "Batch #500 Loss: 4.796443187713623\n",
            "Batch #1000 Loss: 4.760358128547669\n",
            "Batch #1500 Loss: 4.766475918769836\n",
            "Batch #2000 Loss: 4.771062906265259\n",
            "\u001b[92mTrain perplexity: 82.258357 ||| loss 4.409865\u001b[0m\n",
            "\u001b[92mValidation perplexity: 118.983319 ||| loss 4.778983\u001b[0m\n",
            "\u001b[92mTest perplexity: 116.449566 ||| loss 4.757458\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18, LR: 1 ------------\n",
            "Batch #500 Loss: 4.774930470466614\n",
            "Batch #1000 Loss: 4.741846442222595\n",
            "Batch #1500 Loss: 4.743097381591797\n",
            "Batch #2000 Loss: 4.754107538223266\n",
            "\u001b[92mTrain perplexity: 80.354735 ||| loss 4.386451\u001b[0m\n",
            "\u001b[92mValidation perplexity: 118.239672 ||| loss 4.772714\u001b[0m\n",
            "\u001b[92mTest perplexity: 115.420451 ||| loss 4.748582\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19, LR: 1 ------------\n",
            "Batch #500 Loss: 4.757243793964386\n",
            "Batch #1000 Loss: 4.720581992149353\n",
            "Batch #1500 Loss: 4.725581070899963\n",
            "Batch #2000 Loss: 4.738882071495056\n",
            "\u001b[92mTrain perplexity: 78.228123 ||| loss 4.359629\u001b[0m\n",
            "\u001b[92mValidation perplexity: 116.797680 ||| loss 4.760443\u001b[0m\n",
            "\u001b[92mTest perplexity: 113.913596 ||| loss 4.735440\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20, LR: 1 ------------\n",
            "Batch #500 Loss: 4.741217330932617\n",
            "Batch #1000 Loss: 4.704153912544251\n",
            "Batch #1500 Loss: 4.709439079284668\n",
            "Batch #2000 Loss: 4.718537243366241\n",
            "\u001b[92mTrain perplexity: 75.963985 ||| loss 4.330259\u001b[0m\n",
            "\u001b[92mValidation perplexity: 115.401332 ||| loss 4.748416\u001b[0m\n",
            "\u001b[92mTest perplexity: 112.435080 ||| loss 4.722376\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21, LR: 0.75 ------------\n",
            "Batch #500 Loss: 4.710562652587891\n",
            "Batch #1000 Loss: 4.668199602127075\n",
            "Batch #1500 Loss: 4.664919579982757\n",
            "Batch #2000 Loss: 4.6736473426818845\n",
            "\u001b[92mTrain perplexity: 73.089330 ||| loss 4.291682\u001b[0m\n",
            "\u001b[92mValidation perplexity: 113.197109 ||| loss 4.729131\u001b[0m\n",
            "\u001b[92mTest perplexity: 110.014314 ||| loss 4.700610\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22, LR: 0.5625 ------------\n",
            "Batch #500 Loss: 4.67797367477417\n",
            "Batch #1000 Loss: 4.6348823566436765\n",
            "Batch #1500 Loss: 4.634338230133056\n",
            "Batch #2000 Loss: 4.6405217752456664\n",
            "\u001b[92mTrain perplexity: 70.804169 ||| loss 4.259918\u001b[0m\n",
            "\u001b[92mValidation perplexity: 111.025701 ||| loss 4.709762\u001b[0m\n",
            "\u001b[92mTest perplexity: 108.001420 ||| loss 4.682144\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23, LR: 0.421875 ------------\n",
            "Batch #500 Loss: 4.655999617099762\n",
            "Batch #1000 Loss: 4.608232926368713\n",
            "Batch #1500 Loss: 4.606405859470367\n",
            "Batch #2000 Loss: 4.612924069404602\n",
            "\u001b[92mTrain perplexity: 68.933307 ||| loss 4.233139\u001b[0m\n",
            "\u001b[92mValidation perplexity: 109.555585 ||| loss 4.696432\u001b[0m\n",
            "\u001b[92mTest perplexity: 106.561688 ||| loss 4.668724\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24, LR: 0.31640625 ------------\n",
            "Batch #500 Loss: 4.634060566425323\n",
            "Batch #1000 Loss: 4.590800004959107\n",
            "Batch #1500 Loss: 4.588588950634002\n",
            "Batch #2000 Loss: 4.5948191637992855\n",
            "\u001b[92mTrain perplexity: 67.618894 ||| loss 4.213887\u001b[0m\n",
            "\u001b[92mValidation perplexity: 108.251619 ||| loss 4.684458\u001b[0m\n",
            "\u001b[92mTest perplexity: 105.253564 ||| loss 4.656372\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25, LR: 0.2373046875 ------------\n",
            "Batch #500 Loss: 4.622537035942077\n",
            "Batch #1000 Loss: 4.574309103488922\n",
            "Batch #1500 Loss: 4.57300763130188\n",
            "Batch #2000 Loss: 4.580370263576508\n",
            "\u001b[92mTrain perplexity: 66.687924 ||| loss 4.200024\u001b[0m\n",
            "\u001b[92mValidation perplexity: 107.651207 ||| loss 4.678896\u001b[0m\n",
            "\u001b[92mTest perplexity: 104.543091 ||| loss 4.649599\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09e8200fdab94d768427bd16aad636c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.6496</td></tr><tr><td>Test-perplexity</td><td>104.54309</td></tr><tr><td>Train-loss</td><td>4.20002</td></tr><tr><td>Train-perplexity</td><td>66.68792</td></tr><tr><td>Validation-loss</td><td>4.6789</td></tr><tr><td>Validation-perplexity</td><td>107.65121</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">peachy-water-9</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/qlelgi3q' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/qlelgi3q</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240926_025244-qlelgi3q/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './models/gru_drop.pth')"
      ],
      "metadata": {
        "id": "rRXl71g2Z9aL"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}