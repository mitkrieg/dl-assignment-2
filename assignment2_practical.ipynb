{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define PTBText Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBText(Dataset):\n",
    "    def __init__(self, filename, sequence_len, prior_vocab=None) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenized_text = []\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.tokenizer = RegexpTokenizer(r'<unk>|<pad>|<oov>|<sos>|<eos>|\\w+').tokenize\n",
    "        self.seq_len = sequence_len\n",
    "        self.max_len = 0\n",
    "        if prior_vocab:\n",
    "            self.vocab = prior_vocab\n",
    "        else:\n",
    "            self.vocab = {'<pad>':0,'<oov>':1,'<sos>':2,'<eos>':3,'<unk>':4}\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = self.tokenizer(line)\n",
    "\n",
    "                #only build new vocab if prior vocab is not given\n",
    "                if prior_vocab is None:\n",
    "                    idx = len(self.vocab)\n",
    "                    for word in tokens:\n",
    "                        if word not in self.vocab:\n",
    "                            self.vocab[word] = idx\n",
    "                            idx += 1\n",
    "                \n",
    "                self.tokenized_text.append(['<sos>'] + tokens + ['<eos>'])\n",
    "                self.max_len = max(self.max_len, len(tokens) + 2)\n",
    "        \n",
    "\n",
    "        self.encoded_text = [self.encode_text(x, pad=True) for x in self.tokenized_text]\n",
    "\n",
    "        #build sequences\n",
    "        for tokens in self.tokenized_text:\n",
    "            for i in range(len(tokens) - self.seq_len):\n",
    "                self.data.append(tokens[i:i+self.seq_len])\n",
    "                self.labels.append(tokens[i+self.seq_len])\n",
    "        self.encoded_labels = [self.vocab.get(x,1) for x in self.labels]\n",
    "        self.encoded_data = [self.encode_text(x) for x in self.data]\n",
    "\n",
    "    def encode_text(self, tokens: list[str], pad=False):\n",
    "        encoded = []\n",
    "        for word in tokens:\n",
    "            encoded.append(self.vocab.get(word,1))\n",
    "\n",
    "        if pad and len(encoded) < self.max_len:\n",
    "            encoded.extend([0]* (self.max_len - len(encoded)))\n",
    "        elif len(encoded) < self.seq_len:\n",
    "            encoded.extend([0]*(self.seq_len - len(encoded)))\n",
    "\n",
    "        return encoded\n",
    "    \n",
    "    def resequence_data(self, seqence_len):\n",
    "        self.seq_len = seqence_len\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for tokens in self.tokenized_text:\n",
    "            for i in range(len(tokens) - self.seq_len):\n",
    "                self.data.append(tokens[i:i+self.seq_len])\n",
    "                self.labels.append(tokens[i+self.seq_len])\n",
    "\n",
    "        self.encoded_labels = [self.vocab.get(x,1) for x in self.labels]\n",
    "        self.encoded_data = [self.encode_text(x) for x in self.data]\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.encoded_data[index]), torch.tensor(self.encoded_labels[index])\n",
    "    \n",
    "    def get_tokens(self, index):\n",
    "        return self.tokenized_text[index]\n",
    "    \n",
    "    def get_encoded_tokens(self, index):\n",
    "        return self.encoded_text[index]\n",
    "    \n",
    "    def get_sequence(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "    def get_encoded_sequence(self, index):\n",
    "        return self.__getitem__(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Create Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vocab size: 9648\n",
      "Training sample raw:  (['is', 'unusually', '<unk>', 'once', 'it'], 'enters')\n",
      "Training sample encoded: (tensor([45, 91,  4, 68, 84]), tensor(92))\n"
     ]
    }
   ],
   "source": [
    "train = PTBText('./data/ptb.train.txt', 5)\n",
    "val = PTBText('./data/ptb.valid.txt', 5, prior_vocab=train.vocab)\n",
    "test = PTBText('./data/ptb.test.txt', 5, prior_vocab=train.vocab)\n",
    "\n",
    "gen = torch.Generator().manual_seed(123)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "\n",
    "dataloaders = {\n",
    "    'train':train_loader,\n",
    "    'val':val_loader,\n",
    "    'test':test_loader\n",
    "}\n",
    "\n",
    "print('Training vocab size:', len(train.vocab))\n",
    "print('Training sample raw: ', train.get_sequence(100))\n",
    "print('Training sample encoded:',train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZarembaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_units=200, num_lstm_layers=2, dropout_rate= 0) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_units, num_lstm_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, vocab_size)\n",
    "        self.dropout_rate = dropout_rate \n",
    "        if self.dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = self.dropout(x[:, -1, :])\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            x = self.fc(x[:, -1, :])\n",
    "\n",
    "\n",
    "        return x\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network, dataloader, loss_fn, optimizer, device, epoch, verbosity: int):\n",
    "    \"\"\"Train one epoch of a network\"\"\"\n",
    "\n",
    "    network.train()\n",
    "    batch_loss = 0\n",
    "\n",
    "    # iterate over all batches\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "        if i % verbosity == verbosity - 1:\n",
    "            print(f'Batch #{i + 1} Loss: {batch_loss / verbosity}')\n",
    "            batch_loss = 0\n",
    "    \n",
    "def perplexity(loss, batches):\n",
    "    return math.exp(loss / batches)\n",
    "\n",
    "def eval_network(title, network, dataloader, loss_fn, epoch):\n",
    "    \"\"\"Evaluate model and log metrics to wandb\"\"\"\n",
    "\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            data, labels = data\n",
    "            outputs = network(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            loss += loss_fn(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        perp = perplexity(loss, len(dataloader))\n",
    "        wandb.log({\n",
    "            f'{title}-loss': loss / len(dataloader),\n",
    "            f'{title}-perplexity': perp\n",
    "        }, step=epoch)\n",
    "  \n",
    "    print(f'\\033[92m{title} perplexity: {perp:.6f} ||| loss {loss / len(dataloader):.6f}\\033[0m')\n",
    "    return perp\n",
    "\n",
    "def train_network(network, dataloaders, loss_fn, optimizer, schedule, device, epochs: int, verbosity: int):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'----------- Epoch #{epoch + 1}, LR: {optimizer.param_groups[0]['lr']} ------------')\n",
    "        train_epoch(network, dataloaders['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
    "        train_perplexity = eval_network('Train', network, dataloaders['train'], loss_fn, epoch)\n",
    "        val_perplexity = eval_network('Validation', network, dataloaders['val'], loss_fn, epoch)\n",
    "        test_perplexity = eval_network('Test', network, dataloaders['test'], loss_fn, epoch)\n",
    "        print('------------------------------------\\n')\n",
    "\n",
    "        schedule.step()\n",
    "    print('----------- Train Complete! ------------')\n",
    "    return {\n",
    "        'train':train_perplexity,\n",
    "        'val':val_perplexity,\n",
    "        'test':test_perplexity\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(epoch):\n",
    "    if epoch < 4:\n",
    "        return .1\n",
    "    else:\n",
    "        return 0.5 ** (epoch - 3)\n",
    "\n",
    "model = ZarembaRNN(len(train.vocab), 10)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "adam = optim.Adam(model.parameters(), lr=1)\n",
    "schedule = optim.lr_scheduler.LambdaLR(adam, lr_lambda)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4ozw4krf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>▁▇█</td></tr><tr><td>Train-loss</td><td>▁██</td></tr><tr><td>Validation-loss</td><td>▁▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>257.97849</td></tr><tr><td>Test-perplexity</td><td>1.0930311322468983e+112</td></tr><tr><td>Train-loss</td><td>256.31937</td></tr><tr><td>Train-perplexity</td><td>2.080110740121543e+111</td></tr><tr><td>Validation-loss</td><td>263.00842</td></tr><tr><td>Validation-perplexity</td><td>1.671501376252282e+114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-grass-16</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/4ozw4krf' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/4ozw4krf</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240920_221816-4ozw4krf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4ozw4krf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mitchellkrieger/Documents/GitHub/dl-assignment-2/wandb/run-20240920_222738-pt6qfxyy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/pt6qfxyy' target=\"_blank\">vivid-surf-17</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/pt6qfxyy' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/pt6qfxyy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch #1, LR: 0.1 ------------\n",
      "Batch #1000 Loss: 8.146729045391083\n",
      "Batch #2000 Loss: 6.6877510733604435\n",
      "Batch #3000 Loss: 6.6735848937034605\n",
      "Batch #4000 Loss: 6.673708264350891\n",
      "Batch #5000 Loss: 6.655115474700928\n",
      "\u001b[92mTrain perplexity: 709.837322 ||| loss 6.565036\u001b[0m\n",
      "\u001b[92mValidation perplexity: 724.363467 ||| loss 6.585293\u001b[0m\n",
      "\u001b[92mTest perplexity: 672.669305 ||| loss 6.511254\u001b[0m\n",
      "------------------------------------\n",
      "\n",
      "----------- Epoch #2, LR: 0.1 ------------\n",
      "Batch #1000 Loss: 6.576430155754089\n",
      "Batch #2000 Loss: 6.560266708374024\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"dl-assignment2\")\n",
    "results = train_network(model, dataloaders, cross_entropy, adam, schedule, device, 13, 1000)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[0.7332, 0.9473, 0.8291,  ..., 0.6799, 0.8324, 0.8936],\n",
       "        [0.9341, 0.6996, 0.9489,  ..., 0.7854, 0.8351, 0.8760],\n",
       "        [0.8873, 0.9887, 0.8474,  ..., 0.7111, 0.5906, 0.6415],\n",
       "        ...,\n",
       "        [0.7961, 0.7227, 0.9180,  ..., 0.9373, 0.6811, 0.9479],\n",
       "        [0.7929, 0.6888, 0.9234,  ..., 0.8842, 0.8649, 0.9375],\n",
       "        [0.8669, 0.9894, 0.7400,  ..., 0.9723, 0.9387, 0.8379]]),\n",
       "indices=tensor([[1, 4, 0,  ..., 2, 1, 3],\n",
       "        [4, 1, 4,  ..., 0, 0, 2],\n",
       "        [4, 1, 2,  ..., 2, 2, 1],\n",
       "        ...,\n",
       "        [4, 3, 2,  ..., 4, 1, 1],\n",
       "        [1, 0, 2,  ..., 1, 1, 0],\n",
       "        [3, 1, 4,  ..., 4, 3, 1]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.rand([32, 5, 9648]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test  0.325434'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'test {.325434365478: .6}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 9648])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    inputs, labels = data\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    print(outputs.shape)\n",
    "    loss = cross_entropy(outputs, labels)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1751, 0.1727, 0.2612, 0.2387, 0.1522])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([.45,.436,.85,.76,.31]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "a,b = torch.max(torch.tensor([1,2,3,4,5]), 0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(5),\n",
       "indices=tensor(4))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor([1,2,3,4,5]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
