{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-2/blob/main/assignment2_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijX0qBNZlwRY",
        "outputId": "3fa1a502-3d8b-4642-9655-408c235781ff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXlIsAxnaoz5",
        "outputId": "458b3020-02b2-469f-ab9d-82951721cfe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "CUDA GPU Available: True\n",
            "MPS GPU Available: False\n",
            "GPU Name: Tesla T4\n",
            "GPU Count: 1\n",
            "GPU Memory Allocated: 74159616\n",
            "GPU Memory Cached: 236978176\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import wandb\n",
        "\n",
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('CUDA GPU Available:',torch.cuda.is_available())\n",
        "print('MPS GPU Available:', torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "#   device = torch.device('mps')\n",
        "#   print('Pytorch GPU Build:',torch.backends.mps.is_built())\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9d895XBaoz6"
      },
      "source": [
        "## Define PTBText Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TmKDKXflaoz7"
      },
      "outputs": [],
      "source": [
        "class PTBText(Dataset):\n",
        "    def __init__(self, filename, sequence_len, prior_vocab=None,device=torch.device('cpu')) -> None:\n",
        "        super().__init__()\n",
        "        self.tokenized_text = []\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.device = device\n",
        "        self.tokenizer = RegexpTokenizer(r'<unk>|<pad>|<oov>|<sos>|<eos>|\\w+').tokenize\n",
        "        self.seq_len = sequence_len\n",
        "        self.max_len = 0\n",
        "        if prior_vocab:\n",
        "            self.vocab = prior_vocab\n",
        "        else:\n",
        "            self.vocab = {'<pad>':0,'<oov>':1,'<sos>':2,'<eos>':3,'<unk>':4}\n",
        "\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                tokens = self.tokenizer(line)\n",
        "\n",
        "                #only build new vocab if prior vocab is not given\n",
        "                if prior_vocab is None:\n",
        "                    idx = len(self.vocab)\n",
        "                    for word in tokens:\n",
        "                        if word not in self.vocab:\n",
        "                            self.vocab[word] = idx\n",
        "                            idx += 1\n",
        "\n",
        "                self.tokenized_text.append(['<sos>'] + tokens + ['<eos>'])\n",
        "                self.max_len = max(self.max_len, len(tokens) + 2)\n",
        "\n",
        "\n",
        "        self.encoded_text = [self.encode_text(x, pad=True) for x in self.tokenized_text]\n",
        "\n",
        "        #build sequences\n",
        "        for tokens in self.tokenized_text:\n",
        "            for i in range(len(tokens) - self.seq_len):\n",
        "                self.data.append(tokens[i:i+self.seq_len])\n",
        "                self.labels.append(tokens[i+self.seq_len])\n",
        "        self.encoded_labels = [self.vocab.get(x,1) for x in self.labels]\n",
        "        self.encoded_data = [self.encode_text(x) for x in self.data]\n",
        "\n",
        "    def encode_text(self, tokens: list[str], pad=False):\n",
        "        encoded = []\n",
        "        for word in tokens:\n",
        "            encoded.append(self.vocab.get(word,1))\n",
        "\n",
        "        if pad and len(encoded) < self.max_len:\n",
        "            encoded.extend([0]* (self.max_len - len(encoded)))\n",
        "        elif len(encoded) < self.seq_len:\n",
        "            encoded.extend([0]*(self.seq_len - len(encoded)))\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def resequence_data(self, seqence_len):\n",
        "        self.seq_len = seqence_len\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for tokens in self.tokenized_text:\n",
        "            for i in range(len(tokens) - self.seq_len):\n",
        "                self.data.append(tokens[i:i+self.seq_len])\n",
        "                self.labels.append(tokens[i+self.seq_len])\n",
        "\n",
        "        self.encoded_labels = [self.vocab.get(x,1) for x in self.labels]\n",
        "        self.encoded_data = [self.encode_text(x) for x in self.data]\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.encoded_data[index]).to(self.device), torch.tensor(self.encoded_labels[index]).to(self.device)\n",
        "\n",
        "    def get_tokens(self, index):\n",
        "        return self.tokenized_text[index]\n",
        "\n",
        "    def get_encoded_tokens(self, index):\n",
        "        return self.encoded_text[index]\n",
        "\n",
        "    def get_sequence(self, index):\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "    def get_encoded_sequence(self, index):\n",
        "        return self.__getitem__(index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZubswHHaoz7"
      },
      "source": [
        "### Load Data & Create Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhw_Ric-aoz7",
        "outputId": "e8c7b324-2c9c-4800-c64c-f21860f74a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size: 9648\n",
            "Training sample raw:  (['is', 'unusually', '<unk>', 'once', 'it'], 'enters')\n",
            "Training sample encoded: (tensor([45, 91,  4, 68, 84]), tensor(92))\n"
          ]
        }
      ],
      "source": [
        "train = PTBText('/content/ptb.train.txt', 5)\n",
        "val = PTBText('/content/ptb.valid.txt', 5, prior_vocab=train.vocab)\n",
        "test = PTBText('/content/ptb.test.txt', 5, prior_vocab=train.vocab)\n",
        "\n",
        "gen = torch.Generator().manual_seed(123)\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "\n",
        "dataloaders = {\n",
        "    'train':train_loader,\n",
        "    'val':val_loader,\n",
        "    'test':test_loader\n",
        "}\n",
        "\n",
        "print('Training vocab size:', len(train.vocab))\n",
        "print('Training sample raw: ', train.get_sequence(100))\n",
        "print('Training sample encoded:',train[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBtifwp8aoz8"
      },
      "source": [
        "## Define LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "z9KNjkJ0aoz8"
      },
      "outputs": [],
      "source": [
        "class ZarembaRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_units=200, num_lstm_layers=2, dropout_rate= 0) -> None:\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embedding_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,hidden_units, num_lstm_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_units, vocab_size)\n",
        "        self.dropout_rate = dropout_rate\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout(x[:, -1, :])\n",
        "            x = self.fc(x)\n",
        "        else:\n",
        "            x = self.fc(x[:, -1, :])\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ylH6Tu6yaoz8"
      },
      "outputs": [],
      "source": [
        "def train_epoch(network, dataloader, loss_fn, optimizer, device, epoch, verbosity: int):\n",
        "    \"\"\"Train one epoch of a network\"\"\"\n",
        "\n",
        "    network.train()\n",
        "    batch_loss = 0\n",
        "\n",
        "    # iterate over all batches\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "        if i % verbosity == verbosity - 1:\n",
        "            print(f'Batch #{i + 1} Loss: {batch_loss / verbosity}')\n",
        "            batch_loss = 0\n",
        "\n",
        "def perplexity(loss, batches):\n",
        "    return math.exp(loss / batches)\n",
        "\n",
        "def eval_network(title, network, dataloader, loss_fn, epoch):\n",
        "    \"\"\"Evaluate model and log metrics to wandb\"\"\"\n",
        "\n",
        "    network.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            data, labels = data\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = network(data)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            loss += loss_fn(outputs, labels)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        perp = perplexity(loss, len(dataloader))\n",
        "        wandb.log({\n",
        "            f'{title}-loss': loss / len(dataloader),\n",
        "            f'{title}-perplexity': perp\n",
        "        }, step=epoch)\n",
        "\n",
        "    print(f'\\033[92m{title} perplexity: {perp:.6f} ||| loss {loss / len(dataloader):.6f}\\033[0m')\n",
        "    return perp\n",
        "\n",
        "def train_network(network, dataloaders, loss_fn, optimizer, schedule, device, epochs: int, verbosity: int):\n",
        "    for epoch in range(epochs):\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'----------- Epoch #{epoch + 1}, LR: {lr} ------------')\n",
        "        train_epoch(network, dataloaders['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "        train_perplexity = eval_network('Train', network, dataloaders['train'], loss_fn, epoch)\n",
        "        val_perplexity = eval_network('Validation', network, dataloaders['val'], loss_fn, epoch)\n",
        "        test_perplexity = eval_network('Test', network, dataloaders['test'], loss_fn, epoch)\n",
        "        print('------------------------------------\\n')\n",
        "\n",
        "        schedule.step()\n",
        "    print('----------- Train Complete! ------------')\n",
        "    return {\n",
        "        'train':train_perplexity,\n",
        "        'val':val_perplexity,\n",
        "        'test':test_perplexity\n",
        "    }\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "QxURR3Tlaoz8"
      },
      "outputs": [],
      "source": [
        "def lr_lambda(epoch):\n",
        "    if epoch < 7:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0.5 ** (epoch - 6)\n",
        "\n",
        "model = ZarembaRNN(len(train.vocab), 10)\n",
        "model = model.to(device)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "adam = optim.Adam(model.parameters(), lr=1e-2)\n",
        "schedule = optim.lr_scheduler.LambdaLR(adam, lr_lambda)\n",
        "# device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8d6970011d084a489695e610df70e138",
            "64d478a90b294738b9b3f42eeac79f65",
            "52158bd9d5674831a80f417d36c865f3",
            "cb723cd2cc6c47268ed90f9572b5a6a6",
            "ee02a7de8af04231811912b4574007a9",
            "7ffbfb456a5846268fddb7a0732766f1",
            "cd4e492960914e0baa9c95dd4b7100bc",
            "dd8125964e144a4bbcdda0676350184e",
            "8b96660e31274912aa43d851b6054298",
            "5cdf83c45e134c529cf61ca9eb2ba2ee",
            "81cc6302100b4027953ce66667057df8",
            "fc82557ba2d6413a83599bea1c7e9383",
            "e677297e2d734456a1d18d9348207411",
            "7d60b60b6b1d45059135e337bc41fbbe",
            "63093d97c488456cba21fcb41a412e66",
            "7c99859d81a7451e8811361cb0274b0e"
          ]
        },
        "id": "oZmvghb8aoz9",
        "outputId": "c00fe2a5-3caf-401d-9da8-41ebd27b943a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:50auqten) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d6970011d084a489695e610df70e138"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▅▄▃▂▂▂▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▃▂▂▂▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▅▄▃▂▂▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▂▂▂▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▅▃▂▂▂▂▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▄▃▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>5.2165</td></tr><tr><td>Test-perplexity</td><td>184.28748</td></tr><tr><td>Train-loss</td><td>4.77569</td></tr><tr><td>Train-perplexity</td><td>118.5919</td></tr><tr><td>Validation-loss</td><td>5.34356</td></tr><tr><td>Validation-perplexity</td><td>209.25638</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dry-night-23</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/50auqten' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/50auqten</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240921_032536-50auqten/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:50auqten). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240921_033715-e3aghbky</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/e3aghbky' target=\"_blank\">swift-dew-24</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/e3aghbky' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/e3aghbky</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 0.01 ------------\n",
            "Batch #1000 Loss: 6.403616511821747\n",
            "Batch #2000 Loss: 5.946282475471497\n",
            "Batch #3000 Loss: 5.719841088294983\n",
            "Batch #4000 Loss: 5.598230762481689\n",
            "Batch #5000 Loss: 5.515763603687287\n",
            "\u001b[92mTrain perplexity: 195.687745 ||| loss 5.276520\u001b[0m\n",
            "\u001b[92mValidation perplexity: 228.820556 ||| loss 5.432938\u001b[0m\n",
            "\u001b[92mTest perplexity: 215.637667 ||| loss 5.373600\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 0.01 ------------\n",
            "Batch #1000 Loss: 5.2413937554359435\n",
            "Batch #2000 Loss: 5.222617111682892\n",
            "Batch #3000 Loss: 5.204514781475067\n",
            "Batch #4000 Loss: 5.176063044071197\n",
            "Batch #5000 Loss: 5.147251231193542\n",
            "\u001b[92mTrain perplexity: 140.384227 ||| loss 4.944383\u001b[0m\n",
            "\u001b[92mValidation perplexity: 190.958044 ||| loss 5.252054\u001b[0m\n",
            "\u001b[92mTest perplexity: 178.278382 ||| loss 5.183346\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 0.01 ------------\n",
            "Batch #1000 Loss: 4.9371884202957155\n",
            "Batch #2000 Loss: 4.933050584793091\n",
            "Batch #3000 Loss: 4.947193994522094\n",
            "Batch #4000 Loss: 4.9464706001281735\n",
            "Batch #5000 Loss: 4.957391579151153\n",
            "\u001b[92mTrain perplexity: 116.749237 ||| loss 4.760028\u001b[0m\n",
            "\u001b[92mValidation perplexity: 186.993143 ||| loss 5.231072\u001b[0m\n",
            "\u001b[92mTest perplexity: 173.344604 ||| loss 5.155282\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 0.01 ------------\n",
            "Batch #1000 Loss: 4.757266831874848\n",
            "Batch #2000 Loss: 4.764908854007721\n",
            "Batch #3000 Loss: 4.795926164150238\n",
            "Batch #4000 Loss: 4.806669893741607\n",
            "Batch #5000 Loss: 4.800007431983948\n",
            "\u001b[92mTrain perplexity: 101.103250 ||| loss 4.616142\u001b[0m\n",
            "\u001b[92mValidation perplexity: 188.001906 ||| loss 5.236452\u001b[0m\n",
            "\u001b[92mTest perplexity: 173.750098 ||| loss 5.157618\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 0.01 ------------\n",
            "Batch #1000 Loss: 4.632950607299804\n",
            "Batch #2000 Loss: 4.649089884996414\n",
            "Batch #3000 Loss: 4.671184907436371\n",
            "Batch #4000 Loss: 4.703646666765213\n",
            "Batch #5000 Loss: 4.699745532274246\n",
            "\u001b[92mTrain perplexity: 92.904878 ||| loss 4.531576\u001b[0m\n",
            "\u001b[92mValidation perplexity: 195.073139 ||| loss 5.273375\u001b[0m\n",
            "\u001b[92mTest perplexity: 176.374288 ||| loss 5.172608\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 0.01 ------------\n",
            "Batch #1000 Loss: 4.5319290709495545\n",
            "Batch #2000 Loss: 4.57222880268097\n",
            "Batch #3000 Loss: 4.581015888214111\n",
            "Batch #4000 Loss: 4.6286083846092225\n",
            "Batch #5000 Loss: 4.621632259845733\n",
            "\u001b[92mTrain perplexity: 84.849845 ||| loss 4.440883\u001b[0m\n",
            "\u001b[92mValidation perplexity: 200.661046 ||| loss 5.301617\u001b[0m\n",
            "\u001b[92mTest perplexity: 180.718990 ||| loss 5.196943\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 0.01 ------------\n",
            "Batch #1000 Loss: 4.450899543523788\n",
            "Batch #2000 Loss: 4.489459479570389\n",
            "Batch #3000 Loss: 4.510677394866943\n",
            "Batch #4000 Loss: 4.540901101589203\n",
            "Batch #5000 Loss: 4.5670328636169435\n",
            "\u001b[92mTrain perplexity: 79.165005 ||| loss 4.371534\u001b[0m\n",
            "\u001b[92mValidation perplexity: 209.173981 ||| loss 5.343166\u001b[0m\n",
            "\u001b[92mTest perplexity: 188.209462 ||| loss 5.237556\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 0.005 ------------\n",
            "Batch #1000 Loss: 4.305499176979065\n",
            "Batch #2000 Loss: 4.304112576007843\n",
            "Batch #3000 Loss: 4.327884399414063\n",
            "Batch #4000 Loss: 4.332191935777664\n",
            "Batch #5000 Loss: 4.349235709905624\n",
            "\u001b[92mTrain perplexity: 66.650745 ||| loss 4.199466\u001b[0m\n",
            "\u001b[92mValidation perplexity: 213.297026 ||| loss 5.362686\u001b[0m\n",
            "\u001b[92mTest perplexity: 189.801689 ||| loss 5.245980\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 0.0025 ------------\n",
            "Batch #1000 Loss: 4.166866557359695\n",
            "Batch #2000 Loss: 4.157507673740387\n",
            "Batch #3000 Loss: 4.167533344984054\n",
            "Batch #4000 Loss: 4.189168982505798\n",
            "Batch #5000 Loss: 4.201224579334259\n",
            "\u001b[92mTrain perplexity: 59.320921 ||| loss 4.082962\u001b[0m\n",
            "\u001b[92mValidation perplexity: 235.428258 ||| loss 5.461406\u001b[0m\n",
            "\u001b[92mTest perplexity: 208.502004 ||| loss 5.339949\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 0.00125 ------------\n",
            "Batch #1000 Loss: 4.067459537506103\n",
            "Batch #2000 Loss: 4.082408076763153\n",
            "Batch #3000 Loss: 4.087576494932175\n",
            "Batch #4000 Loss: 4.099397252559662\n",
            "Batch #5000 Loss: 4.1035669326782225\n",
            "\u001b[92mTrain perplexity: 56.398984 ||| loss 4.032451\u001b[0m\n",
            "\u001b[92mValidation perplexity: 241.574830 ||| loss 5.487179\u001b[0m\n",
            "\u001b[92mTest perplexity: 211.036426 ||| loss 5.352031\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 0.000625 ------------\n",
            "Batch #1000 Loss: 4.027341484069824\n",
            "Batch #2000 Loss: 4.044772358179093\n",
            "Batch #3000 Loss: 4.038175625801086\n",
            "Batch #4000 Loss: 4.040846210956573\n",
            "Batch #5000 Loss: 4.055590651750564\n",
            "\u001b[92mTrain perplexity: 54.918922 ||| loss 4.005858\u001b[0m\n",
            "\u001b[92mValidation perplexity: 256.399241 ||| loss 5.546736\u001b[0m\n",
            "\u001b[92mTest perplexity: 221.359673 ||| loss 5.399789\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 0.0003125 ------------\n",
            "Batch #1000 Loss: 4.013062832355499\n",
            "Batch #2000 Loss: 4.024141523122787\n",
            "Batch #3000 Loss: 4.018759448051453\n",
            "Batch #4000 Loss: 4.005372230052948\n",
            "Batch #5000 Loss: 4.007820907354355\n",
            "\u001b[92mTrain perplexity: 54.307963 ||| loss 3.994671\u001b[0m\n",
            "\u001b[92mValidation perplexity: 261.470974 ||| loss 5.566323\u001b[0m\n",
            "\u001b[92mTest perplexity: 226.429981 ||| loss 5.422436\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 0.00015625 ------------\n",
            "Batch #1000 Loss: 4.005723569869995\n",
            "Batch #2000 Loss: 3.986899334192276\n",
            "Batch #3000 Loss: 3.9959752345085144\n",
            "Batch #4000 Loss: 4.004019556045532\n",
            "Batch #5000 Loss: 3.9955939757823944\n",
            "\u001b[92mTrain perplexity: 54.008308 ||| loss 3.989138\u001b[0m\n",
            "\u001b[92mValidation perplexity: 264.872460 ||| loss 5.579248\u001b[0m\n",
            "\u001b[92mTest perplexity: 229.715368 ||| loss 5.436841\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 7.8125e-05 ------------\n",
            "Batch #1000 Loss: 3.9878747072219847\n",
            "Batch #2000 Loss: 3.9974361157417295\n",
            "Batch #3000 Loss: 3.9961169254779816\n",
            "Batch #4000 Loss: 3.990428312778473\n",
            "Batch #5000 Loss: 3.9894876217842103\n",
            "\u001b[92mTrain perplexity: 53.876112 ||| loss 3.986687\u001b[0m\n",
            "\u001b[92mValidation perplexity: 264.877260 ||| loss 5.579267\u001b[0m\n",
            "\u001b[92mTest perplexity: 228.417967 ||| loss 5.431177\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b96660e31274912aa43d851b6054298"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>▆▂▁▁▁▂▃▃▆▆▇███</td></tr><tr><td>Test-perplexity</td><td>▆▂▁▁▁▂▃▃▅▆▇███</td></tr><tr><td>Train-loss</td><td>█▆▅▄▄▃▃▂▂▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▃▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>▅▁▁▁▂▂▃▄▆▆▇███</td></tr><tr><td>Validation-perplexity</td><td>▅▁▁▁▂▂▃▃▅▆▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>5.43118</td></tr><tr><td>Test-perplexity</td><td>228.41797</td></tr><tr><td>Train-loss</td><td>3.98669</td></tr><tr><td>Train-perplexity</td><td>53.87611</td></tr><tr><td>Validation-loss</td><td>5.57927</td></tr><tr><td>Validation-perplexity</td><td>264.87726</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swift-dew-24</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/e3aghbky' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/e3aghbky</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240921_033715-e3aghbky/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project=\"dl-assignment2\")\n",
        "results = train_network(model, dataloaders, cross_entropy, adam, schedule, device, 14, 1000)\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvN_lQb-dGa6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d6970011d084a489695e610df70e138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64d478a90b294738b9b3f42eeac79f65",
              "IPY_MODEL_52158bd9d5674831a80f417d36c865f3"
            ],
            "layout": "IPY_MODEL_cb723cd2cc6c47268ed90f9572b5a6a6"
          }
        },
        "64d478a90b294738b9b3f42eeac79f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee02a7de8af04231811912b4574007a9",
            "placeholder": "​",
            "style": "IPY_MODEL_7ffbfb456a5846268fddb7a0732766f1",
            "value": "0.016 MB of 0.016 MB uploaded\r"
          }
        },
        "52158bd9d5674831a80f417d36c865f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd4e492960914e0baa9c95dd4b7100bc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd8125964e144a4bbcdda0676350184e",
            "value": 1
          }
        },
        "cb723cd2cc6c47268ed90f9572b5a6a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee02a7de8af04231811912b4574007a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ffbfb456a5846268fddb7a0732766f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd4e492960914e0baa9c95dd4b7100bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd8125964e144a4bbcdda0676350184e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b96660e31274912aa43d851b6054298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cdf83c45e134c529cf61ca9eb2ba2ee",
              "IPY_MODEL_81cc6302100b4027953ce66667057df8"
            ],
            "layout": "IPY_MODEL_fc82557ba2d6413a83599bea1c7e9383"
          }
        },
        "5cdf83c45e134c529cf61ca9eb2ba2ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e677297e2d734456a1d18d9348207411",
            "placeholder": "​",
            "style": "IPY_MODEL_7d60b60b6b1d45059135e337bc41fbbe",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "81cc6302100b4027953ce66667057df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63093d97c488456cba21fcb41a412e66",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c99859d81a7451e8811361cb0274b0e",
            "value": 1
          }
        },
        "fc82557ba2d6413a83599bea1c7e9383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e677297e2d734456a1d18d9348207411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d60b60b6b1d45059135e337bc41fbbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63093d97c488456cba21fcb41a412e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c99859d81a7451e8811361cb0274b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}