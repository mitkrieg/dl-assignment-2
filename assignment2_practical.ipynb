{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdOXr7LWKCXok16YP6O9/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d8b58361ff246f69a452b87f3fc9488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fdc9035eec844c0ada47dc0c7bd41b3",
              "IPY_MODEL_2bb6cda9a65547db885f5d015d2711d7"
            ],
            "layout": "IPY_MODEL_1cee6d3c0ef74285b5ebb8705fc996b6"
          }
        },
        "6fdc9035eec844c0ada47dc0c7bd41b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e94f201bbd94a2bb41824e21b1bc639",
            "placeholder": "​",
            "style": "IPY_MODEL_de466fc4533e49d1b5b037e4ff1f6674",
            "value": "0.016 MB of 0.016 MB uploaded\r"
          }
        },
        "2bb6cda9a65547db885f5d015d2711d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f769761a6e7421e8721d3529b2561cf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61d6b42e016741ef8894f6e7a112b8d4",
            "value": 1
          }
        },
        "1cee6d3c0ef74285b5ebb8705fc996b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e94f201bbd94a2bb41824e21b1bc639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de466fc4533e49d1b5b037e4ff1f6674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f769761a6e7421e8721d3529b2561cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61d6b42e016741ef8894f6e7a112b8d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c7fcd1fe53c4bbea581526da5e65aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc5e32ddfc8c4933a7f88f6a549d6979",
              "IPY_MODEL_c293aa7273c24346a195189afb50cfaf"
            ],
            "layout": "IPY_MODEL_07ba3ed2f25d4e2b80d22bccae60b8c5"
          }
        },
        "fc5e32ddfc8c4933a7f88f6a549d6979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97fb2e90c0d4d7798f6c1f3ab739379",
            "placeholder": "​",
            "style": "IPY_MODEL_a7c06511f32d4c298ee3fc6ca1208474",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "c293aa7273c24346a195189afb50cfaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6a0c9c0bc6c4e429e270d6d09c91f53",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58104bba13ae4464b86af6245f712d20",
            "value": 1
          }
        },
        "07ba3ed2f25d4e2b80d22bccae60b8c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a97fb2e90c0d4d7798f6c1f3ab739379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7c06511f32d4c298ee3fc6ca1208474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a0c9c0bc6c4e429e270d6d09c91f53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58104bba13ae4464b86af6245f712d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd92dfab2d524b8f8e58122229f98d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd8724777cba4bf9aa16088b3c2f95d0",
              "IPY_MODEL_a4b053e1acd942d9b33a4b3f2c080520"
            ],
            "layout": "IPY_MODEL_217799e237a147c0b6e2ca65de0decf3"
          }
        },
        "bd8724777cba4bf9aa16088b3c2f95d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf409bccac7f4d7ba863ff1e4ec56e97",
            "placeholder": "​",
            "style": "IPY_MODEL_13b3be981904409f8ea26fda32e31a56",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "a4b053e1acd942d9b33a4b3f2c080520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03fa377792bd4143a0df732761dee601",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f094d9be26f142d4974aaecc42a0ec51",
            "value": 1
          }
        },
        "217799e237a147c0b6e2ca65de0decf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf409bccac7f4d7ba863ff1e4ec56e97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13b3be981904409f8ea26fda32e31a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03fa377792bd4143a0df732761dee601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f094d9be26f142d4974aaecc42a0ec51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "642055a0c4f641be9d7e7403e2905888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e8d4b81ce504b7ea46c841f16d3dea9",
              "IPY_MODEL_b8cca241a0fe46668181ee3056b5c3f4"
            ],
            "layout": "IPY_MODEL_447f00fe7a38425491d4a651a2767f59"
          }
        },
        "4e8d4b81ce504b7ea46c841f16d3dea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5631242a1c1346ef9ebf678dc9a95f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_647825e63c0c4df385e49749f70d35ae",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "b8cca241a0fe46668181ee3056b5c3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edc1138557c94ecb9795d5e32e55187b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_347a90215cb84a8d9e26ee762a69a02e",
            "value": 1
          }
        },
        "447f00fe7a38425491d4a651a2767f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5631242a1c1346ef9ebf678dc9a95f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "647825e63c0c4df385e49749f70d35ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edc1138557c94ecb9795d5e32e55187b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "347a90215cb84a8d9e26ee762a69a02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfd7ddc6a72246208b351104af164755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a105e885077b415d9a4b8b92fcb7c279",
              "IPY_MODEL_8ddf7941600e4d95b2301ff3f17450a1"
            ],
            "layout": "IPY_MODEL_67ce22d827a44ed4902ddb4fde7d0062"
          }
        },
        "a105e885077b415d9a4b8b92fcb7c279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f4e1fa86c6409cb67994246619c25e",
            "placeholder": "​",
            "style": "IPY_MODEL_b2d36dea581249a79e70296d16dd0cef",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "8ddf7941600e4d95b2301ff3f17450a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76317ed9c7e64c958f65bebe0a098df1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc06dcdb2f5b48f4809f021d157d0e71",
            "value": 1
          }
        },
        "67ce22d827a44ed4902ddb4fde7d0062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f4e1fa86c6409cb67994246619c25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2d36dea581249a79e70296d16dd0cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76317ed9c7e64c958f65bebe0a098df1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc06dcdb2f5b48f4809f021d157d0e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85b9aa29e49c493c9a9a754fd4962c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d271aadd4c4346a686478530bac70664",
              "IPY_MODEL_3215affbeebe44b687aea9a342892c1d"
            ],
            "layout": "IPY_MODEL_f2d99449d2dc4256ae7ed915037eb071"
          }
        },
        "d271aadd4c4346a686478530bac70664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_654d579b36514c5686b9139b74b40b01",
            "placeholder": "​",
            "style": "IPY_MODEL_b3f1765c73534a568eee8f066d188c9c",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "3215affbeebe44b687aea9a342892c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cbf77cb429548c687dd3cdd5402f473",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7a766363ac04c51ad8033d376b17a6c",
            "value": 1
          }
        },
        "f2d99449d2dc4256ae7ed915037eb071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "654d579b36514c5686b9139b74b40b01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f1765c73534a568eee8f066d188c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cbf77cb429548c687dd3cdd5402f473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7a766363ac04c51ad8033d376b17a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-2/blob/main/assignment2_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 5787 Deep Learning Assignment 2\n",
        "\n",
        "This notebook implements the \"small\" LSTM model as described in \"Recurrent Neural Network Regularization\" by Zaremba et al (2014)."
      ],
      "metadata": {
        "id": "VkzfolXnJkjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {
        "id": "h5Ra9H5dKOUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Weights & Biases"
      ],
      "metadata": {
        "id": "EmcvGaiIKSW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4R2RS5a0QQ9",
        "outputId": "0e4ee3df-2d07-4a9d-8b83-bb37e44fd4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & GPU Check"
      ],
      "metadata": {
        "id": "wGqfF7ufKaJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import wandb\n",
        "\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "torch.cuda.manual_seed_all(123)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('CUDA GPU Available:',torch.cuda.is_available())\n",
        "print('MPS GPU Available:', torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "#   device = torch.device('mps')\n",
        "#   print('Pytorch GPU Build:',torch.backends.mps.is_built())\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ],
      "metadata": {
        "id": "9BLt6FUx0VtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7ac585-c936-4c95-9a6e-df85b66ed4f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "CUDA GPU Available: True\n",
            "MPS GPU Available: False\n",
            "GPU Name: Tesla T4\n",
            "GPU Count: 1\n",
            "GPU Memory Allocated: 0\n",
            "GPU Memory Cached: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary & PTBText Dataset Classes\n",
        "\n",
        "Parse data from raw files & create dataset class to interact with"
      ],
      "metadata": {
        "id": "FAORGwMbKekd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, pre_built_dict: dict=None):\n",
        "        if pre_built_dict:\n",
        "            self.vocab = pre_built_dict\n",
        "        else:\n",
        "            self.vocab = {'<pad>': 0, '<oov>': 1, '<sos>': 2, '<eos>': 3, '<unk>': 4}\n",
        "        self.idx = len(self.vocab)\n",
        "\n",
        "    def add_word(self, word: str) -> None:\n",
        "        if word not in self.vocab:\n",
        "            self.vocab[word] = self.idx\n",
        "            self.idx += 1\n",
        "\n",
        "    def encode(self, tokens: list[str]) -> list[int]:\n",
        "        return [self.vocab.get(word, self.vocab['<unk>']) for word in tokens]\n",
        "\n",
        "    def decode(self, indicies: list[int]) -> list[str]:\n",
        "        return [list(self.vocab.keys())[list(self.vocab.values()).index(idx)] for idx in indicies]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "class PTBText(Dataset):\n",
        "    def __init__(self, path: str, vocab: Vocab=Vocab(), build_vocab=True, batch_size=20, seqence_length=20, device=torch.device('cpu')):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.vocab = vocab\n",
        "        self.data = self.load_data(build_vocab)\n",
        "        self.batch_size = batch_size\n",
        "        self.chunk_size = len(self.data) // batch_size\n",
        "        self.seq_len = seqence_length\n",
        "        self.minibatches = self.create_batches()\n",
        "\n",
        "    def load_data(self, build_vocab):\n",
        "        data = []\n",
        "        with open(self.path, 'r') as f:\n",
        "            count = 0\n",
        "            for line in f:\n",
        "                count += 1\n",
        "                tokens = line.strip().split() + ['<eos>']\n",
        "                if build_vocab:\n",
        "                    for token in tokens:\n",
        "                        self.vocab.add_word(token)\n",
        "\n",
        "                encoded_line = self.vocab.encode(tokens)\n",
        "                data.extend(encoded_line)\n",
        "        return data\n",
        "\n",
        "    def create_batches(self):\n",
        "        return [self.data[i*self.chunk_size: (i+1)*self.chunk_size] for i in range(self.batch_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, j):\n",
        "        inputs = torch.stack([\n",
        "            torch.LongTensor(self.minibatches[i][j * self.seq_len : (j + 1) * self.seq_len])\n",
        "            for i in range(self.batch_size)], dim=0)\n",
        "        labels = torch.stack([\n",
        "            torch.LongTensor(self.minibatches[i][j * self.seq_len + 1 : (j + 1) * self.seq_len + 1])\n",
        "            for i in range(self.batch_size)], dim=0)\n",
        "\n",
        "        return inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "    def get_tokens(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_decoded_tokens(self, idx):\n",
        "        return self.vocab.decode(self.data[idx])\n",
        "\n",
        "\n",
        "train = PTBText('/content/ptb.train.txt', device=device)\n",
        "val = PTBText('/content/ptb.valid.txt', vocab=train.vocab, build_vocab=False, device=device)\n",
        "test = PTBText('/content/ptb.test.txt', vocab=train.vocab, build_vocab=False, device=device)\n",
        "\n",
        "datasets = {\n",
        "    'train': train,\n",
        "    'val': val,\n",
        "    'test': test\n",
        "}\n",
        "\n",
        "print(\"Vocab size:\", len(train.vocab))\n",
        "print(\"Train data size:\", len(train))\n",
        "print(\"Val data size:\", len(val))\n",
        "print(\"Test data size:\", len(test))"
      ],
      "metadata": {
        "id": "KUza40e_165_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaee58b-2eb6-4e93-c03d-d9adf0191bb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 10003\n",
            "Train data size: 929589\n",
            "Val data size: 73760\n",
            "Test data size: 82430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model Architecture\n",
        "\n",
        "The `ZamrembaRNN` module allows for either LSTM or GRU models to be implmented with or without dropout as described in the paper"
      ],
      "metadata": {
        "id": "N7x6ITOOKlbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ZamrembaRNN(nn.Module):\n",
        "    def __init__(self, rnn_type, vocab_size, batch_size=20, embedding_dim=200, hidden_dim=200, num_layers=2, dropout=0, rnn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "        self.batch_size = batch_size\n",
        "        if rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=rnn_dropout, batch_first=True)\n",
        "        elif rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=rnn_dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid RNN type: must be 'lstm' or 'gru'\")\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        if dropout > 0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "\n",
        "        #LSTM has two states (hidden& cell) where as GRU only has one hidden state\n",
        "        if self.rnn_type == 'lstm':\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        elif self.rnn_type == 'gru':\n",
        "            output, hidden = self.rnn(output, hidden[0])\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        gen = torch.Generator().manual_seed(132)\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.embedding.weight, -initrange, initrange, generator=gen)\n",
        "        nn.init.uniform_(self.rnn.weight_ih_l0, -initrange, initrange, generator=gen)\n",
        "        nn.init.uniform_(self.rnn.weight_hh_l0, -initrange, initrange, generator=gen)\n",
        "        nn.init.uniform_(self.fc.weight, -initrange, initrange, generator=gen)"
      ],
      "metadata": {
        "id": "XRRsn3Df_mAi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training & Evaluation Loops"
      ],
      "metadata": {
        "id": "qjYvdz1ULLOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_hidden(model):\n",
        "    if model.rnn_type == 'lstm':\n",
        "        return (torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device),\n",
        "              torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device))\n",
        "    elif model.rnn_type == 'gru':\n",
        "        return torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device).unsqueeze(0)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid RNN type: must be 'lstm' or 'gru'\")\n",
        "\n",
        "def detach_hidden(hidden):\n",
        "    if isinstance(hidden, tuple):\n",
        "        return tuple([h.detach() for h in hidden])\n",
        "    else:\n",
        "        return hidden.detach()\n",
        "\n",
        "def train_epoch(model, dataset, loss_fn, optimizer, device, epoch, verbosity):\n",
        "    \"\"\"Train one epoch of a network\"\"\"\n",
        "    model.train()\n",
        "    batch_loss = 0\n",
        "\n",
        "    hidden = get_new_hidden(model)\n",
        "\n",
        "    for j in range(dataset.chunk_size // dataset.seq_len):\n",
        "\n",
        "        inputs, labels = dataset[j]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hidden = detach_hidden(hidden)\n",
        "\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        if model.rnn_type == 'gru':\n",
        "            hidden = hidden.unsqueeze(0)\n",
        "\n",
        "        loss = loss_fn(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "        if (j + 1) % verbosity == 0:\n",
        "            print(f'Batch #{j + 1} Loss: {batch_loss / verbosity}')\n",
        "            batch_loss = 0\n",
        "\n",
        "def perplexity(loss, batches):\n",
        "    return math.exp(loss / batches)\n",
        "\n",
        "def evaluate_model(title, model, dataset, loss_fn, seq_len, batch_size, epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataset) // (batch_size * seq_len)\n",
        "\n",
        "    hidden = get_new_hidden(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for j in range(num_batches):\n",
        "\n",
        "            inputs, labels = dataset[j]\n",
        "\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            if model.rnn_type == 'gru':\n",
        "                hidden = hidden.unsqueeze(0)\n",
        "            loss = loss_fn(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    perp = perplexity(total_loss, num_batches)\n",
        "    wandb.log({\n",
        "            f'{title}-loss': total_loss / num_batches,\n",
        "            f'{title}-perplexity': perp\n",
        "        }, step=epoch)\n",
        "\n",
        "    print(f'\\033[92m{title} perplexity: {perp:.6f} ||| loss {total_loss / num_batches:.6f}\\033[0m')\n",
        "\n",
        "    return perp\n",
        "\n",
        "def train_network(model, datasets, loss_fn, optimizer, schedule, device, epochs: int, verbosity: int):\n",
        "    for epoch in range(epochs):\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'----------- Epoch #{epoch + 1}, LR: {lr} ------------')\n",
        "        train_epoch(model, datasets['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "        train_perplexity = evaluate_model('Train', model, datasets['train'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        val_perplexity = evaluate_model('Validation', model, datasets['val'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        test_perplexity = evaluate_model('Test', model, datasets['test'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        print('------------------------------------\\n')\n",
        "\n",
        "        schedule.step()\n",
        "    print('----------- Train Complete! ------------')\n",
        "    return {\n",
        "        'train':train_perplexity,\n",
        "        'val':val_perplexity,\n",
        "        'test':test_perplexity\n",
        "    }"
      ],
      "metadata": {
        "id": "V3z6xiymAzyB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Models"
      ],
      "metadata": {
        "id": "o3Tn4mEGJgxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM No Regularization"
      ],
      "metadata": {
        "id": "ss17f_DgLSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 10\n",
        "learning_rate_decay = 0.5\n",
        "lr = 4\n",
        "dropout_rate = 0\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('lstm', len(train.vocab)).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-quad\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 14, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d8b58361ff246f69a452b87f3fc9488",
            "6fdc9035eec844c0ada47dc0c7bd41b3",
            "2bb6cda9a65547db885f5d015d2711d7",
            "1cee6d3c0ef74285b5ebb8705fc996b6",
            "0e94f201bbd94a2bb41824e21b1bc639",
            "de466fc4533e49d1b5b037e4ff1f6674",
            "1f769761a6e7421e8721d3529b2561cf",
            "61d6b42e016741ef8894f6e7a112b8d4",
            "7c7fcd1fe53c4bbea581526da5e65aeb",
            "fc5e32ddfc8c4933a7f88f6a549d6979",
            "c293aa7273c24346a195189afb50cfaf",
            "07ba3ed2f25d4e2b80d22bccae60b8c5",
            "a97fb2e90c0d4d7798f6c1f3ab739379",
            "a7c06511f32d4c298ee3fc6ca1208474",
            "c6a0c9c0bc6c4e429e270d6d09c91f53",
            "58104bba13ae4464b86af6245f712d20"
          ]
        },
        "id": "0CDIw79OTAr6",
        "outputId": "458c55b8-3011-488b-c654-5a2b8ff93e20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:cklubvhv) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d8b58361ff246f69a452b87f3fc9488"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▄▄▄▄▅▅▅▂▆▁▁</td></tr><tr><td>Test-perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▄▄▄▄▅▅▅▂▆▁▁</td></tr><tr><td>Train-perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▅▄▄▄▅▅▅▂▆▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>6.47588</td></tr><tr><td>Test-perplexity</td><td>649.29014</td></tr><tr><td>Train-loss</td><td>6.52741</td></tr><tr><td>Train-perplexity</td><td>683.62764</td></tr><tr><td>Validation-loss</td><td>6.54278</td></tr><tr><td>Validation-perplexity</td><td>694.21434</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">golden-gorge-5</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/cklubvhv' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/cklubvhv</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240925_220257-cklubvhv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:cklubvhv). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240925_220612-aedi1n1t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/aedi1n1t' target=\"_blank\">sweet-resonance-6</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/aedi1n1t' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/aedi1n1t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 4 ------------\n",
            "Batch #500 Loss: 6.814496244430542\n",
            "Batch #1000 Loss: 6.190024091720581\n",
            "Batch #1500 Loss: 5.95184453201294\n",
            "Batch #2000 Loss: 5.80175147819519\n",
            "\u001b[92mTrain perplexity: 288.813738 ||| loss 5.665782\u001b[0m\n",
            "\u001b[92mValidation perplexity: 294.080869 ||| loss 5.683855\u001b[0m\n",
            "\u001b[92mTest perplexity: 287.957971 ||| loss 5.662815\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 4 ------------\n",
            "Batch #500 Loss: 5.615063290596009\n",
            "Batch #1000 Loss: 5.535354831695557\n",
            "Batch #1500 Loss: 5.4458003463745115\n",
            "Batch #2000 Loss: 5.38455379486084\n",
            "\u001b[92mTrain perplexity: 203.791267 ||| loss 5.317096\u001b[0m\n",
            "\u001b[92mValidation perplexity: 220.461390 ||| loss 5.395723\u001b[0m\n",
            "\u001b[92mTest perplexity: 214.807699 ||| loss 5.369743\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 4 ------------\n",
            "Batch #500 Loss: 5.295789780616761\n",
            "Batch #1000 Loss: 5.249943510055542\n",
            "Batch #1500 Loss: 5.1965356426239016\n",
            "Batch #2000 Loss: 5.156316002845764\n",
            "\u001b[92mTrain perplexity: 166.985471 ||| loss 5.117907\u001b[0m\n",
            "\u001b[92mValidation perplexity: 190.198644 ||| loss 5.248069\u001b[0m\n",
            "\u001b[92mTest perplexity: 184.965189 ||| loss 5.220168\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 4 ------------\n",
            "Batch #500 Loss: 5.1005642786026\n",
            "Batch #1000 Loss: 5.059943042755127\n",
            "Batch #1500 Loss: 5.022519619941711\n",
            "Batch #2000 Loss: 4.993981801986695\n",
            "\u001b[92mTrain perplexity: 141.519327 ||| loss 4.952436\u001b[0m\n",
            "\u001b[92mValidation perplexity: 169.197348 ||| loss 5.131066\u001b[0m\n",
            "\u001b[92mTest perplexity: 164.653321 ||| loss 5.103842\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 4 ------------\n",
            "Batch #500 Loss: 4.952375690460205\n",
            "Batch #1000 Loss: 4.910931775093078\n",
            "Batch #1500 Loss: 4.88071145439148\n",
            "Batch #2000 Loss: 4.8619288072586055\n",
            "\u001b[92mTrain perplexity: 124.163616 ||| loss 4.821600\u001b[0m\n",
            "\u001b[92mValidation perplexity: 156.063353 ||| loss 5.050262\u001b[0m\n",
            "\u001b[92mTest perplexity: 152.700183 ||| loss 5.028476\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 4 ------------\n",
            "Batch #500 Loss: 4.828995505332947\n",
            "Batch #1000 Loss: 4.785153299331665\n",
            "Batch #1500 Loss: 4.760435444355011\n",
            "Batch #2000 Loss: 4.748765632629395\n",
            "\u001b[92mTrain perplexity: 110.938002 ||| loss 4.708972\u001b[0m\n",
            "\u001b[92mValidation perplexity: 147.087353 ||| loss 4.991027\u001b[0m\n",
            "\u001b[92mTest perplexity: 143.246689 ||| loss 4.964568\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 4 ------------\n",
            "Batch #500 Loss: 4.722463170051575\n",
            "Batch #1000 Loss: 4.676240677833557\n",
            "Batch #1500 Loss: 4.657453967094422\n",
            "Batch #2000 Loss: 4.650091633319855\n",
            "\u001b[92mTrain perplexity: 101.016399 ||| loss 4.615283\u001b[0m\n",
            "\u001b[92mValidation perplexity: 140.863579 ||| loss 4.947792\u001b[0m\n",
            "\u001b[92mTest perplexity: 137.616502 ||| loss 4.924471\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 4 ------------\n",
            "Batch #500 Loss: 4.627607270717621\n",
            "Batch #1000 Loss: 4.579852375030518\n",
            "Batch #1500 Loss: 4.567068667411804\n",
            "Batch #2000 Loss: 4.564118046283722\n",
            "\u001b[92mTrain perplexity: 93.519098 ||| loss 4.538166\u001b[0m\n",
            "\u001b[92mValidation perplexity: 137.579307 ||| loss 4.924201\u001b[0m\n",
            "\u001b[92mTest perplexity: 134.657136 ||| loss 4.902732\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 4 ------------\n",
            "Batch #500 Loss: 4.5456001672744755\n",
            "Batch #1000 Loss: 4.494837258338928\n",
            "Batch #1500 Loss: 4.486348701953888\n",
            "Batch #2000 Loss: 4.487645203590393\n",
            "\u001b[92mTrain perplexity: 87.292133 ||| loss 4.469260\u001b[0m\n",
            "\u001b[92mValidation perplexity: 135.327869 ||| loss 4.907700\u001b[0m\n",
            "\u001b[92mTest perplexity: 132.047492 ||| loss 4.883162\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 4 ------------\n",
            "Batch #500 Loss: 4.471367019176483\n",
            "Batch #1000 Loss: 4.420014813899994\n",
            "Batch #1500 Loss: 4.41467496585846\n",
            "Batch #2000 Loss: 4.416802980422974\n",
            "\u001b[92mTrain perplexity: 81.925785 ||| loss 4.405814\u001b[0m\n",
            "\u001b[92mValidation perplexity: 134.102498 ||| loss 4.898604\u001b[0m\n",
            "\u001b[92mTest perplexity: 130.962141 ||| loss 4.874908\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 2.0 ------------\n",
            "Batch #500 Loss: 4.383173785209656\n",
            "Batch #1000 Loss: 4.298415382385254\n",
            "Batch #1500 Loss: 4.270872814655304\n",
            "Batch #2000 Loss: 4.254927682876587\n",
            "\u001b[92mTrain perplexity: 71.857896 ||| loss 4.274691\u001b[0m\n",
            "\u001b[92mValidation perplexity: 129.505050 ||| loss 4.863720\u001b[0m\n",
            "\u001b[92mTest perplexity: 126.213051 ||| loss 4.837971\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 1.0 ------------\n",
            "Batch #500 Loss: 4.29715141916275\n",
            "Batch #1000 Loss: 4.212103711605072\n",
            "Batch #1500 Loss: 4.179617018699646\n",
            "Batch #2000 Loss: 4.158776790142059\n",
            "\u001b[92mTrain perplexity: 66.302919 ||| loss 4.194234\u001b[0m\n",
            "\u001b[92mValidation perplexity: 127.869577 ||| loss 4.851011\u001b[0m\n",
            "\u001b[92mTest perplexity: 124.129267 ||| loss 4.821324\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 0.5 ------------\n",
            "Batch #500 Loss: 4.247687514305115\n",
            "Batch #1000 Loss: 4.16312681722641\n",
            "Batch #1500 Loss: 4.12942155456543\n",
            "Batch #2000 Loss: 4.1070581865310665\n",
            "\u001b[92mTrain perplexity: 63.138532 ||| loss 4.145331\u001b[0m\n",
            "\u001b[92mValidation perplexity: 126.955453 ||| loss 4.843836\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.997843 ||| loss 4.812167\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 0.25 ------------\n",
            "Batch #500 Loss: 4.220257702350616\n",
            "Batch #1000 Loss: 4.137447773933411\n",
            "Batch #1500 Loss: 4.102935510158539\n",
            "Batch #2000 Loss: 4.0797562069892885\n",
            "\u001b[92mTrain perplexity: 61.226368 ||| loss 4.114578\u001b[0m\n",
            "\u001b[92mValidation perplexity: 126.268721 ||| loss 4.838412\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.204430 ||| loss 4.805695\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c7fcd1fe53c4bbea581526da5e65aeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▆▅▄▄▃▃▃▂▂▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.8057</td></tr><tr><td>Test-perplexity</td><td>122.20443</td></tr><tr><td>Train-loss</td><td>4.11458</td></tr><tr><td>Train-perplexity</td><td>61.22637</td></tr><tr><td>Validation-loss</td><td>4.83841</td></tr><tr><td>Validation-perplexity</td><td>126.26872</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sweet-resonance-6</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/aedi1n1t' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad/runs/aedi1n1t</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-quad</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240925_220612-aedi1n1t/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM with Dropout"
      ],
      "metadata": {
        "id": "UQDV3GOtLZml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 11\n",
        "learning_rate_decay = 0.75\n",
        "lr = 6\n",
        "dropout_rate = 0.5\n",
        "lstm_dropout = 0.2\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('lstm', len(train.vocab), dropout=dropout_rate, rnn_dropout=lstm_dropout).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-tri\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'lstm_dropout':lstm_dropout,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 25, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fd92dfab2d524b8f8e58122229f98d59",
            "bd8724777cba4bf9aa16088b3c2f95d0",
            "a4b053e1acd942d9b33a4b3f2c080520",
            "217799e237a147c0b6e2ca65de0decf3",
            "cf409bccac7f4d7ba863ff1e4ec56e97",
            "13b3be981904409f8ea26fda32e31a56",
            "03fa377792bd4143a0df732761dee601",
            "f094d9be26f142d4974aaecc42a0ec51"
          ]
        },
        "id": "baKhTg-1LYrI",
        "outputId": "158cd399-4d6f-40ca-b10c-acddf3305ce4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240925_195926-l9zibc0w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/l9zibc0w' target=\"_blank\">dashing-plant-44</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/l9zibc0w' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/l9zibc0w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 6 ------------\n",
            "Batch #500 Loss: 6.763347356796265\n",
            "Batch #1000 Loss: 6.229209800720215\n",
            "Batch #1500 Loss: 5.98650328540802\n",
            "Batch #2000 Loss: 5.827206930160522\n",
            "\u001b[92mTrain perplexity: 265.904277 ||| loss 5.583136\u001b[0m\n",
            "\u001b[92mValidation perplexity: 271.359342 ||| loss 5.603444\u001b[0m\n",
            "\u001b[92mTest perplexity: 266.137427 ||| loss 5.584013\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 6 ------------\n",
            "Batch #500 Loss: 5.658116242408752\n",
            "Batch #1000 Loss: 5.586261615753174\n",
            "Batch #1500 Loss: 5.505580702781677\n",
            "Batch #2000 Loss: 5.445070253372192\n",
            "\u001b[92mTrain perplexity: 183.753716 ||| loss 5.213596\u001b[0m\n",
            "\u001b[92mValidation perplexity: 197.874569 ||| loss 5.287633\u001b[0m\n",
            "\u001b[92mTest perplexity: 192.846486 ||| loss 5.261894\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 6 ------------\n",
            "Batch #500 Loss: 5.376493459701538\n",
            "Batch #1000 Loss: 5.333149443626404\n",
            "Batch #1500 Loss: 5.2882169313430785\n",
            "Batch #2000 Loss: 5.256061985969543\n",
            "\u001b[92mTrain perplexity: 150.616347 ||| loss 5.014736\u001b[0m\n",
            "\u001b[92mValidation perplexity: 169.584283 ||| loss 5.133350\u001b[0m\n",
            "\u001b[92mTest perplexity: 165.406402 ||| loss 5.108405\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 6 ------------\n",
            "Batch #500 Loss: 5.217459015846252\n",
            "Batch #1000 Loss: 5.179496033668518\n",
            "Batch #1500 Loss: 5.147639248847962\n",
            "Batch #2000 Loss: 5.132258419036865\n",
            "\u001b[92mTrain perplexity: 131.288558 ||| loss 4.877398\u001b[0m\n",
            "\u001b[92mValidation perplexity: 152.711215 ||| loss 5.028549\u001b[0m\n",
            "\u001b[92mTest perplexity: 149.184063 ||| loss 5.005181\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 6 ------------\n",
            "Batch #500 Loss: 5.107024456977844\n",
            "Batch #1000 Loss: 5.0694794120788575\n",
            "Batch #1500 Loss: 5.050835129737854\n",
            "Batch #2000 Loss: 5.03734954071045\n",
            "\u001b[92mTrain perplexity: 118.437693 ||| loss 4.774387\u001b[0m\n",
            "\u001b[92mValidation perplexity: 141.585875 ||| loss 4.952906\u001b[0m\n",
            "\u001b[92mTest perplexity: 138.353277 ||| loss 4.929810\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 6 ------------\n",
            "Batch #500 Loss: 5.026852187156678\n",
            "Batch #1000 Loss: 4.992242564201355\n",
            "Batch #1500 Loss: 4.9744319238662715\n",
            "Batch #2000 Loss: 4.974126847267151\n",
            "\u001b[92mTrain perplexity: 108.616813 ||| loss 4.687826\u001b[0m\n",
            "\u001b[92mValidation perplexity: 134.071360 ||| loss 4.898372\u001b[0m\n",
            "\u001b[92mTest perplexity: 130.939614 ||| loss 4.874736\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 6 ------------\n",
            "Batch #500 Loss: 4.960475533485413\n",
            "Batch #1000 Loss: 4.9263418703079225\n",
            "Batch #1500 Loss: 4.916201029777527\n",
            "Batch #2000 Loss: 4.915418807029724\n",
            "\u001b[92mTrain perplexity: 101.315488 ||| loss 4.618239\u001b[0m\n",
            "\u001b[92mValidation perplexity: 127.994998 ||| loss 4.851991\u001b[0m\n",
            "\u001b[92mTest perplexity: 125.574652 ||| loss 4.832900\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 6 ------------\n",
            "Batch #500 Loss: 4.907649106025696\n",
            "Batch #1000 Loss: 4.872106245994567\n",
            "Batch #1500 Loss: 4.871768929481506\n",
            "Batch #2000 Loss: 4.8677378883361815\n",
            "\u001b[92mTrain perplexity: 96.101403 ||| loss 4.565404\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.247576 ||| loss 4.822276\u001b[0m\n",
            "\u001b[92mTest perplexity: 121.398160 ||| loss 4.799076\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 6 ------------\n",
            "Batch #500 Loss: 4.862363732337951\n",
            "Batch #1000 Loss: 4.827556169509887\n",
            "Batch #1500 Loss: 4.830601161003113\n",
            "Batch #2000 Loss: 4.829658273696899\n",
            "\u001b[92mTrain perplexity: 90.853242 ||| loss 4.509245\u001b[0m\n",
            "\u001b[92mValidation perplexity: 119.943805 ||| loss 4.787023\u001b[0m\n",
            "\u001b[92mTest perplexity: 117.441947 ||| loss 4.765944\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 6 ------------\n",
            "Batch #500 Loss: 4.827549146652221\n",
            "Batch #1000 Loss: 4.789548171997071\n",
            "Batch #1500 Loss: 4.795007291793823\n",
            "Batch #2000 Loss: 4.798644776344299\n",
            "\u001b[92mTrain perplexity: 87.458175 ||| loss 4.471161\u001b[0m\n",
            "\u001b[92mValidation perplexity: 117.998710 ||| loss 4.770674\u001b[0m\n",
            "\u001b[92mTest perplexity: 115.113677 ||| loss 4.745920\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 6 ------------\n",
            "Batch #500 Loss: 4.7937859411239625\n",
            "Batch #1000 Loss: 4.7534056673049925\n",
            "Batch #1500 Loss: 4.756941323280334\n",
            "Batch #2000 Loss: 4.768098460197448\n",
            "\u001b[92mTrain perplexity: 84.249852 ||| loss 4.433787\u001b[0m\n",
            "\u001b[92mValidation perplexity: 115.601380 ||| loss 4.750148\u001b[0m\n",
            "\u001b[92mTest perplexity: 112.878165 ||| loss 4.726309\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 4.5 ------------\n",
            "Batch #500 Loss: 4.755071650028229\n",
            "Batch #1000 Loss: 4.707028944015503\n",
            "Batch #1500 Loss: 4.705454873085022\n",
            "Batch #2000 Loss: 4.706948844909668\n",
            "\u001b[92mTrain perplexity: 78.161621 ||| loss 4.358779\u001b[0m\n",
            "\u001b[92mValidation perplexity: 110.874029 ||| loss 4.708395\u001b[0m\n",
            "\u001b[92mTest perplexity: 107.804019 ||| loss 4.680315\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 3.375 ------------\n",
            "Batch #500 Loss: 4.709173620700836\n",
            "Batch #1000 Loss: 4.658705662727356\n",
            "Batch #1500 Loss: 4.653226839542389\n",
            "Batch #2000 Loss: 4.6622742004394535\n",
            "\u001b[92mTrain perplexity: 74.543319 ||| loss 4.311380\u001b[0m\n",
            "\u001b[92mValidation perplexity: 108.193003 ||| loss 4.683917\u001b[0m\n",
            "\u001b[92mTest perplexity: 105.342309 ||| loss 4.657215\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 2.53125 ------------\n",
            "Batch #500 Loss: 4.672242458343506\n",
            "Batch #1000 Loss: 4.623487093448639\n",
            "Batch #1500 Loss: 4.617293438911438\n",
            "Batch #2000 Loss: 4.620694615364075\n",
            "\u001b[92mTrain perplexity: 71.708849 ||| loss 4.272614\u001b[0m\n",
            "\u001b[92mValidation perplexity: 106.243022 ||| loss 4.665729\u001b[0m\n",
            "\u001b[92mTest perplexity: 103.058450 ||| loss 4.635296\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15, LR: 1.8984375 ------------\n",
            "Batch #500 Loss: 4.642085168361664\n",
            "Batch #1000 Loss: 4.587389070987701\n",
            "Batch #1500 Loss: 4.585728419303894\n",
            "Batch #2000 Loss: 4.590054343223572\n",
            "\u001b[92mTrain perplexity: 69.444648 ||| loss 4.240530\u001b[0m\n",
            "\u001b[92mValidation perplexity: 104.520723 ||| loss 4.649385\u001b[0m\n",
            "\u001b[92mTest perplexity: 101.345254 ||| loss 4.618533\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16, LR: 1.423828125 ------------\n",
            "Batch #500 Loss: 4.6216488466262815\n",
            "Batch #1000 Loss: 4.5658892993927\n",
            "Batch #1500 Loss: 4.56264056634903\n",
            "Batch #2000 Loss: 4.567939594745636\n",
            "\u001b[92mTrain perplexity: 67.708867 ||| loss 4.215217\u001b[0m\n",
            "\u001b[92mValidation perplexity: 103.214254 ||| loss 4.636807\u001b[0m\n",
            "\u001b[92mTest perplexity: 99.932158 ||| loss 4.604492\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17, LR: 1.06787109375 ------------\n",
            "Batch #500 Loss: 4.603432002067566\n",
            "Batch #1000 Loss: 4.544381371498108\n",
            "Batch #1500 Loss: 4.549010717391968\n",
            "Batch #2000 Loss: 4.54989683675766\n",
            "\u001b[92mTrain perplexity: 66.522028 ||| loss 4.197533\u001b[0m\n",
            "\u001b[92mValidation perplexity: 102.400533 ||| loss 4.628892\u001b[0m\n",
            "\u001b[92mTest perplexity: 99.157627 ||| loss 4.596711\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18, LR: 0.8009033203125 ------------\n",
            "Batch #500 Loss: 4.590645541667938\n",
            "Batch #1000 Loss: 4.531670098304748\n",
            "Batch #1500 Loss: 4.532038239955902\n",
            "Batch #2000 Loss: 4.534936419963836\n",
            "\u001b[92mTrain perplexity: 65.633142 ||| loss 4.184081\u001b[0m\n",
            "\u001b[92mValidation perplexity: 101.915669 ||| loss 4.624146\u001b[0m\n",
            "\u001b[92mTest perplexity: 98.592080 ||| loss 4.590991\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19, LR: 0.600677490234375 ------------\n",
            "Batch #500 Loss: 4.581699630260467\n",
            "Batch #1000 Loss: 4.523200025558472\n",
            "Batch #1500 Loss: 4.5207827334404\n",
            "Batch #2000 Loss: 4.528070062637329\n",
            "\u001b[92mTrain perplexity: 64.911703 ||| loss 4.173028\u001b[0m\n",
            "\u001b[92mValidation perplexity: 101.426670 ||| loss 4.619336\u001b[0m\n",
            "\u001b[92mTest perplexity: 98.086642 ||| loss 4.585851\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20, LR: 0.45050811767578125 ------------\n",
            "Batch #500 Loss: 4.56913969707489\n",
            "Batch #1000 Loss: 4.517657227039337\n",
            "Batch #1500 Loss: 4.515100915431976\n",
            "Batch #2000 Loss: 4.515937972068786\n",
            "\u001b[92mTrain perplexity: 64.418050 ||| loss 4.165394\u001b[0m\n",
            "\u001b[92mValidation perplexity: 101.021036 ||| loss 4.615329\u001b[0m\n",
            "\u001b[92mTest perplexity: 97.649077 ||| loss 4.581380\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21, LR: 0.33788108825683594 ------------\n",
            "Batch #500 Loss: 4.5628586177825925\n",
            "Batch #1000 Loss: 4.509501156330109\n",
            "Batch #1500 Loss: 4.50677881193161\n",
            "Batch #2000 Loss: 4.512281797885895\n",
            "\u001b[92mTrain perplexity: 64.006467 ||| loss 4.158984\u001b[0m\n",
            "\u001b[92mValidation perplexity: 100.714680 ||| loss 4.612292\u001b[0m\n",
            "\u001b[92mTest perplexity: 97.205115 ||| loss 4.576823\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22, LR: 0.25341081619262695 ------------\n",
            "Batch #500 Loss: 4.558172004699707\n",
            "Batch #1000 Loss: 4.5056684885025025\n",
            "Batch #1500 Loss: 4.500515224456787\n",
            "Batch #2000 Loss: 4.5085679955482485\n",
            "\u001b[92mTrain perplexity: 63.669473 ||| loss 4.153705\u001b[0m\n",
            "\u001b[92mValidation perplexity: 100.475397 ||| loss 4.609913\u001b[0m\n",
            "\u001b[92mTest perplexity: 96.953465 ||| loss 4.574231\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23, LR: 0.19005811214447021 ------------\n",
            "Batch #500 Loss: 4.553723240375519\n",
            "Batch #1000 Loss: 4.500239157676697\n",
            "Batch #1500 Loss: 4.497889380931854\n",
            "Batch #2000 Loss: 4.503390612125397\n",
            "\u001b[92mTrain perplexity: 63.524193 ||| loss 4.151421\u001b[0m\n",
            "\u001b[92mValidation perplexity: 100.393000 ||| loss 4.609092\u001b[0m\n",
            "\u001b[92mTest perplexity: 96.796151 ||| loss 4.572607\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24, LR: 0.14254358410835266 ------------\n",
            "Batch #500 Loss: 4.55158018732071\n",
            "Batch #1000 Loss: 4.498512252807617\n",
            "Batch #1500 Loss: 4.4975929007530215\n",
            "Batch #2000 Loss: 4.500510771274566\n",
            "\u001b[92mTrain perplexity: 63.361402 ||| loss 4.148855\u001b[0m\n",
            "\u001b[92mValidation perplexity: 100.295781 ||| loss 4.608124\u001b[0m\n",
            "\u001b[92mTest perplexity: 96.699515 ||| loss 4.571608\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25, LR: 0.1069076880812645 ------------\n",
            "Batch #500 Loss: 4.550971925735474\n",
            "Batch #1000 Loss: 4.493854329586029\n",
            "Batch #1500 Loss: 4.495091830730439\n",
            "Batch #2000 Loss: 4.497498232841492\n",
            "\u001b[92mTrain perplexity: 63.239659 ||| loss 4.146932\u001b[0m\n",
            "\u001b[92mValidation perplexity: 100.199308 ||| loss 4.607161\u001b[0m\n",
            "\u001b[92mTest perplexity: 96.624854 ||| loss 4.570836\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd92dfab2d524b8f8e58122229f98d59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.57084</td></tr><tr><td>Test-perplexity</td><td>96.62485</td></tr><tr><td>Train-loss</td><td>4.14693</td></tr><tr><td>Train-perplexity</td><td>63.23966</td></tr><tr><td>Validation-loss</td><td>4.60716</td></tr><tr><td>Validation-perplexity</td><td>100.19931</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dashing-plant-44</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/l9zibc0w' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/l9zibc0w</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240925_195926-l9zibc0w/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU No Regularization"
      ],
      "metadata": {
        "id": "p_T8dAqcLosM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 6\n",
        "learning_rate_decay = 0.5\n",
        "lr = 1\n",
        "dropout_rate = 0\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('gru', len(train.vocab)).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-tri\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 14, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "642055a0c4f641be9d7e7403e2905888",
            "4e8d4b81ce504b7ea46c841f16d3dea9",
            "b8cca241a0fe46668181ee3056b5c3f4",
            "447f00fe7a38425491d4a651a2767f59",
            "5631242a1c1346ef9ebf678dc9a95f6b",
            "647825e63c0c4df385e49749f70d35ae",
            "edc1138557c94ecb9795d5e32e55187b",
            "347a90215cb84a8d9e26ee762a69a02e",
            "dfd7ddc6a72246208b351104af164755",
            "a105e885077b415d9a4b8b92fcb7c279",
            "8ddf7941600e4d95b2301ff3f17450a1",
            "67ce22d827a44ed4902ddb4fde7d0062",
            "94f4e1fa86c6409cb67994246619c25e",
            "b2d36dea581249a79e70296d16dd0cef",
            "76317ed9c7e64c958f65bebe0a098df1",
            "dc06dcdb2f5b48f4809f021d157d0e71"
          ]
        },
        "id": "8VG4kuatBR_r",
        "outputId": "1bbc746f-b903-4a60-e647-8e02290b6cfc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:w96mb89c) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "642055a0c4f641be9d7e7403e2905888"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dashing-elevator-47</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/w96mb89c' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/w96mb89c</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240925_200702-w96mb89c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:w96mb89c). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240925_200721-zi8iwbue</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/zi8iwbue' target=\"_blank\">glorious-fire-48</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/zi8iwbue' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/zi8iwbue</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 1 ------------\n",
            "Batch #500 Loss: 6.739433595657348\n",
            "Batch #1000 Loss: 6.212292663574218\n",
            "Batch #1500 Loss: 5.9382801446914675\n",
            "Batch #2000 Loss: 5.768225525856018\n",
            "\u001b[92mTrain perplexity: 276.808866 ||| loss 5.623327\u001b[0m\n",
            "\u001b[92mValidation perplexity: 284.441684 ||| loss 5.650528\u001b[0m\n",
            "\u001b[92mTest perplexity: 277.617216 ||| loss 5.626243\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 1 ------------\n",
            "Batch #500 Loss: 5.572338928222656\n",
            "Batch #1000 Loss: 5.485908900260926\n",
            "Batch #1500 Loss: 5.387465888977051\n",
            "Batch #2000 Loss: 5.321869004249573\n",
            "\u001b[92mTrain perplexity: 192.254633 ||| loss 5.258821\u001b[0m\n",
            "\u001b[92mValidation perplexity: 211.627044 ||| loss 5.354825\u001b[0m\n",
            "\u001b[92mTest perplexity: 206.307696 ||| loss 5.329369\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 1 ------------\n",
            "Batch #500 Loss: 5.230482802391053\n",
            "Batch #1000 Loss: 5.177635939598083\n",
            "Batch #1500 Loss: 5.1146213245391845\n",
            "Batch #2000 Loss: 5.074763534545898\n",
            "\u001b[92mTrain perplexity: 151.951717 ||| loss 5.023563\u001b[0m\n",
            "\u001b[92mValidation perplexity: 177.944561 ||| loss 5.181472\u001b[0m\n",
            "\u001b[92mTest perplexity: 173.562060 ||| loss 5.156535\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 1 ------------\n",
            "Batch #500 Loss: 5.016349548339844\n",
            "Batch #1000 Loss: 4.970461405754089\n",
            "Batch #1500 Loss: 4.924133978843689\n",
            "Batch #2000 Loss: 4.897825441360474\n",
            "\u001b[92mTrain perplexity: 127.302022 ||| loss 4.846562\u001b[0m\n",
            "\u001b[92mValidation perplexity: 158.598513 ||| loss 5.066376\u001b[0m\n",
            "\u001b[92mTest perplexity: 154.885994 ||| loss 5.042689\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 1 ------------\n",
            "Batch #500 Loss: 4.855009938240051\n",
            "Batch #1000 Loss: 4.810623522758484\n",
            "Batch #1500 Loss: 4.774712313652039\n",
            "Batch #2000 Loss: 4.756831029891968\n",
            "\u001b[92mTrain perplexity: 110.873621 ||| loss 4.708391\u001b[0m\n",
            "\u001b[92mValidation perplexity: 147.045359 ||| loss 4.990741\u001b[0m\n",
            "\u001b[92mTest perplexity: 143.861789 ||| loss 4.968853\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 1 ------------\n",
            "Batch #500 Loss: 4.723305136680603\n",
            "Batch #1000 Loss: 4.67817564868927\n",
            "Batch #1500 Loss: 4.65044617986679\n",
            "Batch #2000 Loss: 4.6389024310112\n",
            "\u001b[92mTrain perplexity: 99.025147 ||| loss 4.595374\u001b[0m\n",
            "\u001b[92mValidation perplexity: 140.108073 ||| loss 4.942414\u001b[0m\n",
            "\u001b[92mTest perplexity: 137.388132 ||| loss 4.922810\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 0.5 ------------\n",
            "Batch #500 Loss: 4.592615532398224\n",
            "Batch #1000 Loss: 4.527901623725891\n",
            "Batch #1500 Loss: 4.494719846248627\n",
            "Batch #2000 Loss: 4.4778439660072324\n",
            "\u001b[92mTrain perplexity: 87.625862 ||| loss 4.473076\u001b[0m\n",
            "\u001b[92mValidation perplexity: 131.201816 ||| loss 4.876737\u001b[0m\n",
            "\u001b[92mTest perplexity: 128.441862 ||| loss 4.855476\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 0.25 ------------\n",
            "Batch #500 Loss: 4.511560193061829\n",
            "Batch #1000 Loss: 4.446879031658173\n",
            "Batch #1500 Loss: 4.413697110652923\n",
            "Batch #2000 Loss: 4.395709005355835\n",
            "\u001b[92mTrain perplexity: 81.781680 ||| loss 4.404053\u001b[0m\n",
            "\u001b[92mValidation perplexity: 126.695686 ||| loss 4.841788\u001b[0m\n",
            "\u001b[92mTest perplexity: 123.781359 ||| loss 4.818517\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 0.125 ------------\n",
            "Batch #500 Loss: 4.466450800895691\n",
            "Batch #1000 Loss: 4.403252962112426\n",
            "Batch #1500 Loss: 4.37089256811142\n",
            "Batch #2000 Loss: 4.352253232479096\n",
            "\u001b[92mTrain perplexity: 78.954029 ||| loss 4.368866\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.572158 ||| loss 4.824885\u001b[0m\n",
            "\u001b[92mTest perplexity: 121.532044 ||| loss 4.800178\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 0.0625 ------------\n",
            "Batch #500 Loss: 4.441905447483062\n",
            "Batch #1000 Loss: 4.3798722581863405\n",
            "Batch #1500 Loss: 4.348241357803345\n",
            "Batch #2000 Loss: 4.329121109962464\n",
            "\u001b[92mTrain perplexity: 77.483389 ||| loss 4.350064\u001b[0m\n",
            "\u001b[92mValidation perplexity: 123.403410 ||| loss 4.815459\u001b[0m\n",
            "\u001b[92mTest perplexity: 120.236062 ||| loss 4.789457\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 0.03125 ------------\n",
            "Batch #500 Loss: 4.428710752010345\n",
            "Batch #1000 Loss: 4.36732149887085\n",
            "Batch #1500 Loss: 4.336189767360687\n",
            "Batch #2000 Loss: 4.316694864273071\n",
            "\u001b[92mTrain perplexity: 76.653728 ||| loss 4.339298\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.707436 ||| loss 4.809803\u001b[0m\n",
            "\u001b[92mTest perplexity: 119.383621 ||| loss 4.782342\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 0.015625 ------------\n",
            "Batch #500 Loss: 4.4215827860832215\n",
            "Batch #1000 Loss: 4.360612024307251\n",
            "Batch #1500 Loss: 4.329787972450256\n",
            "Batch #2000 Loss: 4.310022560119629\n",
            "\u001b[92mTrain perplexity: 76.247414 ||| loss 4.333984\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.363581 ||| loss 4.806997\u001b[0m\n",
            "\u001b[92mTest perplexity: 118.929601 ||| loss 4.778532\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 0.0078125 ------------\n",
            "Batch #500 Loss: 4.4177974424362185\n",
            "Batch #1000 Loss: 4.357052966594696\n",
            "Batch #1500 Loss: 4.326388722896576\n",
            "Batch #2000 Loss: 4.306463602542877\n",
            "\u001b[92mTrain perplexity: 76.055037 ||| loss 4.331457\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.175260 ||| loss 4.805457\u001b[0m\n",
            "\u001b[92mTest perplexity: 118.706304 ||| loss 4.776652\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 0.00390625 ------------\n",
            "Batch #500 Loss: 4.41584764289856\n",
            "Batch #1000 Loss: 4.35515608549118\n",
            "Batch #1500 Loss: 4.324584523200989\n",
            "Batch #2000 Loss: 4.304592782974243\n",
            "\u001b[92mTrain perplexity: 75.963070 ||| loss 4.330247\u001b[0m\n",
            "\u001b[92mValidation perplexity: 122.054503 ||| loss 4.804468\u001b[0m\n",
            "\u001b[92mTest perplexity: 118.582539 ||| loss 4.775609\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfd7ddc6a72246208b351104af164755"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▄▃▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▅▄▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▄▃▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.77561</td></tr><tr><td>Test-perplexity</td><td>118.58254</td></tr><tr><td>Train-loss</td><td>4.33025</td></tr><tr><td>Train-perplexity</td><td>75.96307</td></tr><tr><td>Validation-loss</td><td>4.80447</td></tr><tr><td>Validation-perplexity</td><td>122.0545</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glorious-fire-48</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/zi8iwbue' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/zi8iwbue</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240925_200721-zi8iwbue/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU with Dropout"
      ],
      "metadata": {
        "id": "V9EdqsJCLv1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 20\n",
        "learning_rate_decay = 0.75\n",
        "lr = 1\n",
        "dropout_rate = 0.5\n",
        "gru_dropout = 0.2\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('gru', len(train.vocab), dropout=dropout_rate, rnn_dropout=gru_dropout).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"dl-assignment2-tri\", config={\n",
        "    'batch_size':datasets['train'].batch_size,\n",
        "    'embedding_size':model.embedding_dim,\n",
        "    'hidden_units':model.hidden_dim,\n",
        "    'num_lstm_layers':model.num_layers,\n",
        "    'dropout_rate':dropout_rate,\n",
        "    'lstm_dropout':gru_dropout,\n",
        "    'decay_at':decay_start,\n",
        "    'learning_rate_decay':learning_rate_decay,\n",
        "    'learning_rate_start':lr,\n",
        "    'optimizer':'SGD',\n",
        "    'seq_len':datasets['train'].seq_len,\n",
        "    'rnn_type':model.rnn_type\n",
        "})\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 25, 500)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "85b9aa29e49c493c9a9a754fd4962c5d",
            "d271aadd4c4346a686478530bac70664",
            "3215affbeebe44b687aea9a342892c1d",
            "f2d99449d2dc4256ae7ed915037eb071",
            "654d579b36514c5686b9139b74b40b01",
            "b3f1765c73534a568eee8f066d188c9c",
            "6cbf77cb429548c687dd3cdd5402f473",
            "f7a766363ac04c51ad8033d376b17a6c"
          ]
        },
        "id": "kXwylxgxLNEA",
        "outputId": "2c80c051-6d95-4ee6-e5f5-e0215a834cbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240925_221029-6nzi3d5b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/6nzi3d5b' target=\"_blank\">dark-firebrand-54</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/6nzi3d5b' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/6nzi3d5b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 1 ------------\n",
            "Batch #500 Loss: 6.801609323501587\n",
            "Batch #1000 Loss: 6.368978764533996\n",
            "Batch #1500 Loss: 6.138936006546021\n",
            "Batch #2000 Loss: 6.0047424793243405\n",
            "\u001b[92mTrain perplexity: 321.178028 ||| loss 5.771996\u001b[0m\n",
            "\u001b[92mValidation perplexity: 322.832683 ||| loss 5.777134\u001b[0m\n",
            "\u001b[92mTest perplexity: 314.067245 ||| loss 5.749607\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 1 ------------\n",
            "Batch #500 Loss: 5.846682530403137\n",
            "Batch #1000 Loss: 5.781919544219971\n",
            "Batch #1500 Loss: 5.70266277217865\n",
            "Batch #2000 Loss: 5.653602899551392\n",
            "\u001b[92mTrain perplexity: 235.868469 ||| loss 5.463274\u001b[0m\n",
            "\u001b[92mValidation perplexity: 247.050445 ||| loss 5.509593\u001b[0m\n",
            "\u001b[92mTest perplexity: 240.137051 ||| loss 5.481210\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 1 ------------\n",
            "Batch #500 Loss: 5.588469083786011\n",
            "Batch #1000 Loss: 5.556520874977112\n",
            "Batch #1500 Loss: 5.505293214797974\n",
            "Batch #2000 Loss: 5.47390731716156\n",
            "\u001b[92mTrain perplexity: 193.010948 ||| loss 5.262747\u001b[0m\n",
            "\u001b[92mValidation perplexity: 208.731020 ||| loss 5.341046\u001b[0m\n",
            "\u001b[92mTest perplexity: 202.532071 ||| loss 5.310898\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 1 ------------\n",
            "Batch #500 Loss: 5.429628275871277\n",
            "Batch #1000 Loss: 5.409243287086487\n",
            "Batch #1500 Loss: 5.370693639755249\n",
            "Batch #2000 Loss: 5.352316313743591\n",
            "\u001b[92mTrain perplexity: 168.055890 ||| loss 5.124297\u001b[0m\n",
            "\u001b[92mValidation perplexity: 187.332804 ||| loss 5.232887\u001b[0m\n",
            "\u001b[92mTest perplexity: 181.322243 ||| loss 5.200276\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 1 ------------\n",
            "Batch #500 Loss: 5.323071311950684\n",
            "Batch #1000 Loss: 5.304720084190369\n",
            "Batch #1500 Loss: 5.268209651947021\n",
            "Batch #2000 Loss: 5.255569331169128\n",
            "\u001b[92mTrain perplexity: 150.749889 ||| loss 5.015622\u001b[0m\n",
            "\u001b[92mValidation perplexity: 172.363356 ||| loss 5.149605\u001b[0m\n",
            "\u001b[92mTest perplexity: 167.033655 ||| loss 5.118195\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 1 ------------\n",
            "Batch #500 Loss: 5.237139199256897\n",
            "Batch #1000 Loss: 5.214936297416687\n",
            "Batch #1500 Loss: 5.189853968620301\n",
            "Batch #2000 Loss: 5.17994632434845\n",
            "\u001b[92mTrain perplexity: 137.110881 ||| loss 4.920790\u001b[0m\n",
            "\u001b[92mValidation perplexity: 160.404897 ||| loss 5.077701\u001b[0m\n",
            "\u001b[92mTest perplexity: 155.913524 ||| loss 5.049302\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 1 ------------\n",
            "Batch #500 Loss: 5.167327694892883\n",
            "Batch #1000 Loss: 5.1445146007537845\n",
            "Batch #1500 Loss: 5.1248943796157835\n",
            "Batch #2000 Loss: 5.117292463302612\n",
            "\u001b[92mTrain perplexity: 127.769477 ||| loss 4.850228\u001b[0m\n",
            "\u001b[92mValidation perplexity: 152.811005 ||| loss 5.029202\u001b[0m\n",
            "\u001b[92mTest perplexity: 148.761992 ||| loss 5.002348\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 1 ------------\n",
            "Batch #500 Loss: 5.109031490325927\n",
            "Batch #1000 Loss: 5.083031216621399\n",
            "Batch #1500 Loss: 5.0672753210067745\n",
            "Batch #2000 Loss: 5.065395748138427\n",
            "\u001b[92mTrain perplexity: 119.665832 ||| loss 4.784703\u001b[0m\n",
            "\u001b[92mValidation perplexity: 146.987413 ||| loss 4.990347\u001b[0m\n",
            "\u001b[92mTest perplexity: 142.909040 ||| loss 4.962208\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 1 ------------\n",
            "Batch #500 Loss: 5.059023049354553\n",
            "Batch #1000 Loss: 5.0282684507369995\n",
            "Batch #1500 Loss: 5.018251510620117\n",
            "Batch #2000 Loss: 5.0168518409729\n",
            "\u001b[92mTrain perplexity: 112.733935 ||| loss 4.725030\u001b[0m\n",
            "\u001b[92mValidation perplexity: 141.242972 ||| loss 4.950482\u001b[0m\n",
            "\u001b[92mTest perplexity: 137.711328 ||| loss 4.925160\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 1 ------------\n",
            "Batch #500 Loss: 5.0087855405807495\n",
            "Batch #1000 Loss: 4.982355675697327\n",
            "Batch #1500 Loss: 4.974239810943604\n",
            "Batch #2000 Loss: 4.975876244544983\n",
            "\u001b[92mTrain perplexity: 106.990627 ||| loss 4.672741\u001b[0m\n",
            "\u001b[92mValidation perplexity: 136.654841 ||| loss 4.917458\u001b[0m\n",
            "\u001b[92mTest perplexity: 133.293428 ||| loss 4.892553\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 1 ------------\n",
            "Batch #500 Loss: 4.971758461952209\n",
            "Batch #1000 Loss: 4.943067909240723\n",
            "Batch #1500 Loss: 4.9327628717422485\n",
            "Batch #2000 Loss: 4.93999727344513\n",
            "\u001b[92mTrain perplexity: 101.962668 ||| loss 4.624607\u001b[0m\n",
            "\u001b[92mValidation perplexity: 132.913745 ||| loss 4.889700\u001b[0m\n",
            "\u001b[92mTest perplexity: 129.742277 ||| loss 4.865550\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 1 ------------\n",
            "Batch #500 Loss: 4.93224195098877\n",
            "Batch #1000 Loss: 4.9029775762557986\n",
            "Batch #1500 Loss: 4.899567888259888\n",
            "Batch #2000 Loss: 4.90208557510376\n",
            "\u001b[92mTrain perplexity: 97.254418 ||| loss 4.577330\u001b[0m\n",
            "\u001b[92mValidation perplexity: 129.289626 ||| loss 4.862055\u001b[0m\n",
            "\u001b[92mTest perplexity: 125.891717 ||| loss 4.835422\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 1 ------------\n",
            "Batch #500 Loss: 4.903326907157898\n",
            "Batch #1000 Loss: 4.8661570091247555\n",
            "Batch #1500 Loss: 4.866222153663635\n",
            "Batch #2000 Loss: 4.871274255752564\n",
            "\u001b[92mTrain perplexity: 94.188357 ||| loss 4.545297\u001b[0m\n",
            "\u001b[92mValidation perplexity: 127.536912 ||| loss 4.848406\u001b[0m\n",
            "\u001b[92mTest perplexity: 124.613519 ||| loss 4.825217\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 1 ------------\n",
            "Batch #500 Loss: 4.8717027854919435\n",
            "Batch #1000 Loss: 4.8393425998687745\n",
            "Batch #1500 Loss: 4.83821155834198\n",
            "Batch #2000 Loss: 4.846073437690735\n",
            "\u001b[92mTrain perplexity: 90.641647 ||| loss 4.506914\u001b[0m\n",
            "\u001b[92mValidation perplexity: 125.384397 ||| loss 4.831384\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.214368 ||| loss 4.805777\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #15, LR: 1 ------------\n",
            "Batch #500 Loss: 4.846691106796265\n",
            "Batch #1000 Loss: 4.80834445476532\n",
            "Batch #1500 Loss: 4.813008588790893\n",
            "Batch #2000 Loss: 4.819904870033264\n",
            "\u001b[92mTrain perplexity: 87.564835 ||| loss 4.472379\u001b[0m\n",
            "\u001b[92mValidation perplexity: 123.595809 ||| loss 4.817017\u001b[0m\n",
            "\u001b[92mTest perplexity: 120.090175 ||| loss 4.788243\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #16, LR: 1 ------------\n",
            "Batch #500 Loss: 4.8207582025527955\n",
            "Batch #1000 Loss: 4.787831303596497\n",
            "Batch #1500 Loss: 4.790065631866455\n",
            "Batch #2000 Loss: 4.797310683250427\n",
            "\u001b[92mTrain perplexity: 84.571555 ||| loss 4.437598\u001b[0m\n",
            "\u001b[92mValidation perplexity: 121.430548 ||| loss 4.799342\u001b[0m\n",
            "\u001b[92mTest perplexity: 117.950025 ||| loss 4.770261\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #17, LR: 1 ------------\n",
            "Batch #500 Loss: 4.7953193054199215\n",
            "Batch #1000 Loss: 4.764641162872315\n",
            "Batch #1500 Loss: 4.7702190227508545\n",
            "Batch #2000 Loss: 4.775208958625793\n",
            "\u001b[92mTrain perplexity: 82.301150 ||| loss 4.410385\u001b[0m\n",
            "\u001b[92mValidation perplexity: 119.921112 ||| loss 4.786834\u001b[0m\n",
            "\u001b[92mTest perplexity: 116.429683 ||| loss 4.757288\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #18, LR: 1 ------------\n",
            "Batch #500 Loss: 4.782453766822815\n",
            "Batch #1000 Loss: 4.744441987991333\n",
            "Batch #1500 Loss: 4.747508573532104\n",
            "Batch #2000 Loss: 4.7589979858398435\n",
            "\u001b[92mTrain perplexity: 80.305417 ||| loss 4.385837\u001b[0m\n",
            "\u001b[92mValidation perplexity: 118.964951 ||| loss 4.778829\u001b[0m\n",
            "\u001b[92mTest perplexity: 115.416743 ||| loss 4.748549\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #19, LR: 1 ------------\n",
            "Batch #500 Loss: 4.75595606803894\n",
            "Batch #1000 Loss: 4.724407950401306\n",
            "Batch #1500 Loss: 4.729239832878113\n",
            "Batch #2000 Loss: 4.738993319511414\n",
            "\u001b[92mTrain perplexity: 78.354951 ||| loss 4.361249\u001b[0m\n",
            "\u001b[92mValidation perplexity: 117.271099 ||| loss 4.764488\u001b[0m\n",
            "\u001b[92mTest perplexity: 113.680661 ||| loss 4.733393\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #20, LR: 1 ------------\n",
            "Batch #500 Loss: 4.739699469566345\n",
            "Batch #1000 Loss: 4.705644463539124\n",
            "Batch #1500 Loss: 4.712946671485901\n",
            "Batch #2000 Loss: 4.722931357860565\n",
            "\u001b[92mTrain perplexity: 76.901160 ||| loss 4.342521\u001b[0m\n",
            "\u001b[92mValidation perplexity: 116.311450 ||| loss 4.756272\u001b[0m\n",
            "\u001b[92mTest perplexity: 113.399151 ||| loss 4.730914\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #21, LR: 0.75 ------------\n",
            "Batch #500 Loss: 4.710600329399109\n",
            "Batch #1000 Loss: 4.666945834159851\n",
            "Batch #1500 Loss: 4.670724911212921\n",
            "Batch #2000 Loss: 4.6764693422317505\n",
            "\u001b[92mTrain perplexity: 73.348965 ||| loss 4.295228\u001b[0m\n",
            "\u001b[92mValidation perplexity: 113.592023 ||| loss 4.732613\u001b[0m\n",
            "\u001b[92mTest perplexity: 110.393326 ||| loss 4.704050\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #22, LR: 0.5625 ------------\n",
            "Batch #500 Loss: 4.682145027160645\n",
            "Batch #1000 Loss: 4.632636269569397\n",
            "Batch #1500 Loss: 4.63697201347351\n",
            "Batch #2000 Loss: 4.643939054012298\n",
            "\u001b[92mTrain perplexity: 71.032409 ||| loss 4.263136\u001b[0m\n",
            "\u001b[92mValidation perplexity: 111.904888 ||| loss 4.717649\u001b[0m\n",
            "\u001b[92mTest perplexity: 108.589419 ||| loss 4.687574\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #23, LR: 0.421875 ------------\n",
            "Batch #500 Loss: 4.654833753108978\n",
            "Batch #1000 Loss: 4.608616080760956\n",
            "Batch #1500 Loss: 4.612332343101501\n",
            "Batch #2000 Loss: 4.615918928146362\n",
            "\u001b[92mTrain perplexity: 69.354435 ||| loss 4.239230\u001b[0m\n",
            "\u001b[92mValidation perplexity: 110.464895 ||| loss 4.704698\u001b[0m\n",
            "\u001b[92mTest perplexity: 106.974769 ||| loss 4.672593\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #24, LR: 0.31640625 ------------\n",
            "Batch #500 Loss: 4.63970264005661\n",
            "Batch #1000 Loss: 4.5894591135978695\n",
            "Batch #1500 Loss: 4.5932518849372865\n",
            "Batch #2000 Loss: 4.59442818069458\n",
            "\u001b[92mTrain perplexity: 68.003331 ||| loss 4.219557\u001b[0m\n",
            "\u001b[92mValidation perplexity: 109.438719 ||| loss 4.695365\u001b[0m\n",
            "\u001b[92mTest perplexity: 106.101219 ||| loss 4.664394\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #25, LR: 0.2373046875 ------------\n",
            "Batch #500 Loss: 4.619432617664337\n",
            "Batch #1000 Loss: 4.572948816299438\n",
            "Batch #1500 Loss: 4.576089277744293\n",
            "Batch #2000 Loss: 4.580753050804138\n",
            "\u001b[92mTrain perplexity: 66.873447 ||| loss 4.202802\u001b[0m\n",
            "\u001b[92mValidation perplexity: 108.543690 ||| loss 4.687153\u001b[0m\n",
            "\u001b[92mTest perplexity: 104.926817 ||| loss 4.653263\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85b9aa29e49c493c9a9a754fd4962c5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.65326</td></tr><tr><td>Test-perplexity</td><td>104.92682</td></tr><tr><td>Train-loss</td><td>4.2028</td></tr><tr><td>Train-perplexity</td><td>66.87345</td></tr><tr><td>Validation-loss</td><td>4.68715</td></tr><tr><td>Validation-perplexity</td><td>108.54369</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dark-firebrand-54</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/6nzi3d5b' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri/runs/6nzi3d5b</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2-tri</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240925_221029-6nzi3d5b/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRXl71g2Z9aL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}