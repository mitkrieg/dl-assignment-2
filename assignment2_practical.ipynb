{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyfX9PvITIxU2OAC9P+wjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-2/blob/main/assignment2_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4R2RS5a0QQ9",
        "outputId": "f74b9660-0693-493f-ebcc-c1bfc13a995b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "# import wandb\n",
        "\n",
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('CUDA GPU Available:',torch.cuda.is_available())\n",
        "print('MPS GPU Available:', torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "#   device = torch.device('mps')\n",
        "#   print('Pytorch GPU Build:',torch.backends.mps.is_built())\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ],
      "metadata": {
        "id": "9BLt6FUx0VtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d06fa1-7a60-4b47-db43-3ffacb153094"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "CUDA GPU Available: True\n",
            "MPS GPU Available: False\n",
            "GPU Name: Tesla T4\n",
            "GPU Count: 1\n",
            "GPU Memory Allocated: 0\n",
            "GPU Memory Cached: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, pre_built_dict: dict=None):\n",
        "        if pre_built_dict:\n",
        "            self.vocab = pre_built_dict\n",
        "        else:\n",
        "            self.vocab = {'<pad>': 0, '<oov>': 1, '<sos>': 2, '<eos>': 3, '<unk>': 4}\n",
        "        self.idx = len(self.vocab)\n",
        "\n",
        "    def add_word(self, word: str) -> None:\n",
        "        if word not in self.vocab:\n",
        "            self.vocab[word] = self.idx\n",
        "            self.idx += 1\n",
        "\n",
        "    def encode(self, tokens: list[str]) -> list[int]:\n",
        "        return [self.vocab.get(word, self.vocab['<unk>']) for word in tokens]\n",
        "\n",
        "    def decode(self, indicies: list[int]) -> list[str]:\n",
        "        return [list(self.vocab.keys())[list(self.vocab.values()).index(idx)] for idx in indicies]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "class PTBText(Dataset):\n",
        "    def __init__(self, path: str, vocab: Vocab=Vocab(), build_vocab=True, batch_size=20, seqence_length=20, device=torch.device('cpu')):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.vocab = vocab\n",
        "        self.data = self.load_data(build_vocab)\n",
        "        self.batch_size = batch_size\n",
        "        self.chunk_size = len(self.data) // batch_size\n",
        "        self.seq_len = seqence_length\n",
        "        self.minibatches = self.create_batches()\n",
        "\n",
        "    def load_data(self, build_vocab):\n",
        "        data = []\n",
        "        with open(self.path, 'r') as f:\n",
        "            count = 0\n",
        "            for line in f:\n",
        "                count += 1\n",
        "                tokens = line.strip().split() + ['<eos>']\n",
        "                if build_vocab:\n",
        "                    for token in tokens:\n",
        "                        self.vocab.add_word(token)\n",
        "\n",
        "                encoded_line = self.vocab.encode(tokens)\n",
        "                data.extend(encoded_line)\n",
        "        return data\n",
        "\n",
        "    def create_batches(self):\n",
        "        return [self.data[i*self.chunk_size: (i+1)*self.chunk_size] for i in range(self.batch_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, j):\n",
        "        inputs = torch.stack([\n",
        "            torch.LongTensor(self.minibatches[i][j * self.seq_len : (j + 1) * self.seq_len])\n",
        "            for i in range(self.batch_size)], dim=0)\n",
        "        labels = torch.stack([\n",
        "            torch.LongTensor(self.minibatches[i][j * self.seq_len + 1 : (j + 1) * self.seq_len + 1])\n",
        "            for i in range(self.batch_size)], dim=0)\n",
        "\n",
        "        return inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "    def get_tokens(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_decoded_tokens(self, idx):\n",
        "        return self.vocab.decode(self.data[idx])\n",
        "\n",
        "\n",
        "train = PTBText('/content/ptb.train.txt', device=device)\n",
        "val = PTBText('/content/ptb.valid.txt', vocab=train.vocab, build_vocab=False, device=device)\n",
        "test = PTBText('/content/ptb.test.txt', vocab=train.vocab, build_vocab=False, device=device)\n",
        "\n",
        "datasets = {\n",
        "    'train': train,\n",
        "    'val': val,\n",
        "    'test': test\n",
        "}\n",
        "\n",
        "print(\"Vocab size:\", len(train.vocab))\n",
        "print(\"Train data size:\", len(train))\n",
        "print(\"Val data size:\", len(val))\n",
        "print(\"Test data size:\", len(test))"
      ],
      "metadata": {
        "id": "KUza40e_165_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553b2f1b-8500-40c8-861b-fee7832a300d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 10003\n",
            "Train data size: 929589\n",
            "Val data size: 73760\n",
            "Test data size: 82430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ZamrembaRNN(nn.Module):\n",
        "    def __init__(self, rnn_type, vocab_size, batch_size=20, embedding_dim=200, hidden_dim=200, num_layers=2, dropout=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "        self.batch_size = batch_size\n",
        "        if rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        elif rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid RNN type: must be 'lstm' or 'gru'\")\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        if dropout > 0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "        output = F.relu(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n",
        "        nn.init.uniform_(self.rnn.weight_ih_l0, -initrange, initrange)\n",
        "        nn.init.uniform_(self.rnn.weight_hh_l0, -initrange, initrange)\n",
        "        nn.init.uniform_(self.fc.weight, -initrange, initrange)"
      ],
      "metadata": {
        "id": "XRRsn3Df_mAi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataset, loss_fn, optimizer, device, epoch, verbosity):\n",
        "    \"\"\"Train one epoch of a network\"\"\"\n",
        "    model.train()\n",
        "    batch_loss = 0\n",
        "\n",
        "    hidden = (torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device),\n",
        "              torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device))\n",
        "\n",
        "    for j in range(dataset.chunk_size // dataset.seq_len):\n",
        "\n",
        "        inputs, labels = dataset[j]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        loss = loss_fn(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "        if (j + 1) % verbosity == 0:\n",
        "            print(f'Batch #{j + 1} Loss: {batch_loss / verbosity}')\n",
        "            batch_loss = 0\n",
        "\n",
        "def perplexity(loss, batches):\n",
        "    return math.exp(loss / batches)\n",
        "\n",
        "def evaluate_model(title, model, dataset, loss_fn, seq_len, batch_size, epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataset) // (batch_size * seq_len)\n",
        "\n",
        "    hidden = (torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device),\n",
        "              torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for j in range(num_batches):\n",
        "\n",
        "            inputs, labels = dataset[j]\n",
        "\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = loss_fn(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    perp = perplexity(total_loss, num_batches)\n",
        "    # wandb.log({\n",
        "    #         f'{title}-loss': total_loss / num_batches,\n",
        "    #         f'{title}-perplexity': perp\n",
        "    #     }, step=epoch)\n",
        "\n",
        "    print(f'\\033[92m{title} perplexity: {perp:.6f} ||| loss {total_loss / num_batches:.6f}\\033[0m')\n",
        "\n",
        "    return perp\n",
        "\n",
        "def train_network(model, datasets, loss_fn, optimizer, schedule, device, epochs: int, verbosity: int):\n",
        "    for epoch in range(epochs):\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'----------- Epoch #{epoch + 1}, LR: {lr} ------------')\n",
        "        train_epoch(model, datasets['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "        train_perplexity = evaluate_model('Train', model, datasets['train'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        val_perplexity = evaluate_model('Validation', model, datasets['val'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        test_perplexity = evaluate_model('Test', model, datasets['test'], loss_fn, datasets['train'].seq_len, datasets['train'].batch_size, epoch)\n",
        "        print('------------------------------------\\n')\n",
        "\n",
        "        schedule.step()\n",
        "    print('----------- Train Complete! ------------')\n",
        "    return {\n",
        "        'train':train_perplexity,\n",
        "        'val':val_perplexity,\n",
        "        'test':test_perplexity\n",
        "    }"
      ],
      "metadata": {
        "id": "V3z6xiymAzyB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decay_start = 4\n",
        "learning_rate_decay = 0.5\n",
        "momentum=0\n",
        "lr = 4\n",
        "dropout_rate = 0\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < decay_start:\n",
        "        return 1\n",
        "    else:\n",
        "        return learning_rate_decay ** (epoch - (decay_start-1))\n",
        "\n",
        "model = ZamrembaRNN('lstm', len(train.vocab)).to(device)\n",
        "sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "\n",
        "final_metrics = train_network(model, datasets, cross_entropy, sgd, schedule, device, 14, 500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CDIw79OTAr6",
        "outputId": "71c2e07a-36cb-4610-a20d-40c7a0f093cc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 4 ------------\n",
            "Batch #500 Loss: 7.8949539031982425\n",
            "Batch #1000 Loss: 7.33743768119812\n",
            "Batch #1500 Loss: 6.342725848197937\n",
            "Batch #2000 Loss: 6.069165286064148\n",
            "\u001b[92mTrain perplexity: 356.934628 ||| loss 5.877553\u001b[0m\n",
            "\u001b[92mValidation perplexity: 355.698837 ||| loss 5.874084\u001b[0m\n",
            "\u001b[92mTest perplexity: 355.935089 ||| loss 5.874748\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 4 ------------\n",
            "Batch #500 Loss: 5.798190748214721\n",
            "Batch #1000 Loss: 5.693178926467896\n",
            "Batch #1500 Loss: 5.584246485710144\n",
            "Batch #2000 Loss: 5.454984982490539\n",
            "\u001b[92mTrain perplexity: 189.530829 ||| loss 5.244552\u001b[0m\n",
            "\u001b[92mValidation perplexity: 206.213264 ||| loss 5.328911\u001b[0m\n",
            "\u001b[92mTest perplexity: 201.679779 ||| loss 5.306681\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 4 ------------\n",
            "Batch #500 Loss: 5.210991724967957\n",
            "Batch #1000 Loss: 5.142019505500794\n",
            "Batch #1500 Loss: 5.076135538101196\n",
            "Batch #2000 Loss: 5.027217918395996\n",
            "\u001b[92mTrain perplexity: 144.346829 ||| loss 4.972219\u001b[0m\n",
            "\u001b[92mValidation perplexity: 168.360423 ||| loss 5.126107\u001b[0m\n",
            "\u001b[92mTest perplexity: 164.680153 ||| loss 5.104005\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 4 ------------\n",
            "Batch #500 Loss: 4.964230014801025\n",
            "Batch #1000 Loss: 4.902684103965759\n",
            "Batch #1500 Loss: 4.858938058853149\n",
            "Batch #2000 Loss: 4.8327605333328245\n",
            "\u001b[92mTrain perplexity: 119.884780 ||| loss 4.786531\u001b[0m\n",
            "\u001b[92mValidation perplexity: 150.641985 ||| loss 5.014906\u001b[0m\n",
            "\u001b[92mTest perplexity: 147.585881 ||| loss 4.994410\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 2.0 ------------\n",
            "Batch #500 Loss: 4.765457679271698\n",
            "Batch #1000 Loss: 4.688219858169556\n",
            "Batch #1500 Loss: 4.646083247661591\n",
            "Batch #2000 Loss: 4.621039283275604\n",
            "\u001b[92mTrain perplexity: 101.643788 ||| loss 4.621474\u001b[0m\n",
            "\u001b[92mValidation perplexity: 137.970075 ||| loss 4.927037\u001b[0m\n",
            "\u001b[92mTest perplexity: 135.145718 ||| loss 4.906354\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 1.0 ------------\n",
            "Batch #500 Loss: 4.643719145774841\n",
            "Batch #1000 Loss: 4.566816125869751\n",
            "Batch #1500 Loss: 4.524941325187683\n",
            "Batch #2000 Loss: 4.499623291969299\n",
            "\u001b[92mTrain perplexity: 91.503643 ||| loss 4.516379\u001b[0m\n",
            "\u001b[92mValidation perplexity: 131.553661 ||| loss 4.879415\u001b[0m\n",
            "\u001b[92mTest perplexity: 128.825630 ||| loss 4.858460\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 0.5 ------------\n",
            "Batch #500 Loss: 4.573589801311493\n",
            "Batch #1000 Loss: 4.497443791866303\n",
            "Batch #1500 Loss: 4.455864240646362\n",
            "Batch #2000 Loss: 4.43061911725998\n",
            "\u001b[92mTrain perplexity: 85.945680 ||| loss 4.453715\u001b[0m\n",
            "\u001b[92mValidation perplexity: 128.388116 ||| loss 4.855058\u001b[0m\n",
            "\u001b[92mTest perplexity: 125.649979 ||| loss 4.833500\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 0.25 ------------\n",
            "Batch #500 Loss: 4.532810908794403\n",
            "Batch #1000 Loss: 4.45898598909378\n",
            "Batch #1500 Loss: 4.417494369506836\n",
            "Batch #2000 Loss: 4.392164742946624\n",
            "\u001b[92mTrain perplexity: 82.950375 ||| loss 4.418243\u001b[0m\n",
            "\u001b[92mValidation perplexity: 126.734763 ||| loss 4.842096\u001b[0m\n",
            "\u001b[92mTest perplexity: 123.990600 ||| loss 4.820206\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 0.125 ------------\n",
            "Batch #500 Loss: 4.509598120212555\n",
            "Batch #1000 Loss: 4.437886065006256\n",
            "Batch #1500 Loss: 4.396651546955109\n",
            "Batch #2000 Loss: 4.371060894966125\n",
            "\u001b[92mTrain perplexity: 81.388941 ||| loss 4.399239\u001b[0m\n",
            "\u001b[92mValidation perplexity: 125.840306 ||| loss 4.835014\u001b[0m\n",
            "\u001b[92mTest perplexity: 123.062052 ||| loss 4.812689\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 0.0625 ------------\n",
            "Batch #500 Loss: 4.49687094783783\n",
            "Batch #1000 Loss: 4.4261774334907535\n",
            "Batch #1500 Loss: 4.385485166549683\n",
            "Batch #2000 Loss: 4.359716596126557\n",
            "\u001b[92mTrain perplexity: 80.612181 ||| loss 4.389650\u001b[0m\n",
            "\u001b[92mValidation perplexity: 125.348504 ||| loss 4.831098\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.520339 ||| loss 4.808277\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 0.03125 ------------\n",
            "Batch #500 Loss: 4.490047241687774\n",
            "Batch #1000 Loss: 4.419700294494629\n",
            "Batch #1500 Loss: 4.3794899797439575\n",
            "Batch #2000 Loss: 4.353675692081452\n",
            "\u001b[92mTrain perplexity: 80.239455 ||| loss 4.385015\u001b[0m\n",
            "\u001b[92mValidation perplexity: 125.073874 ||| loss 4.828905\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.194039 ||| loss 4.805610\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 0.015625 ------------\n",
            "Batch #500 Loss: 4.486356644630432\n",
            "Batch #1000 Loss: 4.416235778331757\n",
            "Batch #1500 Loss: 4.376196744918823\n",
            "Batch #2000 Loss: 4.350499656200409\n",
            "\u001b[92mTrain perplexity: 80.066249 ||| loss 4.382854\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.908235 ||| loss 4.827579\u001b[0m\n",
            "\u001b[92mTest perplexity: 122.005727 ||| loss 4.804068\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 0.0078125 ------------\n",
            "Batch #500 Loss: 4.484371992588043\n",
            "Batch #1000 Loss: 4.4144653749465945\n",
            "Batch #1500 Loss: 4.374420104026794\n",
            "Batch #2000 Loss: 4.348791377067566\n",
            "\u001b[92mTrain perplexity: 79.985355 ||| loss 4.381844\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.803566 ||| loss 4.826741\u001b[0m\n",
            "\u001b[92mTest perplexity: 121.905133 ||| loss 4.803243\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 0.00390625 ------------\n",
            "Batch #500 Loss: 4.483334658622741\n",
            "Batch #1000 Loss: 4.413565567016602\n",
            "Batch #1500 Loss: 4.373464681625366\n",
            "Batch #2000 Loss: 4.347842999458313\n",
            "\u001b[92mTrain perplexity: 79.946588 ||| loss 4.381359\u001b[0m\n",
            "\u001b[92mValidation perplexity: 124.740316 ||| loss 4.826234\u001b[0m\n",
            "\u001b[92mTest perplexity: 121.848123 ||| loss 4.802775\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = (torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device),\n",
        "              torch.zeros(model.num_layers, model.batch_size, model.hidden_dim).to(device))\n",
        "\n",
        "inputs, labels = train[0]\n",
        "outputs, hidden = model(inputs, hidden)\n",
        "loss = cross_entropy(outputs, labels.view(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "8VG4kuatBR_r",
        "outputId": "952f552a-8f4a-4229-aefc-4ca893254d4d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (20) to match target batch_size (400).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-544cc857d950>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1189\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (20) to match target batch_size (400)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ".shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXwylxgxLNEA",
        "outputId": "76f8b5fe-33bf-4b06-9908-ed897627f424"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400, 10003])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylPfT3Tv2o24",
        "outputId": "e4cef4fa-09bb-44e0-cb60-f3e542ea39c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10003"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uU4w96ev2spQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}