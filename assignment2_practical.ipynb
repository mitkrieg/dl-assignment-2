{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitkrieg/dl-assignment-2/blob/main/assignment2_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijX0qBNZlwRY",
        "outputId": "3f8f7271-e32a-4bbb-aea9-e127d313db22"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXlIsAxnaoz5",
        "outputId": "4870ff52-155f-410a-a8ed-6eb9eb2e0e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ ACCELERATION INFO -----\n",
            "CUDA GPU Available: True\n",
            "MPS GPU Available: False\n",
            "GPU Name: Tesla T4\n",
            "GPU Count: 1\n",
            "GPU Memory Allocated: 0\n",
            "GPU Memory Cached: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import wandb\n",
        "\n",
        "print(\"------ ACCELERATION INFO -----\")\n",
        "print('CUDA GPU Available:',torch.cuda.is_available())\n",
        "print('MPS GPU Available:', torch.backends.mps.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU Name:',torch.cuda.get_device_name(0))\n",
        "  print('GPU Count:',torch.cuda.device_count())\n",
        "  print('GPU Memory Allocated:',torch.cuda.memory_allocated(0))\n",
        "  print('GPU Memory Cached:',torch.cuda.memory_reserved(0))\n",
        "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "#   device = torch.device('mps')\n",
        "#   print('Pytorch GPU Build:',torch.backends.mps.is_built())\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9d895XBaoz6"
      },
      "source": [
        "## Define PTBText Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TmKDKXflaoz7"
      },
      "outputs": [],
      "source": [
        "class PTBText(Dataset):\n",
        "    def __init__(self, filename, sequence_len, prior_vocab=None,device=torch.device('cpu')) -> None:\n",
        "        super().__init__()\n",
        "        self.tokenized_text = []\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.device = device\n",
        "        self.tokenizer = RegexpTokenizer(r'<unk>|<pad>|<oov>|<sos>|<eos>|\\w+').tokenize\n",
        "        self.seq_len = sequence_len\n",
        "        self.max_len = 0\n",
        "        if prior_vocab:\n",
        "            self.vocab = prior_vocab\n",
        "        else:\n",
        "            self.vocab = {'<pad>':0,'<oov>':1,'<sos>':2,'<eos>':3,'<unk>':4}\n",
        "\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                tokens = self.tokenizer(line)\n",
        "\n",
        "                #only build new vocab if prior vocab is not given\n",
        "                if prior_vocab is None:\n",
        "                    idx = len(self.vocab)\n",
        "                    for word in tokens:\n",
        "                        if word not in self.vocab:\n",
        "                            self.vocab[word] = idx\n",
        "                            idx += 1\n",
        "\n",
        "                self.tokenized_text.append(['<sos>'] + tokens + ['<eos>'])\n",
        "                self.max_len = max(self.max_len, len(tokens) + 2)\n",
        "\n",
        "\n",
        "        self.encoded_text = [self.encode_text(x, pad=True) for x in self.tokenized_text]\n",
        "\n",
        "        #build sequences\n",
        "        for tokens in self.tokenized_text:\n",
        "            for i in range(len(tokens) - self.seq_len):\n",
        "                self.data.append(tokens[i:i+self.seq_len])\n",
        "                self.labels.append(tokens[i+self.seq_len])\n",
        "        self.encoded_labels = [self.vocab.get(x,1) for x in self.labels]\n",
        "        self.encoded_data = [self.encode_text(x) for x in self.data]\n",
        "\n",
        "    def encode_text(self, tokens: list[str], pad=False):\n",
        "        encoded = []\n",
        "        for word in tokens:\n",
        "            encoded.append(self.vocab.get(word,1))\n",
        "\n",
        "        if pad and len(encoded) < self.max_len:\n",
        "            encoded.extend([0]* (self.max_len - len(encoded)))\n",
        "        elif len(encoded) < self.seq_len:\n",
        "            encoded.extend([0]*(self.seq_len - len(encoded)))\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def resequence_data(self, seqence_len):\n",
        "        self.seq_len = seqence_len\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for tokens in self.tokenized_text:\n",
        "            for i in range(len(tokens) - self.seq_len):\n",
        "                self.data.append(tokens[i:i+self.seq_len])\n",
        "                self.labels.append(tokens[i+self.seq_len])\n",
        "\n",
        "        self.encoded_labels = [self.vocab.get(x,1) for x in self.labels]\n",
        "        self.encoded_data = [self.encode_text(x) for x in self.data]\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.encoded_data[index]).to(self.device), torch.tensor(self.encoded_labels[index]).to(self.device)\n",
        "\n",
        "    def get_tokens(self, index):\n",
        "        return self.tokenized_text[index]\n",
        "\n",
        "    def get_encoded_tokens(self, index):\n",
        "        return self.encoded_text[index]\n",
        "\n",
        "    def get_sequence(self, index):\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "    def get_encoded_sequence(self, index):\n",
        "        return self.__getitem__(index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZubswHHaoz7"
      },
      "source": [
        "### Load Data & Create Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhw_Ric-aoz7",
        "outputId": "7598490b-f987-4072-e462-a0e0235ca8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size: 9648\n",
            "Training sample raw:  (['symptoms', 'that', 'show', 'up', 'decades', 'later', 'researchers', 'said'], '<eos>')\n",
            "Training sample encoded: (tensor([ 98,  99, 100, 101, 102, 103,  88, 104]), tensor(3))\n"
          ]
        }
      ],
      "source": [
        "train = PTBText('/content/ptb.train.txt', 8)\n",
        "val = PTBText('/content/ptb.valid.txt', 8, prior_vocab=train.vocab)\n",
        "test = PTBText('/content/ptb.test.txt', 8, prior_vocab=train.vocab)\n",
        "\n",
        "gen = torch.Generator().manual_seed(123)\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "\n",
        "dataloaders = {\n",
        "    'train':train_loader,\n",
        "    'val':val_loader,\n",
        "    'test':test_loader\n",
        "}\n",
        "\n",
        "print('Training vocab size:', len(train.vocab))\n",
        "print('Training sample raw: ', train.get_sequence(100))\n",
        "print('Training sample encoded:',train[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBtifwp8aoz8"
      },
      "source": [
        "## Define LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z9KNjkJ0aoz8"
      },
      "outputs": [],
      "source": [
        "class ZarembaRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_units=200, num_lstm_layers=2, dropout_rate= 0) -> None:\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embedding_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1, generator=torch.Generator().manual_seed(1))\n",
        "        self.lstm = nn.LSTM(embedding_size,hidden_units, num_lstm_layers, batch_first=True)\n",
        "        nn.init.uniform_(self.lstm.weight_ih_l0, -0.1, 0.1, generator=torch.Generator().manual_seed(2))\n",
        "        nn.init.uniform_(self.lstm.weight_hh_l0, -0.1, 0.1, generator=torch.Generator().manual_seed(3))\n",
        "        self.fc = nn.Linear(hidden_units, vocab_size)\n",
        "        nn.init.uniform_(self.fc.weight, -0.1, 0.1, generator=torch.Generator().manual_seed(4))\n",
        "        self.dropout_rate = dropout_rate\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout(x[:, -1, :])\n",
        "            x = self.fc(x)\n",
        "        else:\n",
        "            x = self.fc(x[:, -1, :])\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ylH6Tu6yaoz8"
      },
      "outputs": [],
      "source": [
        "def train_epoch(network, dataloader, loss_fn, optimizer, device, epoch, verbosity: int):\n",
        "    \"\"\"Train one epoch of a network\"\"\"\n",
        "\n",
        "    network.train()\n",
        "    batch_loss = 0\n",
        "\n",
        "    # iterate over all batches\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss += loss.item()\n",
        "        if i % verbosity == verbosity - 1:\n",
        "            print(f'Batch #{i + 1} Loss: {batch_loss / verbosity}')\n",
        "            batch_loss = 0\n",
        "\n",
        "def perplexity(loss, batches):\n",
        "    return math.exp(loss / batches)\n",
        "\n",
        "def eval_network(title, network, dataloader, loss_fn, epoch):\n",
        "    \"\"\"Evaluate model and log metrics to wandb\"\"\"\n",
        "\n",
        "    network.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            data, labels = data\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = network(data)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            loss += loss_fn(outputs, labels)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        perp = perplexity(loss, len(dataloader))\n",
        "        wandb.log({\n",
        "            f'{title}-loss': loss / len(dataloader),\n",
        "            f'{title}-perplexity': perp\n",
        "        }, step=epoch)\n",
        "\n",
        "    print(f'\\033[92m{title} perplexity: {perp:.6f} ||| loss {loss / len(dataloader):.6f}\\033[0m')\n",
        "    return perp\n",
        "\n",
        "def train_network(network, dataloaders, loss_fn, optimizer, schedule, device, epochs: int, verbosity: int):\n",
        "    for epoch in range(epochs):\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'----------- Epoch #{epoch + 1}, LR: {lr} ------------')\n",
        "        train_epoch(network, dataloaders['train'], loss_fn, optimizer, device, epoch, verbosity)\n",
        "        train_perplexity = eval_network('Train', network, dataloaders['train'], loss_fn, epoch)\n",
        "        val_perplexity = eval_network('Validation', network, dataloaders['val'], loss_fn, epoch)\n",
        "        test_perplexity = eval_network('Test', network, dataloaders['test'], loss_fn, epoch)\n",
        "        print('------------------------------------\\n')\n",
        "\n",
        "        schedule.step()\n",
        "    print('----------- Train Complete! ------------')\n",
        "    return {\n",
        "        'train':train_perplexity,\n",
        "        'val':val_perplexity,\n",
        "        'test':test_perplexity\n",
        "    }\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QxURR3Tlaoz8"
      },
      "outputs": [],
      "source": [
        "def lr_lambda(epoch):\n",
        "    if epoch < 7:\n",
        "        return 1\n",
        "    elif epoch < 14:\n",
        "        return 0.5\n",
        "    else:\n",
        "        return 0.5 ** (epoch - 6)\n",
        "\n",
        "model = ZarembaRNN(len(train.vocab), 50)\n",
        "model = model.to(device)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "sgd = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "schedule = optim.lr_scheduler.LambdaLR(sgd, lr_lambda)\n",
        "# device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ea45a6d3ec8a44a0a017d77f3e6446ef",
            "013c468a455f4e3b98ca991cbd88f9f3",
            "e1e001ddb22349e4b4a3e9b47c516f78",
            "eff560204349468ebefe15ee839c984a",
            "a9bf9e4017ab4ed0988c7081e5e1e204",
            "2f5fc0c7f5ed47a1819b4479bc2e04f6",
            "514e602c642f4aa483ca73e9db67f17a",
            "b3c8851a143847a0a550a896a3ac6d73"
          ]
        },
        "id": "oZmvghb8aoz9",
        "outputId": "19d8f5c4-588e-4ff0-c08d-ccc7b5e4c217"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240922_050809-doqxrqg8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/doqxrqg8' target=\"_blank\">glamorous-donkey-40</a></strong> to <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/doqxrqg8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/doqxrqg8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- Epoch #1, LR: 0.1 ------------\n",
            "Batch #1000 Loss: 6.65466526556015\n",
            "Batch #2000 Loss: 6.41115735244751\n",
            "Batch #3000 Loss: 6.197577944278717\n",
            "Batch #4000 Loss: 6.030296788692475\n",
            "Batch #5000 Loss: 5.946876762866974\n",
            "\u001b[92mTrain perplexity: 351.975353 ||| loss 5.863561\u001b[0m\n",
            "\u001b[92mValidation perplexity: 355.176504 ||| loss 5.872615\u001b[0m\n",
            "\u001b[92mTest perplexity: 344.787543 ||| loss 5.842928\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #2, LR: 0.1 ------------\n",
            "Batch #1000 Loss: 5.806960593700409\n",
            "Batch #2000 Loss: 5.747286410808563\n",
            "Batch #3000 Loss: 5.673429936408996\n",
            "Batch #4000 Loss: 5.581537051677704\n",
            "Batch #5000 Loss: 5.528361514568329\n",
            "\u001b[92mTrain perplexity: 227.712289 ||| loss 5.428083\u001b[0m\n",
            "\u001b[92mValidation perplexity: 241.622754 ||| loss 5.487378\u001b[0m\n",
            "\u001b[92mTest perplexity: 235.146203 ||| loss 5.460207\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #3, LR: 0.1 ------------\n",
            "Batch #1000 Loss: 5.415270579814911\n",
            "Batch #2000 Loss: 5.355114161491394\n",
            "Batch #3000 Loss: 5.324301508426666\n",
            "Batch #4000 Loss: 5.269013843536377\n",
            "Batch #5000 Loss: 5.2363010501861575\n",
            "\u001b[92mTrain perplexity: 166.310228 ||| loss 5.113855\u001b[0m\n",
            "\u001b[92mValidation perplexity: 187.436731 ||| loss 5.233441\u001b[0m\n",
            "\u001b[92mTest perplexity: 181.356763 ||| loss 5.200466\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #4, LR: 0.1 ------------\n",
            "Batch #1000 Loss: 5.123079922199249\n",
            "Batch #2000 Loss: 5.114200431346894\n",
            "Batch #3000 Loss: 5.074349154472351\n",
            "Batch #4000 Loss: 5.089845503807068\n",
            "Batch #5000 Loss: 5.0440148820877075\n",
            "\u001b[92mTrain perplexity: 138.475463 ||| loss 4.930693\u001b[0m\n",
            "\u001b[92mValidation perplexity: 169.526530 ||| loss 5.133009\u001b[0m\n",
            "\u001b[92mTest perplexity: 161.990435 ||| loss 5.087537\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #5, LR: 0.1 ------------\n",
            "Batch #1000 Loss: 4.9327250623703005\n",
            "Batch #2000 Loss: 4.929187463283538\n",
            "Batch #3000 Loss: 4.931305782318115\n",
            "Batch #4000 Loss: 4.9077439155578615\n",
            "Batch #5000 Loss: 4.89139356136322\n",
            "\u001b[92mTrain perplexity: 116.580400 ||| loss 4.758581\u001b[0m\n",
            "\u001b[92mValidation perplexity: 151.647574 ||| loss 5.021559\u001b[0m\n",
            "\u001b[92mTest perplexity: 145.459055 ||| loss 4.979895\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #6, LR: 0.1 ------------\n",
            "Batch #1000 Loss: 4.774179681777954\n",
            "Batch #2000 Loss: 4.780706325769424\n",
            "Batch #3000 Loss: 4.793677759170532\n",
            "Batch #4000 Loss: 4.791257865428925\n",
            "Batch #5000 Loss: 4.769387162208557\n",
            "\u001b[92mTrain perplexity: 101.777970 ||| loss 4.622794\u001b[0m\n",
            "\u001b[92mValidation perplexity: 144.219764 ||| loss 4.971338\u001b[0m\n",
            "\u001b[92mTest perplexity: 137.686282 ||| loss 4.924978\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #7, LR: 0.1 ------------\n",
            "Batch #1000 Loss: 4.640041600465775\n",
            "Batch #2000 Loss: 4.668796770811081\n",
            "Batch #3000 Loss: 4.668355017900467\n",
            "Batch #4000 Loss: 4.680041443586349\n",
            "Batch #5000 Loss: 4.671560183048248\n",
            "\u001b[92mTrain perplexity: 91.563709 ||| loss 4.517035\u001b[0m\n",
            "\u001b[92mValidation perplexity: 139.196668 ||| loss 4.935888\u001b[0m\n",
            "\u001b[92mTest perplexity: 133.728746 ||| loss 4.895813\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #8, LR: 0.05 ------------\n",
            "Batch #1000 Loss: 4.483796504735946\n",
            "Batch #2000 Loss: 4.474775673627853\n",
            "Batch #3000 Loss: 4.476645473957062\n",
            "Batch #4000 Loss: 4.495593350410461\n",
            "Batch #5000 Loss: 4.493933579206467\n",
            "\u001b[92mTrain perplexity: 79.289523 ||| loss 4.373106\u001b[0m\n",
            "\u001b[92mValidation perplexity: 133.137916 ||| loss 4.891386\u001b[0m\n",
            "\u001b[92mTest perplexity: 127.859332 ||| loss 4.850931\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #9, LR: 0.025 ------------\n",
            "Batch #1000 Loss: 4.357829174041748\n",
            "Batch #2000 Loss: 4.363383934736252\n",
            "Batch #3000 Loss: 4.369365384578705\n",
            "Batch #4000 Loss: 4.361145201683044\n",
            "Batch #5000 Loss: 4.386676142930985\n",
            "\u001b[92mTrain perplexity: 72.989676 ||| loss 4.290318\u001b[0m\n",
            "\u001b[92mValidation perplexity: 130.400903 ||| loss 4.870614\u001b[0m\n",
            "\u001b[92mTest perplexity: 124.898724 ||| loss 4.827503\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #10, LR: 0.0125 ------------\n",
            "Batch #1000 Loss: 4.282430692911148\n",
            "Batch #2000 Loss: 4.300764269590378\n",
            "Batch #3000 Loss: 4.2892064373493195\n",
            "Batch #4000 Loss: 4.303667073965072\n",
            "Batch #5000 Loss: 4.310706379175186\n",
            "\u001b[92mTrain perplexity: 70.125740 ||| loss 4.250290\u001b[0m\n",
            "\u001b[92mValidation perplexity: 130.263806 ||| loss 4.869562\u001b[0m\n",
            "\u001b[92mTest perplexity: 124.768900 ||| loss 4.826463\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #11, LR: 0.00625 ------------\n",
            "Batch #1000 Loss: 4.246046351194382\n",
            "Batch #2000 Loss: 4.260610674858094\n",
            "Batch #3000 Loss: 4.256864105939865\n",
            "Batch #4000 Loss: 4.25817034983635\n",
            "Batch #5000 Loss: 4.265374394893646\n",
            "\u001b[92mTrain perplexity: 68.662665 ||| loss 4.229206\u001b[0m\n",
            "\u001b[92mValidation perplexity: 130.075549 ||| loss 4.868115\u001b[0m\n",
            "\u001b[92mTest perplexity: 124.879966 ||| loss 4.827353\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #12, LR: 0.003125 ------------\n",
            "Batch #1000 Loss: 4.224320861816406\n",
            "Batch #2000 Loss: 4.233138258934021\n",
            "Batch #3000 Loss: 4.231231918334961\n",
            "Batch #4000 Loss: 4.246764368295669\n",
            "Batch #5000 Loss: 4.243819434404373\n",
            "\u001b[92mTrain perplexity: 68.038758 ||| loss 4.220078\u001b[0m\n",
            "\u001b[92mValidation perplexity: 130.257160 ||| loss 4.869511\u001b[0m\n",
            "\u001b[92mTest perplexity: 124.702819 ||| loss 4.825933\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #13, LR: 0.0015625 ------------\n",
            "Batch #1000 Loss: 4.213994126319886\n",
            "Batch #2000 Loss: 4.212513509750366\n",
            "Batch #3000 Loss: 4.231150759935379\n",
            "Batch #4000 Loss: 4.226819678068161\n",
            "Batch #5000 Loss: 4.234708080768585\n",
            "\u001b[92mTrain perplexity: 67.738413 ||| loss 4.215653\u001b[0m\n",
            "\u001b[92mValidation perplexity: 130.329291 ||| loss 4.870064\u001b[0m\n",
            "\u001b[92mTest perplexity: 125.257410 ||| loss 4.830371\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Epoch #14, LR: 0.00078125 ------------\n",
            "Batch #1000 Loss: 4.2226256482601165\n",
            "Batch #2000 Loss: 4.216299195051193\n",
            "Batch #3000 Loss: 4.222445920944214\n",
            "Batch #4000 Loss: 4.211479228019714\n",
            "Batch #5000 Loss: 4.219208149433136\n",
            "\u001b[92mTrain perplexity: 67.602501 ||| loss 4.213645\u001b[0m\n",
            "\u001b[92mValidation perplexity: 130.414210 ||| loss 4.870716\u001b[0m\n",
            "\u001b[92mTest perplexity: 125.304843 ||| loss 4.830750\u001b[0m\n",
            "------------------------------------\n",
            "\n",
            "----------- Train Complete! ------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea45a6d3ec8a44a0a017d77f3e6446ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Test-perplexity</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train-loss</td><td>█▆▅▄▃▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Train-perplexity</td><td>█▅▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation-loss</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation-perplexity</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test-loss</td><td>4.83075</td></tr><tr><td>Test-perplexity</td><td>125.30484</td></tr><tr><td>Train-loss</td><td>4.21364</td></tr><tr><td>Train-perplexity</td><td>67.6025</td></tr><tr><td>Validation-loss</td><td>4.87072</td></tr><tr><td>Validation-perplexity</td><td>130.41421</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glamorous-donkey-40</strong> at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/doqxrqg8' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2/runs/doqxrqg8</a><br/> View project at: <a href='https://wandb.ai/mitkrieger-cornell-university/dl-assignment2' target=\"_blank\">https://wandb.ai/mitkrieger-cornell-university/dl-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240922_050809-doqxrqg8/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(project=\"dl-assignment2\")\n",
        "results = train_network(model, dataloaders, cross_entropy, sgd, schedule, device, 14, 1000)\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_lambda(epoch):\n",
        "    if epoch < 7:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0.5 ** (epoch - 6)\n",
        "\n",
        "model = ZarembaRNN(len(train.vocab), 10)\n",
        "model = model.to(device)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "adam = optim.Adam(model.parameters(), lr=1e-2)\n",
        "schedule = optim.lr_scheduler.LambdaLR(adam, lr_lambda)\n",
        "# device = torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "lvN_lQb-dGa6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDZNx82PIVEW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea45a6d3ec8a44a0a017d77f3e6446ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_013c468a455f4e3b98ca991cbd88f9f3",
              "IPY_MODEL_e1e001ddb22349e4b4a3e9b47c516f78"
            ],
            "layout": "IPY_MODEL_eff560204349468ebefe15ee839c984a"
          }
        },
        "013c468a455f4e3b98ca991cbd88f9f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9bf9e4017ab4ed0988c7081e5e1e204",
            "placeholder": "​",
            "style": "IPY_MODEL_2f5fc0c7f5ed47a1819b4479bc2e04f6",
            "value": "0.017 MB of 0.017 MB uploaded\r"
          }
        },
        "e1e001ddb22349e4b4a3e9b47c516f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_514e602c642f4aa483ca73e9db67f17a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3c8851a143847a0a550a896a3ac6d73",
            "value": 1
          }
        },
        "eff560204349468ebefe15ee839c984a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9bf9e4017ab4ed0988c7081e5e1e204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5fc0c7f5ed47a1819b4479bc2e04f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "514e602c642f4aa483ca73e9db67f17a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c8851a143847a0a550a896a3ac6d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}